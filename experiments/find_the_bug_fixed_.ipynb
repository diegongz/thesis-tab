{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/diego/Git/thesis-tabtrans')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import training, callback, evaluating, attention, data\n",
    "from sklearn import datasets, model_selection, pipeline, metrics\n",
    "import skorch\n",
    "import pandas as pd\n",
    "import openml\n",
    "from skorch.callbacks import Checkpoint, EarlyStopping, LoadInitState, EpochScoring, Checkpoint, TrainEndCheckpoint\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, StandardScaler #to create one hot encoding for categorical variables\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:openml.datasets.dataset:pickle write anneal\n"
     ]
    }
   ],
   "source": [
    "task_id = 233090 #anneal\n",
    "#task_id = 233093 #mfeat\n",
    "#task_id = 233092 #arrhythmia\n",
    "#task_id = 233108 #cnae-9\n",
    "\n",
    "task = openml.tasks.get_task(task_id)  \n",
    "dataset_id = task.dataset_id\n",
    "df = data.read_dataset_by_id(dataset_id) #this function returns a dictionary with the dataset's data and metadata\n",
    "\n",
    "X = df[\"features\"] #features\n",
    "y = df[\"outputs\"].codes #outputs\n",
    "\n",
    "categorical_features = df['categorical'].tolist() #name of the categorical features\n",
    "numerical_features = df['numerical'].tolist() #name of the numerical features\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "seed = 11\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.20, random_state= seed, stratify=y)\n",
    "train_indices, val_indices = model_selection.train_test_split(np.arange(X_train.shape[0]), test_size=1/3, stratify=y_train) #1/3 of train is equal to 20% of total\n",
    "\n",
    "\n",
    "X_categorical = X_train[categorical_features]  # Categorical features\n",
    "X_numerical = X_train[numerical_features]     # Numerical features\n",
    "\n",
    "\n",
    "# Always processing using the imputer. If there were not nan, nothing will happen\n",
    "imputer = pipeline.Pipeline([('imputer', KNNImputer(n_neighbors=10)), ('scaler', StandardScaler())])\n",
    "imputer = imputer.fit(X_numerical.iloc[train_indices])\n",
    "numerical_imputed = imputer.transform(X_numerical)\n",
    "X_numerical = pd.DataFrame(numerical_imputed, columns=X_numerical.columns) # Convert NumPy array back to Pandas DataFrame\n",
    "\n",
    "\n",
    "# Use ordinal encoder, not label encoder\n",
    "# The nan values and non-existing categories are mapped to -1\n",
    "le = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1, encoded_missing_value=-1)\n",
    "le = le.fit(X_categorical.iloc[train_indices])\n",
    "# Adding a 1 ensures that -1->0, 0->1, 1->2 indexing correctly the architecture's embeddings table\n",
    "categorical_imputed = le.transform(X_categorical) + 1\n",
    "X_categorical = pd.DataFrame(categorical_imputed, columns=X_categorical.columns)\n",
    "    \n",
    "    \n",
    "X_ordered = pd.concat([X_numerical, X_categorical], axis=1)\n",
    "X_train = X_ordered.values\n",
    "\n",
    "n_instances = X_ordered.shape[0]\n",
    "n_numerical = X_numerical.shape[1]\n",
    "n_categories = [X_categorical[col].nunique() for col in X_categorical.columns] #list that tells the number of categories for each categorical feature\n",
    "n_labels = len(df[\"labels\"].keys()) #number of labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using these hyperparameters take ~65 epochs to reach the 99% of balancede accuracy\n",
    "n_layers = 4\n",
    "n_heads = 4\n",
    "embed_dim = 128 #The embedding size is set one by one to avoid the out of memory error\n",
    "batch_size = 32 # 32, 64, 128, 256, 512, 1024\n",
    "epochs = 30\n",
    "\n",
    "# Using these hyperparameters take ~65 epochs to reach the 99% of balancede accuracy\n",
    "#n_layers = 2\n",
    "#n_heads = 4\n",
    "#embed_dim = 128 #The embedding size is set one by one to avoid the out of memory error\n",
    "#batch_size = 32 # I recommend to use this batch size\n",
    "#epochs = 100\n",
    "\n",
    "#parameters for the model\n",
    "ff_pw_size = 30  #this value because of the paper \n",
    "attn_dropout = 0.3 #paper\n",
    "ff_dropout = 0.1 #paper value\n",
    "aggregator = \"cls\"\n",
    "aggregator_parameters = None\n",
    "decoder_hidden_units = [128,64] #paper value [128,64]\n",
    "decoder_activation_fn = nn.ReLU()\n",
    "need_weights = False\n",
    "numerical_passthrough = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego/anaconda3/envs/tabtrans/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer was not TransformerEncoderLayer\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Building PyTorch module.\n",
    "\n",
    "We provide a wrapper function for building the PyTorch module.\n",
    "The function is utils.training.build_module.\n",
    "\"\"\"\n",
    "module = training.build_module(\n",
    "    n_categories, # List of number of categories\n",
    "    n_numerical, # Number of numerical features\n",
    "    n_heads, # Number of heads per layer\n",
    "    ff_pw_size, # Size of the MLP inside each transformer encoder layer\n",
    "    n_layers, # Number of transformer encoder layers    \n",
    "    n_labels, # Number of output neurons\n",
    "    embed_dim,\n",
    "    attn_dropout, \n",
    "    ff_dropout, \n",
    "    aggregator, # The aggregator for output vectors before decoder\n",
    "    rnn_aggregator_parameters=aggregator_parameters,\n",
    "    decoder_hidden_units=decoder_hidden_units,\n",
    "    decoder_activation_fn=decoder_activation_fn,\n",
    "    need_weights=need_weights,\n",
    "    numerical_passthrough=numerical_passthrough\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = skorch.NeuralNetClassifier(\n",
    "    module=module,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    batch_size = batch_size,\n",
    "    max_epochs = epochs,\n",
    "    train_split=skorch.dataset.ValidSplit(((train_indices, val_indices),)),\n",
    "    callbacks=[\n",
    "        (\"balanced_accuracy\", skorch.callbacks.EpochScoring(\"balanced_accuracy\", lower_is_better=False)),\n",
    "        (\"duration\", skorch.callbacks.EpochTimer()),\n",
    "        EpochScoring(scoring='accuracy', name='train_acc', on_train=True), #        Checkpoint(monitor='valid_acc_best', dirname=path_of_checkpoint, load_best = True), \n",
    "        EarlyStopping(patience=15)\n",
    "\n",
    "    ],\n",
    "    optimizer__lr=1e-4,\n",
    "    optimizer__weight_decay=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    balanced_accuracy    train_acc    train_loss    valid_acc    valid_loss     dur\n",
      "-------  -------------------  -----------  ------------  -----------  ------------  ------\n",
      "      1               \u001b[36m0.2000\u001b[0m       \u001b[32m0.5669\u001b[0m        \u001b[35m1.6635\u001b[0m       \u001b[31m0.7625\u001b[0m        \u001b[94m1.5233\u001b[0m  0.2810\n",
      "      2               0.2000       0.7615        \u001b[35m1.4103\u001b[0m       0.7625        \u001b[94m1.2488\u001b[0m  0.1508\n",
      "      3               0.2000       0.7615        \u001b[35m1.1260\u001b[0m       0.7625        \u001b[94m0.9906\u001b[0m  0.1531\n",
      "      4               0.2000       0.7615        \u001b[35m0.9522\u001b[0m       0.7625        \u001b[94m0.9057\u001b[0m  0.1500\n",
      "      5               0.2000       0.7615        \u001b[35m0.8845\u001b[0m       0.7625        \u001b[94m0.8539\u001b[0m  0.1491\n",
      "      6               0.2000       0.7615        \u001b[35m0.8462\u001b[0m       0.7625        \u001b[94m0.8302\u001b[0m  0.1493\n",
      "      7               0.2000       0.7615        \u001b[35m0.8272\u001b[0m       0.7625        \u001b[94m0.8111\u001b[0m  0.1491\n",
      "      8               0.2000       0.7615        \u001b[35m0.8038\u001b[0m       0.7625        \u001b[94m0.7803\u001b[0m  0.1505\n",
      "      9               0.2000       0.7615        \u001b[35m0.7491\u001b[0m       0.7625        \u001b[94m0.7071\u001b[0m  0.1584\n",
      "     10               0.2000       0.7615        \u001b[35m0.6713\u001b[0m       0.7625        \u001b[94m0.6180\u001b[0m  0.1549\n",
      "     11               \u001b[36m0.3209\u001b[0m       0.7678        \u001b[35m0.6209\u001b[0m       \u001b[31m0.8208\u001b[0m        \u001b[94m0.5643\u001b[0m  0.1505\n",
      "     12               \u001b[36m0.3660\u001b[0m       0.7887        \u001b[35m0.5490\u001b[0m       \u001b[31m0.8417\u001b[0m        \u001b[94m0.5204\u001b[0m  0.1482\n",
      "     13               \u001b[36m0.4993\u001b[0m       0.7929        \u001b[35m0.5324\u001b[0m       \u001b[31m0.8917\u001b[0m        \u001b[94m0.4795\u001b[0m  0.1485\n",
      "     14               \u001b[36m0.5660\u001b[0m       0.8410        \u001b[35m0.4780\u001b[0m       \u001b[31m0.9167\u001b[0m        \u001b[94m0.4284\u001b[0m  0.1480\n",
      "     15               0.5660       0.8870        \u001b[35m0.4161\u001b[0m       0.9167        \u001b[94m0.3736\u001b[0m  0.1482\n",
      "     16               0.5660       0.9059        \u001b[35m0.3632\u001b[0m       0.9167        \u001b[94m0.3214\u001b[0m  0.1488\n",
      "     17               0.5660       0.8933        \u001b[35m0.3357\u001b[0m       0.9167        \u001b[94m0.2806\u001b[0m  0.1476\n",
      "     18               \u001b[36m0.5670\u001b[0m       0.9163        \u001b[35m0.2769\u001b[0m       \u001b[31m0.9208\u001b[0m        \u001b[94m0.2492\u001b[0m  0.1479\n",
      "     19               0.5670       0.9163        \u001b[35m0.2581\u001b[0m       0.9208        \u001b[94m0.2276\u001b[0m  0.1480\n",
      "     20               0.5670       0.9247        \u001b[35m0.2251\u001b[0m       0.9208        \u001b[94m0.2101\u001b[0m  0.1661\n",
      "     21               \u001b[36m0.5736\u001b[0m       0.9268        \u001b[35m0.2137\u001b[0m       0.9208        \u001b[94m0.1962\u001b[0m  0.1597\n",
      "     22               0.5736       0.9310        \u001b[35m0.1882\u001b[0m       0.9208        \u001b[94m0.1883\u001b[0m  0.1506\n",
      "     23               \u001b[36m0.6436\u001b[0m       0.9331        0.1895       \u001b[31m0.9417\u001b[0m        \u001b[94m0.1806\u001b[0m  0.1494\n",
      "     24               0.6293       0.9393        \u001b[35m0.1735\u001b[0m       0.9375        \u001b[94m0.1756\u001b[0m  0.1513\n",
      "     25               \u001b[36m0.6618\u001b[0m       0.9435        \u001b[35m0.1577\u001b[0m       \u001b[31m0.9458\u001b[0m        \u001b[94m0.1675\u001b[0m  0.1495\n",
      "     26               \u001b[36m0.6981\u001b[0m       0.9331        0.1617       \u001b[31m0.9542\u001b[0m        0.1681  0.1497\n",
      "     27               0.6799       0.9435        \u001b[35m0.1501\u001b[0m       0.9500        \u001b[94m0.1582\u001b[0m  0.1492\n",
      "     28               0.6656       0.9477        \u001b[35m0.1435\u001b[0m       0.9458        \u001b[94m0.1545\u001b[0m  0.1497\n",
      "     29               0.6882       0.9519        \u001b[35m0.1302\u001b[0m       0.9417        0.1547  0.1497\n",
      "     30               0.6733       0.9540        0.1321       0.9500        \u001b[94m0.1438\u001b[0m  0.1489\n"
     ]
    }
   ],
   "source": [
    "model = model.fit(X={\n",
    "        \"x_numerical\": X_train[:, :n_numerical].astype(np.float32),\n",
    "        \"x_categorical\": X_train[:, n_numerical:].astype(np.int32)\n",
    "        }, \n",
    "        y=y_train.astype(np.int64)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict({\n",
    "        \"x_numerical\": X_train[:, :n_numerical].astype(np.float32),\n",
    "        \"x_categorical\": X_train[:, n_numerical:].astype(np.int32)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6733386831747488"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.balanced_accuracy_score(y_train[val_indices].astype(np.int64), preds[val_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
