{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/diego/Git/thesis-tabtrans')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import training, callback, evaluating, attention, data\n",
    "from sklearn import datasets, model_selection, pipeline, metrics\n",
    "import skorch\n",
    "import pandas as pd\n",
    "import openml\n",
    "from skorch.callbacks import Checkpoint, EarlyStopping, LoadInitState, EpochScoring, Checkpoint, TrainEndCheckpoint\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, StandardScaler #to create one hot encoding for categorical variables\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:openml.datasets.dataset:pickle write cnae-9\n"
     ]
    }
   ],
   "source": [
    "#task_id = 233090 #anneal\n",
    "#task_id = 233093 #mfeat\n",
    "#task_id = 233092 #arrhythmia\n",
    "task_id = 233108 #cnae-9\n",
    "\n",
    "task = openml.tasks.get_task(task_id)  \n",
    "dataset_id = task.dataset_id\n",
    "df = data.read_dataset_by_id(dataset_id) #this function returns a dictionary with the dataset's data and metadata\n",
    "\n",
    "X = df[\"features\"] #features\n",
    "y = df[\"outputs\"].codes #outputs\n",
    "\n",
    "categorical_features = df['categorical'].tolist() #name of the categorical features\n",
    "numerical_features = df['numerical'].tolist() #name of the numerical features\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "seed = 11\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.20, random_state= seed, stratify=y)\n",
    "train_indices, val_indices = model_selection.train_test_split(np.arange(X_train.shape[0]), test_size=1/3, stratify=y_train) #1/3 of train is equal to 20% of total\n",
    "\n",
    "\n",
    "X_categorical = X_train[categorical_features]  # Categorical features\n",
    "X_numerical = X_train[numerical_features]     # Numerical features\n",
    "\n",
    "\n",
    "# Always processing using the imputer. If there were not nan, nothing will happen\n",
    "imputer = pipeline.Pipeline([('imputer', KNNImputer(n_neighbors=10)), ('scaler', StandardScaler())])\n",
    "imputer = imputer.fit(X_numerical.iloc[train_indices])\n",
    "numerical_imputed = imputer.transform(X_numerical)\n",
    "X_numerical = pd.DataFrame(numerical_imputed, columns=X_numerical.columns) # Convert NumPy array back to Pandas DataFrame\n",
    "\n",
    "\n",
    "# Use ordinal encoder, not label encoder\n",
    "# The nan values and non-existing categories are mapped to -1\n",
    "le = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1, encoded_missing_value=-1)\n",
    "le = le.fit(X_categorical.iloc[train_indices])\n",
    "# Adding a 1 ensures that -1->0, 0->1, 1->2 indexing correctly the architecture's embeddings table\n",
    "categorical_imputed = le.transform(X_categorical) + 1\n",
    "X_categorical = pd.DataFrame(categorical_imputed, columns=X_categorical.columns)\n",
    "    \n",
    "    \n",
    "X_ordered = pd.concat([X_numerical, X_categorical], axis=1)\n",
    "X_train = X_ordered.values\n",
    "\n",
    "n_instances = X_ordered.shape[0]\n",
    "n_numerical = X_numerical.shape[1]\n",
    "n_categories = [X_categorical[col].nunique() for col in X_categorical.columns] #list that tells the number of categories for each categorical feature\n",
    "n_labels = len(df[\"labels\"].keys()) #number of labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using these hyperparameters take ~65 epochs to reach the 99% of balancede accuracy\n",
    "n_layers = 4\n",
    "n_heads = 4\n",
    "embed_dim = 128 #The embedding size is set one by one to avoid the out of memory error\n",
    "batch_size = 32 # 32, 64, 128, 256, 512, 1024\n",
    "epochs = 150\n",
    "\n",
    "# Using these hyperparameters take ~65 epochs to reach the 99% of balancede accuracy\n",
    "#n_layers = 2\n",
    "#n_heads = 4\n",
    "#embed_dim = 128 #The embedding size is set one by one to avoid the out of memory error\n",
    "#batch_size = 32 # I recommend to use this batch size\n",
    "#epochs = 100\n",
    "\n",
    "#parameters for the model\n",
    "ff_pw_size = 30  #this value because of the paper \n",
    "attn_dropout = 0.3 #paper\n",
    "ff_dropout = 0.1 #paper value\n",
    "aggregator = \"cls\"\n",
    "aggregator_parameters = None\n",
    "decoder_hidden_units = [128,64] #paper value [128,64]\n",
    "decoder_activation_fn = nn.ReLU()\n",
    "need_weights = False\n",
    "numerical_passthrough = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego/anaconda3/envs/tabtrans/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer was not TransformerEncoderLayer\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Building PyTorch module.\n",
    "\n",
    "We provide a wrapper function for building the PyTorch module.\n",
    "The function is utils.training.build_module.\n",
    "\"\"\"\n",
    "module = training.build_module(\n",
    "    n_categories, # List of number of categories\n",
    "    n_numerical, # Number of numerical features\n",
    "    n_heads, # Number of heads per layer\n",
    "    ff_pw_size, # Size of the MLP inside each transformer encoder layer\n",
    "    n_layers, # Number of transformer encoder layers    \n",
    "    n_labels, # Number of output neurons\n",
    "    embed_dim,\n",
    "    attn_dropout, \n",
    "    ff_dropout, \n",
    "    aggregator, # The aggregator for output vectors before decoder\n",
    "    rnn_aggregator_parameters=aggregator_parameters,\n",
    "    decoder_hidden_units=decoder_hidden_units,\n",
    "    decoder_activation_fn=decoder_activation_fn,\n",
    "    need_weights=need_weights,\n",
    "    numerical_passthrough=numerical_passthrough\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = skorch.NeuralNetClassifier(\n",
    "    module=module,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    batch_size = batch_size,\n",
    "    max_epochs = epochs,\n",
    "    train_split=skorch.dataset.ValidSplit(((train_indices, val_indices),)),\n",
    "    callbacks=[\n",
    "        (\"balanced_accuracy\", skorch.callbacks.EpochScoring(\"balanced_accuracy\", lower_is_better=False)),\n",
    "        (\"duration\", skorch.callbacks.EpochTimer()),\n",
    "        EpochScoring(scoring='accuracy', name='train_acc', on_train=True), #        Checkpoint(monitor='valid_acc_best', dirname=path_of_checkpoint, load_best = True), \n",
    "        EarlyStopping(patience=15)\n",
    "\n",
    "    ],\n",
    "    optimizer__lr=1e-4,\n",
    "    optimizer__weight_decay=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    balanced_accuracy    train_acc    train_loss    valid_acc    valid_loss     dur\n",
      "-------  -------------------  -----------  ------------  -----------  ------------  ------\n",
      "      1               \u001b[36m0.1111\u001b[0m       \u001b[32m0.1128\u001b[0m        \u001b[35m2.2073\u001b[0m       \u001b[31m0.1111\u001b[0m        \u001b[94m2.2004\u001b[0m  7.0588\n",
      "      2               0.1111       \u001b[32m0.1024\u001b[0m        \u001b[35m2.1996\u001b[0m       0.1111        \u001b[94m2.1979\u001b[0m  6.9855\n",
      "      3               0.1111       0.1215        \u001b[35m2.1978\u001b[0m       0.1111        \u001b[94m2.1975\u001b[0m  6.9846\n",
      "      4               0.1111       \u001b[32m0.0990\u001b[0m        2.1990       0.1111        \u001b[94m2.1973\u001b[0m  6.9854\n",
      "      5               0.1111       0.1354        \u001b[35m2.1978\u001b[0m       0.1111        \u001b[94m2.1972\u001b[0m  6.9854\n",
      "      6               0.1111       \u001b[32m0.0920\u001b[0m        2.1984       0.1111        \u001b[94m2.1971\u001b[0m  6.9850\n",
      "      7               0.1111       0.1076        \u001b[35m2.1973\u001b[0m       0.1111        \u001b[94m2.1970\u001b[0m  6.9849\n",
      "      8               0.1111       0.0938        2.1985       0.1111        \u001b[94m2.1970\u001b[0m  6.9839\n",
      "      9               0.1111       0.1128        2.1985       0.1111        \u001b[94m2.1970\u001b[0m  6.9846\n",
      "     10               0.1111       0.1076        2.1979       0.1111        \u001b[94m2.1969\u001b[0m  6.9848\n",
      "     11               0.1111       0.1042        2.1981       0.1111        \u001b[94m2.1969\u001b[0m  6.9857\n",
      "     12               0.1111       0.1007        2.1975       0.1111        2.1970  6.9855\n",
      "     13               0.1111       0.1076        2.1986       0.1111        2.1969  6.9851\n",
      "     14               0.1111       0.1198        2.1981       0.1111        2.1970  6.9852\n",
      "     15               0.1111       0.1181        2.1977       0.1111        2.1969  6.9853\n",
      "     16               \u001b[36m0.1146\u001b[0m       0.1267        2.1976       \u001b[31m0.1146\u001b[0m        \u001b[94m2.1969\u001b[0m  6.9862\n",
      "     17               0.1076       0.0920        2.1980       0.1076        \u001b[94m2.1967\u001b[0m  6.9846\n",
      "     18               0.1111       0.1198        \u001b[35m2.1971\u001b[0m       0.1111        \u001b[94m2.1966\u001b[0m  6.9864\n",
      "     19               \u001b[36m0.1389\u001b[0m       \u001b[32m0.0868\u001b[0m        2.1983       \u001b[31m0.1389\u001b[0m        \u001b[94m2.1965\u001b[0m  6.9846\n",
      "     20               0.1111       0.1198        2.1978       0.1111        \u001b[94m2.1964\u001b[0m  6.9851\n",
      "     21               0.1111       0.1163        2.1979       0.1111        2.1964  6.9848\n",
      "     22               \u001b[36m0.1632\u001b[0m       0.0938        2.1986       \u001b[31m0.1632\u001b[0m        \u001b[94m2.1964\u001b[0m  6.9852\n",
      "     23               0.1354       0.1285        \u001b[35m2.1967\u001b[0m       0.1354        \u001b[94m2.1962\u001b[0m  6.9857\n",
      "     24               \u001b[36m0.1667\u001b[0m       0.1042        2.1978       \u001b[31m0.1667\u001b[0m        \u001b[94m2.1960\u001b[0m  6.9859\n",
      "     25               0.1111       0.0868        2.1973       0.1111        \u001b[94m2.1960\u001b[0m  6.9852\n",
      "     26               \u001b[36m0.1979\u001b[0m       0.1215        2.1968       \u001b[31m0.1979\u001b[0m        \u001b[94m2.1959\u001b[0m  6.9859\n",
      "     27               0.1806       0.1285        \u001b[35m2.1965\u001b[0m       0.1806        \u001b[94m2.1957\u001b[0m  6.9844\n",
      "     28               0.1736       0.1163        \u001b[35m2.1964\u001b[0m       0.1736        \u001b[94m2.1954\u001b[0m  6.9857\n",
      "     29               \u001b[36m0.2083\u001b[0m       0.1441        \u001b[35m2.1955\u001b[0m       \u001b[31m0.2083\u001b[0m        \u001b[94m2.1951\u001b[0m  6.9851\n",
      "     30               0.1250       0.1354        \u001b[35m2.1955\u001b[0m       0.1250        \u001b[94m2.1943\u001b[0m  6.9853\n",
      "     31               \u001b[36m0.2500\u001b[0m       0.1788        \u001b[35m2.1882\u001b[0m       \u001b[31m0.2500\u001b[0m        \u001b[94m2.1661\u001b[0m  6.9851\n",
      "     32               \u001b[36m0.3021\u001b[0m       0.2118        \u001b[35m2.1430\u001b[0m       \u001b[31m0.3021\u001b[0m        \u001b[94m2.0804\u001b[0m  6.9849\n",
      "     33               0.3021       0.2726        \u001b[35m2.0387\u001b[0m       0.3021        \u001b[94m1.9391\u001b[0m  6.9851\n",
      "     34               \u001b[36m0.4028\u001b[0m       0.3212        \u001b[35m1.9052\u001b[0m       \u001b[31m0.4028\u001b[0m        \u001b[94m1.8304\u001b[0m  6.9854\n",
      "     35               \u001b[36m0.4826\u001b[0m       0.3594        \u001b[35m1.7489\u001b[0m       \u001b[31m0.4826\u001b[0m        \u001b[94m1.6511\u001b[0m  6.9848\n",
      "     36               0.4826       0.4462        \u001b[35m1.5735\u001b[0m       0.4826        \u001b[94m1.4419\u001b[0m  6.9846\n",
      "     37               \u001b[36m0.4896\u001b[0m       0.4983        \u001b[35m1.3791\u001b[0m       \u001b[31m0.4896\u001b[0m        \u001b[94m1.2720\u001b[0m  6.9854\n",
      "     38               \u001b[36m0.6181\u001b[0m       0.5295        \u001b[35m1.2592\u001b[0m       \u001b[31m0.6181\u001b[0m        \u001b[94m1.1415\u001b[0m  6.9846\n",
      "     39               0.6042       0.5851        \u001b[35m1.1243\u001b[0m       0.6042        \u001b[94m1.0817\u001b[0m  6.9843\n",
      "     40               \u001b[36m0.6285\u001b[0m       0.6458        \u001b[35m0.9858\u001b[0m       \u001b[31m0.6285\u001b[0m        \u001b[94m0.9928\u001b[0m  6.9845\n",
      "     41               \u001b[36m0.6736\u001b[0m       0.6545        \u001b[35m0.8946\u001b[0m       \u001b[31m0.6736\u001b[0m        \u001b[94m0.9226\u001b[0m  6.9845\n",
      "     42               \u001b[36m0.6944\u001b[0m       0.7205        \u001b[35m0.8404\u001b[0m       \u001b[31m0.6944\u001b[0m        \u001b[94m0.8808\u001b[0m  6.9841\n",
      "     43               \u001b[36m0.7257\u001b[0m       0.7257        \u001b[35m0.7530\u001b[0m       \u001b[31m0.7257\u001b[0m        \u001b[94m0.8367\u001b[0m  6.9855\n",
      "     44               \u001b[36m0.7639\u001b[0m       0.7778        \u001b[35m0.6837\u001b[0m       \u001b[31m0.7639\u001b[0m        \u001b[94m0.8069\u001b[0m  6.9846\n",
      "     45               0.7604       0.7865        \u001b[35m0.6652\u001b[0m       0.7604        \u001b[94m0.7630\u001b[0m  6.9841\n",
      "     46               \u001b[36m0.7778\u001b[0m       0.8038        \u001b[35m0.5850\u001b[0m       \u001b[31m0.7778\u001b[0m        \u001b[94m0.7328\u001b[0m  6.9844\n",
      "     47               \u001b[36m0.7917\u001b[0m       0.8194        \u001b[35m0.5569\u001b[0m       \u001b[31m0.7917\u001b[0m        \u001b[94m0.7154\u001b[0m  6.9846\n",
      "     48               0.7778       0.8316        \u001b[35m0.5442\u001b[0m       0.7778        \u001b[94m0.7106\u001b[0m  6.9851\n",
      "     49               \u001b[36m0.8229\u001b[0m       0.8663        \u001b[35m0.4684\u001b[0m       \u001b[31m0.8229\u001b[0m        \u001b[94m0.6639\u001b[0m  6.9851\n",
      "     50               \u001b[36m0.8299\u001b[0m       0.8403        0.4830       \u001b[31m0.8299\u001b[0m        \u001b[94m0.6503\u001b[0m  6.9844\n",
      "     51               0.8264       0.8611        \u001b[35m0.4157\u001b[0m       0.8264        \u001b[94m0.6494\u001b[0m  6.9852\n",
      "     52               \u001b[36m0.8368\u001b[0m       0.8715        \u001b[35m0.4059\u001b[0m       \u001b[31m0.8368\u001b[0m        \u001b[94m0.6455\u001b[0m  6.9860\n",
      "     53               0.8264       0.8854        \u001b[35m0.3776\u001b[0m       0.8264        \u001b[94m0.6322\u001b[0m  6.9846\n",
      "     54               0.8368       0.8889        \u001b[35m0.3504\u001b[0m       0.8368        \u001b[94m0.6165\u001b[0m  6.9850\n",
      "     55               0.8264       0.8837        \u001b[35m0.3430\u001b[0m       0.8264        \u001b[94m0.6155\u001b[0m  6.9846\n",
      "     56               0.8333       0.8976        \u001b[35m0.3189\u001b[0m       0.8333        0.6266  6.9852\n",
      "     57               0.8264       0.9028        \u001b[35m0.3079\u001b[0m       0.8264        0.6278  6.9850\n",
      "     58               0.8299       0.9149        \u001b[35m0.2993\u001b[0m       0.8299        0.6385  6.9847\n",
      "     59               \u001b[36m0.8403\u001b[0m       0.9201        \u001b[35m0.2635\u001b[0m       \u001b[31m0.8403\u001b[0m        \u001b[94m0.6066\u001b[0m  6.9840\n",
      "     60               \u001b[36m0.8507\u001b[0m       0.9271        \u001b[35m0.2419\u001b[0m       \u001b[31m0.8507\u001b[0m        \u001b[94m0.6034\u001b[0m  6.9843\n",
      "     61               0.8438       0.9340        \u001b[35m0.2291\u001b[0m       0.8438        \u001b[94m0.6014\u001b[0m  6.9842\n",
      "     62               0.8368       0.9358        0.2337       0.8368        0.6260  6.9841\n",
      "     63               0.8333       0.9253        0.2391       0.8333        0.6045  6.9840\n",
      "     64               0.8472       0.9392        \u001b[35m0.2134\u001b[0m       0.8472        \u001b[94m0.5760\u001b[0m  6.9850\n",
      "     65               \u001b[36m0.8646\u001b[0m       0.9497        \u001b[35m0.1968\u001b[0m       \u001b[31m0.8646\u001b[0m        \u001b[94m0.5696\u001b[0m  6.9843\n",
      "     66               0.8507       0.9531        \u001b[35m0.1675\u001b[0m       0.8507        0.5892  6.9833\n",
      "     67               0.8507       0.9497        \u001b[35m0.1564\u001b[0m       0.8507        \u001b[94m0.5613\u001b[0m  6.9848\n",
      "     68               0.8576       0.9670        0.1605       0.8576        \u001b[94m0.5582\u001b[0m  6.9838\n",
      "     69               0.8576       0.9583        \u001b[35m0.1553\u001b[0m       0.8576        0.5667  6.9852\n",
      "     70               0.8438       0.9653        \u001b[35m0.1474\u001b[0m       0.8438        0.5789  6.9847\n",
      "     71               0.8611       0.9757        \u001b[35m0.1227\u001b[0m       0.8611        0.5593  6.9848\n",
      "     72               \u001b[36m0.8715\u001b[0m       0.9740        \u001b[35m0.1203\u001b[0m       \u001b[31m0.8715\u001b[0m        \u001b[94m0.5415\u001b[0m  6.9849\n",
      "     73               0.8542       0.9757        \u001b[35m0.1187\u001b[0m       0.8542        0.5546  6.9842\n",
      "     74               0.8576       0.9774        \u001b[35m0.0946\u001b[0m       0.8576        0.5487  6.9845\n",
      "     75               0.8576       0.9757        \u001b[35m0.0938\u001b[0m       0.8576        0.5730  6.9849\n",
      "     76               0.8576       0.9740        0.1191       0.8576        0.5704  6.9848\n",
      "     77               0.8646       0.9792        0.0970       0.8646        0.5649  6.9844\n",
      "     78               0.8611       0.9740        \u001b[35m0.0912\u001b[0m       0.8611        0.5648  6.9853\n",
      "     79               0.8576       0.9705        0.1198       0.8576        0.5623  6.9849\n",
      "     80               0.8611       0.9809        \u001b[35m0.0699\u001b[0m       0.8611        0.5698  6.9850\n",
      "     81               0.8576       0.9792        0.0831       0.8576        0.5583  6.9841\n",
      "     82               0.8715       0.9792        0.0741       0.8715        0.5581  6.9846\n",
      "     83               \u001b[36m0.8785\u001b[0m       0.9809        0.0788       \u001b[31m0.8785\u001b[0m        0.5462  6.9840\n",
      "     84               0.8750       0.9809        \u001b[35m0.0663\u001b[0m       0.8750        0.5503  6.9850\n",
      "     85               0.8750       0.9809        0.0959       0.8750        0.5478  6.9850\n",
      "     86               0.8646       0.9774        0.0742       0.8646        0.5611  6.9857\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n"
     ]
    }
   ],
   "source": [
    "model = model.fit(X={\n",
    "        \"x_numerical\": X_train[:, :n_numerical].astype(np.float32),\n",
    "        \"x_categorical\": X_train[:, n_numerical:].astype(np.int32)\n",
    "        }, \n",
    "        y=y_train.astype(np.int64)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict({\n",
    "        \"x_numerical\": X_train[:, :n_numerical].astype(np.float32),\n",
    "        \"x_categorical\": X_train[:, n_numerical:].astype(np.int32)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8576388888888888"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.balanced_accuracy_score(y_train[val_indices].astype(np.int64), preds[val_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
