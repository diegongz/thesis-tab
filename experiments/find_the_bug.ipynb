{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/diego/Git/thesis-tabtrans')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import training, callback, evaluating, attention, data\n",
    "from sklearn import datasets, model_selection\n",
    "import skorch\n",
    "import pandas as pd\n",
    "import openml\n",
    "from skorch.callbacks import Checkpoint, EarlyStopping, LoadInitState, EpochScoring, Checkpoint, TrainEndCheckpoint\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder #to create one hot encoding for categorical variables\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = 233092\n",
    "task = openml.tasks.get_task(task_id)  \n",
    "dataset_id = task.dataset_id\n",
    "df = data.read_dataset_by_id(dataset_id) #this function returns a dictionary with the dataset's data and metadata\n",
    "\n",
    "X = df[\"features\"] #features\n",
    "y = df[\"outputs\"].codes #outputs\n",
    "\n",
    "categorical_features = df['categorical'].tolist() #name of the categorical features\n",
    "numerical_features = df['numerical'].tolist() #name of the numerical features\n",
    "\n",
    "\n",
    "# Create numerical and categorical datasets\n",
    "X_categorical = X[categorical_features]  # Categorical features\n",
    "X_numerical = X[numerical_features]     # Numerical features\n",
    "\n",
    "if X_numerical.isnull().values.any():\n",
    "        imputer = KNNImputer(n_neighbors=10)\n",
    "        numerical_imputed = imputer.fit_transform(X_numerical)\n",
    "        X_numerical = pd.DataFrame(numerical_imputed, columns=X_numerical.columns) # Convert NumPy array back to Pandas DataFrame\n",
    "\n",
    "\n",
    "# Filter out categorical columns with only one unique value\n",
    "redundant_columns = [col for col in X_categorical.columns if X_categorical[col].nunique() <= 1]\n",
    "X_categorical = X_categorical.drop(columns=redundant_columns)\n",
    "\n",
    "# Recompute categorical features after filtering\n",
    "categorical_features = [col for col in categorical_features if col not in redundant_columns]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a LabelEncoder object\n",
    "le = LabelEncoder()\n",
    "for col in X_categorical.columns:\n",
    "    X_categorical[col] = le.fit_transform(X_categorical[col].astype(str))\n",
    "\n",
    "\n",
    "X_ordered = pd.concat([X_numerical, X_categorical], axis=1)\n",
    "\n",
    "n_instances = X_ordered.shape[0]\n",
    "n_numerical = X_numerical.shape[1]\n",
    "n_categories = [X_categorical[col].nunique() for col in X_categorical.columns] #list that tells the number of categories for each categorical feature\n",
    "#n_categories_2 = df[\"n_categorical\"] #this one is from the metadata\n",
    "n_labels = len(df[\"labels\"].keys()) #number of labels\n",
    "\n",
    "seed = 11\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X_ordered, y, test_size=0.20, random_state= seed, stratify=y)\n",
    "\n",
    "X_train = X_train.values.astype(np.float32)\n",
    "X_test = X_test.values.astype(np.float32)\n",
    "\n",
    "\n",
    "train_indices, val_indices = model_selection.train_test_split(np.arange(X_train.shape[0]), test_size=1/3, stratify=y_train) #1/3 of train is equal to 20% of total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_categorical.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_heads = 4 # In average 4 works better\n",
    "embed_dim = 128 # In average 256 works better\n",
    "n_layers = 4\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "\n",
    "#parameters for the model\n",
    "ff_pw_size = 30  #this value because of the paper \n",
    "attn_dropout = 0.3 #paper\n",
    "ff_dropout = 0.1 #paper value\n",
    "aggregator = \"cls\"\n",
    "aggregator_parameters = None\n",
    "decoder_hidden_units = [128,64] #paper value\n",
    "decoder_activation_fn = nn.ReLU()\n",
    "need_weights = False\n",
    "numerical_passthrough = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Building PyTorch module.\n",
    "\n",
    "We provide a wrapper function for building the PyTorch module.\n",
    "The function is utils.training.build_module.\n",
    "\"\"\"\n",
    "module = training.build_module(\n",
    "    n_categories, # List of number of categories\n",
    "    n_numerical, # Number of numerical features\n",
    "    n_heads, # Number of heads per layer\n",
    "    ff_pw_size, # Size of the MLP inside each transformer encoder layer\n",
    "    n_layers, # Number of transformer encoder layers    \n",
    "    n_labels, # Number of output neurons\n",
    "    embed_dim,\n",
    "    attn_dropout, \n",
    "    ff_dropout, \n",
    "    aggregator, # The aggregator for output vectors before decoder\n",
    "    rnn_aggregator_parameters=aggregator_parameters,\n",
    "    decoder_hidden_units=decoder_hidden_units,\n",
    "    decoder_activation_fn=decoder_activation_fn,\n",
    "    need_weights=need_weights,\n",
    "    numerical_passthrough=numerical_passthrough\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = skorch.NeuralNetClassifier(\n",
    "    module=module,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    device = \"cpu\", #if torch.cuda.is_available() else \"cpu\",\n",
    "    batch_size = batch_size,\n",
    "    max_epochs = epochs,\n",
    "    train_split=skorch.dataset.ValidSplit(((train_indices, val_indices),)),\n",
    "    callbacks=[\n",
    "        (\"balanced_accuracy\", skorch.callbacks.EpochScoring(\"balanced_accuracy\", lower_is_better=False)),\n",
    "        (\"duration\", skorch.callbacks.EpochTimer()),\n",
    "        EpochScoring(scoring='accuracy', name='train_acc', on_train=True), #        Checkpoint(monitor='valid_acc_best', dirname=path_of_checkpoint, load_best = True), \n",
    "        EarlyStopping(patience=15)\n",
    "\n",
    "    ],\n",
    "    optimizer__lr=1e-4,\n",
    "    optimizer__weight_decay=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.fit(X={\n",
    "        \"x_numerical\": X_train[:, :n_numerical].astype(np.float32),\n",
    "        \"x_categorical\": X_train[:, n_numerical:].astype(np.int32)\n",
    "        }, \n",
    "        y=y_train.astype(np.int64)\n",
    "    )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Training and validation\n",
    "\n",
    "\n",
    "model = model.fit(X={\n",
    "        \"x_numerical\": X_train[:, :n_numerical].astype(np.float32),\n",
    "        \"x_categorical\": X_train[:, n_numerical:].astype(np.int32)\n",
    "        }, \n",
    "        y=y_train.astype(np.int64)\n",
    "    )\n",
    "    \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabtrans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
