{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/diego/Git/thesis-tabtrans')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import training, callback, evaluating, attention, data\n",
    "from sklearn import datasets, model_selection\n",
    "import skorch\n",
    "import pandas as pd\n",
    "import openml\n",
    "from skorch.callbacks import Checkpoint, EarlyStopping, LoadInitState, EpochScoring, Checkpoint, TrainEndCheckpoint\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler #to create one hot encoding for categorical variables\n",
    "from sklearn.impute import KNNImputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:openml.datasets.dataset:pickle write mfeat-factors\n"
     ]
    }
   ],
   "source": [
    "#import the data\n",
    "task_id = 233093\n",
    "task = openml.tasks.get_task(task_id)\n",
    "dataset_id = task.dataset_id #suppose we input the task id \n",
    "df = data.read_dataset_by_id(dataset_id)\n",
    "\n",
    "X = df[\"features\"].values.astype(np.float32) #features\n",
    "y = df[\"outputs\"].codes #outputs\n",
    "\n",
    "\n",
    "categorical_features = df['categorical'].tolist() #name of the categorical features\n",
    "numerical_features = df['numerical'].tolist() #name of the numerical features\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.20, random_state= 11, stratify=y)\n",
    "train_indices, val_indices = model_selection.train_test_split(np.arange(X_train.shape[0]), test_size=1/3, stratify=y_train) #1/3 of train is equal to 20% of total\n",
    "\n",
    "''' \n",
    "#NORMALIZATION\n",
    "X_train, X_val = X_train[train_indices], X_train[val_indices]\n",
    "\n",
    "#normalize data\n",
    "scaler = StandardScaler() \n",
    "\n",
    "# transform data \n",
    "X_train = scaler.fit_transform(X_train) #scaler returns a numpy array\n",
    "X_val = scaler.transform(X_val) #scaler returns a numpy array\n",
    "X_test = scaler.transform(X_test) #scaler returns a numpy array\n",
    "\n",
    "X_train = np.vstack((X_train, X_val))\n",
    "#### End of the normalization\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "n_instances = X_train.shape[0]\n",
    "n_numerical = X_train.shape[1]\n",
    "n_categories = [] #list that tells the number of categories for each categorical feature\n",
    "n_labels = len(df[\"labels\"].keys()) #number of labels\n",
    "\n",
    "\n",
    "n_layers = 4\n",
    "n_heads = 4\n",
    "embed_dim = 128 #The embedding size is set one by one to avoid the out of memory error\n",
    "batch_size = 64 # 32, 64, 128, 256, 512, 1024\n",
    "epochs = 100\n",
    "\n",
    "#parameters for the model\n",
    "ff_pw_size = 30  #this value because of the paper \n",
    "attn_dropout = 0.3 #paper\n",
    "ff_dropout = 0.1 #paper value\n",
    "aggregator = \"cls\"\n",
    "aggregator_parameters = None\n",
    "decoder_hidden_units = [128,64] #paper value [128,64]\n",
    "decoder_activation_fn = nn.ReLU()\n",
    "need_weights = False\n",
    "numerical_passthrough = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego/anaconda3/envs/tabtrans/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer was not TransformerEncoderLayer\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Building PyTorch module.\n",
    "\n",
    "We provide a wrapper function for building the PyTorch module.\n",
    "The function is utils.training.build_module.\n",
    "\"\"\"\n",
    "module = training.build_module(\n",
    "    n_categories, # List of number of categories\n",
    "    n_numerical, # Number of numerical features\n",
    "    n_heads, # Number of heads per layer\n",
    "    ff_pw_size, # Size of the MLP inside each transformer encoder layer\n",
    "    n_layers, # Number of transformer encoder layers    \n",
    "    n_labels, # Number of output neurons\n",
    "    embed_dim,\n",
    "    attn_dropout, \n",
    "    ff_dropout, \n",
    "    aggregator, # The aggregator for output vectors before decoder\n",
    "    rnn_aggregator_parameters=aggregator_parameters,\n",
    "    decoder_hidden_units=decoder_hidden_units,\n",
    "    decoder_activation_fn=decoder_activation_fn,\n",
    "    need_weights=need_weights,\n",
    "    numerical_passthrough=numerical_passthrough\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = skorch.NeuralNetClassifier(\n",
    "            module=module,\n",
    "            criterion=torch.nn.CrossEntropyLoss,\n",
    "            optimizer=torch.optim.AdamW,\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "            batch_size = batch_size,\n",
    "            max_epochs = epochs,\n",
    "            train_split=skorch.dataset.ValidSplit(((train_indices, val_indices),)),\n",
    "            callbacks=[\n",
    "                (\"balanced_accuracy\", skorch.callbacks.EpochScoring(\"balanced_accuracy\", lower_is_better=False)),\n",
    "                (\"accuracy\", skorch.callbacks.EpochScoring(\"accuracy\", lower_is_better=False)),\n",
    "                (\"duration\", skorch.callbacks.EpochTimer()),\n",
    "                EpochScoring(scoring='accuracy', name='train_acc', on_train=True)\n",
    "            ],\n",
    "            optimizer__lr=1e-4,\n",
    "            optimizer__weight_decay=1e-4\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    accuracy    balanced_accuracy    train_acc    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  -------------------  -----------  ------------  -----------  ------------  ------\n",
      "      1      \u001b[36m0.1011\u001b[0m               \u001b[32m0.1000\u001b[0m       \u001b[35m0.0901\u001b[0m        \u001b[31m2.3163\u001b[0m       \u001b[94m0.1011\u001b[0m        \u001b[36m2.3067\u001b[0m  1.5911\n",
      "      2      0.1011               0.1000       \u001b[35m0.0891\u001b[0m        \u001b[31m2.3056\u001b[0m       0.1011        \u001b[36m2.3029\u001b[0m  1.5038\n",
      "      3      0.0993               0.1000       0.0910        \u001b[31m2.3028\u001b[0m       0.0993        \u001b[36m2.3014\u001b[0m  1.5049\n",
      "      4      \u001b[36m0.1030\u001b[0m               \u001b[32m0.1019\u001b[0m       0.1060        \u001b[31m2.3018\u001b[0m       \u001b[94m0.1030\u001b[0m        \u001b[36m2.3006\u001b[0m  1.5049\n",
      "      5      0.1011               0.1000       0.1004        \u001b[31m2.3012\u001b[0m       0.1011        \u001b[36m2.2998\u001b[0m  1.5055\n",
      "      6      \u001b[36m0.1292\u001b[0m               \u001b[32m0.1278\u001b[0m       0.1013        \u001b[31m2.3011\u001b[0m       \u001b[94m0.1292\u001b[0m        \u001b[36m2.2986\u001b[0m  1.5028\n",
      "      7      \u001b[36m0.2434\u001b[0m               \u001b[32m0.2422\u001b[0m       0.1126        \u001b[31m2.2988\u001b[0m       \u001b[94m0.2434\u001b[0m        \u001b[36m2.2971\u001b[0m  1.5022\n",
      "      8      \u001b[36m0.2734\u001b[0m               \u001b[32m0.2730\u001b[0m       0.1257        \u001b[31m2.2970\u001b[0m       \u001b[94m0.2734\u001b[0m        \u001b[36m2.2913\u001b[0m  1.5024\n",
      "      9      \u001b[36m0.2978\u001b[0m               \u001b[32m0.2977\u001b[0m       0.1923        \u001b[31m2.2830\u001b[0m       \u001b[94m0.2978\u001b[0m        \u001b[36m2.2604\u001b[0m  1.5031\n",
      "     10      \u001b[36m0.3015\u001b[0m               \u001b[32m0.3005\u001b[0m       0.2598        \u001b[31m2.2276\u001b[0m       \u001b[94m0.3015\u001b[0m        \u001b[36m2.1601\u001b[0m  1.5046\n",
      "     11      \u001b[36m0.3127\u001b[0m               \u001b[32m0.3117\u001b[0m       0.2795        \u001b[31m2.0970\u001b[0m       \u001b[94m0.3127\u001b[0m        \u001b[36m1.9737\u001b[0m  1.5038\n",
      "     12      \u001b[36m0.3483\u001b[0m               \u001b[32m0.3474\u001b[0m       0.2983        \u001b[31m1.9072\u001b[0m       \u001b[94m0.3483\u001b[0m        \u001b[36m1.7605\u001b[0m  1.5041\n",
      "     13      \u001b[36m0.4345\u001b[0m               \u001b[32m0.4337\u001b[0m       0.3490        \u001b[31m1.7253\u001b[0m       \u001b[94m0.4345\u001b[0m        \u001b[36m1.5966\u001b[0m  1.5061\n",
      "     14      \u001b[36m0.4738\u001b[0m               \u001b[32m0.4734\u001b[0m       0.4053        \u001b[31m1.5920\u001b[0m       \u001b[94m0.4738\u001b[0m        \u001b[36m1.4570\u001b[0m  1.5058\n",
      "     15      \u001b[36m0.4925\u001b[0m               \u001b[32m0.4928\u001b[0m       0.4296        \u001b[31m1.4734\u001b[0m       \u001b[94m0.4925\u001b[0m        \u001b[36m1.3372\u001b[0m  1.5040\n",
      "     16      \u001b[36m0.5000\u001b[0m               \u001b[32m0.5003\u001b[0m       0.4343        \u001b[31m1.3854\u001b[0m       \u001b[94m0.5000\u001b[0m        \u001b[36m1.2514\u001b[0m  1.5046\n",
      "     17      \u001b[36m0.5393\u001b[0m               \u001b[32m0.5399\u001b[0m       0.4681        \u001b[31m1.3098\u001b[0m       \u001b[94m0.5393\u001b[0m        \u001b[36m1.1919\u001b[0m  1.5035\n",
      "     18      0.5375               0.5377       0.4831        \u001b[31m1.2466\u001b[0m       0.5375        \u001b[36m1.1357\u001b[0m  1.5037\n",
      "     19      \u001b[36m0.5412\u001b[0m               \u001b[32m0.5418\u001b[0m       0.5084        \u001b[31m1.2121\u001b[0m       \u001b[94m0.5412\u001b[0m        \u001b[36m1.1195\u001b[0m  1.5047\n",
      "     20      \u001b[36m0.5730\u001b[0m               \u001b[32m0.5737\u001b[0m       0.5197        \u001b[31m1.1717\u001b[0m       \u001b[94m0.5730\u001b[0m        \u001b[36m1.0747\u001b[0m  1.5050\n",
      "     21      \u001b[36m0.5768\u001b[0m               \u001b[32m0.5774\u001b[0m       0.5432        \u001b[31m1.1610\u001b[0m       \u001b[94m0.5768\u001b[0m        \u001b[36m1.0523\u001b[0m  1.5052\n",
      "     22      \u001b[36m0.5861\u001b[0m               \u001b[32m0.5869\u001b[0m       0.5300        \u001b[31m1.1187\u001b[0m       \u001b[94m0.5861\u001b[0m        \u001b[36m1.0361\u001b[0m  1.5039\n",
      "     23      \u001b[36m0.5993\u001b[0m               \u001b[32m0.6000\u001b[0m       0.5582        \u001b[31m1.1011\u001b[0m       \u001b[94m0.5993\u001b[0m        \u001b[36m1.0189\u001b[0m  1.5041\n",
      "     24      \u001b[36m0.6105\u001b[0m               \u001b[32m0.6109\u001b[0m       0.5647        \u001b[31m1.0836\u001b[0m       \u001b[94m0.6105\u001b[0m        \u001b[36m0.9858\u001b[0m  1.5053\n",
      "     25      0.6086               0.6090       0.5694        \u001b[31m1.0456\u001b[0m       0.6086        \u001b[36m0.9777\u001b[0m  1.5035\n",
      "     26      \u001b[36m0.6161\u001b[0m               \u001b[32m0.6168\u001b[0m       0.5901        \u001b[31m1.0391\u001b[0m       \u001b[94m0.6161\u001b[0m        \u001b[36m0.9690\u001b[0m  1.5047\n",
      "     27      \u001b[36m0.6199\u001b[0m               \u001b[32m0.6201\u001b[0m       0.5835        \u001b[31m1.0139\u001b[0m       \u001b[94m0.6199\u001b[0m        \u001b[36m0.9436\u001b[0m  1.5056\n",
      "     28      \u001b[36m0.6330\u001b[0m               \u001b[32m0.6334\u001b[0m       0.5966        \u001b[31m0.9885\u001b[0m       \u001b[94m0.6330\u001b[0m        \u001b[36m0.9369\u001b[0m  1.5048\n",
      "     29      \u001b[36m0.6367\u001b[0m               \u001b[32m0.6370\u001b[0m       0.5788        \u001b[31m0.9871\u001b[0m       \u001b[94m0.6367\u001b[0m        \u001b[36m0.9247\u001b[0m  1.5043\n",
      "     30      \u001b[36m0.6423\u001b[0m               \u001b[32m0.6425\u001b[0m       0.6079        \u001b[31m0.9652\u001b[0m       \u001b[94m0.6423\u001b[0m        \u001b[36m0.9051\u001b[0m  1.5061\n",
      "     31      \u001b[36m0.6554\u001b[0m               \u001b[32m0.6557\u001b[0m       0.6126        \u001b[31m0.9496\u001b[0m       \u001b[94m0.6554\u001b[0m        \u001b[36m0.8899\u001b[0m  1.5056\n",
      "     32      0.6461               0.6462       0.6201        \u001b[31m0.9353\u001b[0m       0.6461        \u001b[36m0.8862\u001b[0m  1.5041\n",
      "     33      \u001b[36m0.6610\u001b[0m               \u001b[32m0.6613\u001b[0m       0.6276        \u001b[31m0.9349\u001b[0m       \u001b[94m0.6610\u001b[0m        \u001b[36m0.8698\u001b[0m  1.5049\n",
      "     34      0.6536               0.6537       0.6285        \u001b[31m0.9148\u001b[0m       0.6536        0.8716  1.5044\n",
      "     35      \u001b[36m0.6704\u001b[0m               \u001b[32m0.6703\u001b[0m       0.6126        0.9300       \u001b[94m0.6704\u001b[0m        \u001b[36m0.8540\u001b[0m  1.5041\n",
      "     36      \u001b[36m0.6779\u001b[0m               \u001b[32m0.6778\u001b[0m       0.6548        \u001b[31m0.8810\u001b[0m       \u001b[94m0.6779\u001b[0m        \u001b[36m0.8384\u001b[0m  1.5050\n",
      "     37      0.6779               \u001b[32m0.6779\u001b[0m       0.6557        \u001b[31m0.8687\u001b[0m       0.6779        \u001b[36m0.8366\u001b[0m  1.5049\n",
      "     38      \u001b[36m0.6910\u001b[0m               \u001b[32m0.6911\u001b[0m       0.6501        0.8695       \u001b[94m0.6910\u001b[0m        \u001b[36m0.8337\u001b[0m  1.5052\n",
      "     39      \u001b[36m0.6948\u001b[0m               \u001b[32m0.6949\u001b[0m       0.6492        0.8825       \u001b[94m0.6948\u001b[0m        \u001b[36m0.8292\u001b[0m  1.5036\n",
      "     40      0.6948               0.6948       0.6341        0.8715       0.6948        \u001b[36m0.8182\u001b[0m  1.5057\n",
      "     41      \u001b[36m0.7041\u001b[0m               \u001b[32m0.7043\u001b[0m       0.6520        \u001b[31m0.8545\u001b[0m       \u001b[94m0.7041\u001b[0m        \u001b[36m0.8168\u001b[0m  1.5033\n",
      "     42      0.6929               0.6928       0.6595        \u001b[31m0.8338\u001b[0m       0.6929        \u001b[36m0.8134\u001b[0m  1.5034\n",
      "     43      \u001b[36m0.7060\u001b[0m               \u001b[32m0.7061\u001b[0m       0.6632        \u001b[31m0.8336\u001b[0m       \u001b[94m0.7060\u001b[0m        \u001b[36m0.7980\u001b[0m  1.5035\n",
      "     44      \u001b[36m0.7079\u001b[0m               \u001b[32m0.7080\u001b[0m       0.6754        \u001b[31m0.8129\u001b[0m       \u001b[94m0.7079\u001b[0m        \u001b[36m0.7904\u001b[0m  1.5038\n",
      "     45      \u001b[36m0.7172\u001b[0m               \u001b[32m0.7175\u001b[0m       0.6961        \u001b[31m0.8024\u001b[0m       \u001b[94m0.7172\u001b[0m        0.7905  1.5031\n",
      "     46      0.7079               0.7081       0.6848        \u001b[31m0.7885\u001b[0m       0.7079        \u001b[36m0.7792\u001b[0m  1.5034\n",
      "     47      \u001b[36m0.7228\u001b[0m               \u001b[32m0.7232\u001b[0m       0.6932        \u001b[31m0.7681\u001b[0m       \u001b[94m0.7228\u001b[0m        \u001b[36m0.7748\u001b[0m  1.5041\n",
      "     48      0.7228               0.7228       0.6848        0.7813       0.7228        \u001b[36m0.7653\u001b[0m  1.5046\n",
      "     49      \u001b[36m0.7378\u001b[0m               \u001b[32m0.7378\u001b[0m       0.6979        0.7871       \u001b[94m0.7378\u001b[0m        \u001b[36m0.7527\u001b[0m  1.5040\n",
      "     50      \u001b[36m0.7434\u001b[0m               \u001b[32m0.7434\u001b[0m       0.7054        \u001b[31m0.7661\u001b[0m       \u001b[94m0.7434\u001b[0m        \u001b[36m0.7477\u001b[0m  1.5047\n",
      "     51      \u001b[36m0.7491\u001b[0m               \u001b[32m0.7491\u001b[0m       0.7148        \u001b[31m0.7389\u001b[0m       \u001b[94m0.7491\u001b[0m        \u001b[36m0.7326\u001b[0m  1.5043\n",
      "     52      \u001b[36m0.7566\u001b[0m               \u001b[32m0.7566\u001b[0m       0.7289        \u001b[31m0.7289\u001b[0m       \u001b[94m0.7566\u001b[0m        \u001b[36m0.7279\u001b[0m  1.5037\n",
      "     53      \u001b[36m0.7772\u001b[0m               \u001b[32m0.7775\u001b[0m       0.7214        \u001b[31m0.7073\u001b[0m       \u001b[94m0.7772\u001b[0m        \u001b[36m0.7142\u001b[0m  1.5038\n",
      "     54      0.7622               0.7625       0.7392        0.7110       0.7622        \u001b[36m0.7018\u001b[0m  1.5050\n",
      "     55      0.7753               0.7757       0.7373        0.7085       0.7753        \u001b[36m0.6964\u001b[0m  1.5037\n",
      "     56      \u001b[36m0.7865\u001b[0m               \u001b[32m0.7868\u001b[0m       0.7505        \u001b[31m0.6874\u001b[0m       \u001b[94m0.7865\u001b[0m        \u001b[36m0.6766\u001b[0m  1.5042\n",
      "     57      \u001b[36m0.7940\u001b[0m               \u001b[32m0.7944\u001b[0m       0.7467        \u001b[31m0.6744\u001b[0m       \u001b[94m0.7940\u001b[0m        \u001b[36m0.6684\u001b[0m  1.5040\n",
      "     58      \u001b[36m0.8034\u001b[0m               \u001b[32m0.8036\u001b[0m       0.7645        \u001b[31m0.6430\u001b[0m       \u001b[94m0.8034\u001b[0m        \u001b[36m0.6434\u001b[0m  1.5042\n",
      "     59      0.7996               0.8000       0.7617        0.6434       0.7996        0.6494  1.5034\n",
      "     60      0.7978               0.7983       0.7711        \u001b[31m0.6304\u001b[0m       0.7978        0.6461  1.5031\n",
      "     61      \u001b[36m0.8165\u001b[0m               \u001b[32m0.8168\u001b[0m       0.7777        \u001b[31m0.6297\u001b[0m       \u001b[94m0.8165\u001b[0m        \u001b[36m0.6281\u001b[0m  1.5028\n",
      "     62      0.8052               0.8056       0.7861        \u001b[31m0.6033\u001b[0m       0.8052        0.6347  1.5029\n",
      "     63      0.8127               0.8132       0.7589        0.6264       0.8127        \u001b[36m0.6272\u001b[0m  1.5035\n",
      "     64      \u001b[36m0.8184\u001b[0m               \u001b[32m0.8185\u001b[0m       0.7786        0.6068       \u001b[94m0.8184\u001b[0m        \u001b[36m0.6153\u001b[0m  1.5033\n",
      "     65      0.8184               \u001b[32m0.8186\u001b[0m       0.7824        0.6107       0.8184        \u001b[36m0.6103\u001b[0m  1.5037\n",
      "     66      0.8127               0.8130       0.7758        \u001b[31m0.6010\u001b[0m       0.8127        0.6106  1.5032\n",
      "     67      \u001b[36m0.8202\u001b[0m               \u001b[32m0.8206\u001b[0m       0.7795        \u001b[31m0.5808\u001b[0m       \u001b[94m0.8202\u001b[0m        \u001b[36m0.6036\u001b[0m  1.5036\n",
      "     68      0.8127               0.8129       0.7927        0.5849       0.8127        0.6058  1.5026\n",
      "     69      0.8202               0.8204       0.7852        \u001b[31m0.5684\u001b[0m       0.8202        \u001b[36m0.5985\u001b[0m  1.5035\n",
      "     70      0.8146               0.8149       0.7936        0.5794       0.8146        0.6023  1.5031\n",
      "     71      0.8165               0.8167       0.7974        0.5769       0.8165        \u001b[36m0.5920\u001b[0m  1.5034\n",
      "     72      \u001b[36m0.8221\u001b[0m               \u001b[32m0.8223\u001b[0m       0.7899        \u001b[31m0.5566\u001b[0m       \u001b[94m0.8221\u001b[0m        \u001b[36m0.5766\u001b[0m  1.5033\n",
      "     73      \u001b[36m0.8258\u001b[0m               \u001b[32m0.8261\u001b[0m       0.8058        \u001b[31m0.5503\u001b[0m       \u001b[94m0.8258\u001b[0m        \u001b[36m0.5748\u001b[0m  1.5029\n",
      "     74      0.8146               0.8149       0.8189        \u001b[31m0.5425\u001b[0m       0.8146        0.5755  1.5036\n",
      "     75      \u001b[36m0.8296\u001b[0m               \u001b[32m0.8299\u001b[0m       0.8039        \u001b[31m0.5380\u001b[0m       \u001b[94m0.8296\u001b[0m        \u001b[36m0.5663\u001b[0m  1.5035\n",
      "     76      0.8221               0.8223       0.7861        0.5790       0.8221        \u001b[36m0.5649\u001b[0m  1.5040\n",
      "     77      0.8202               0.8203       0.8133        \u001b[31m0.5218\u001b[0m       0.8202        \u001b[36m0.5638\u001b[0m  1.5033\n",
      "     78      0.8258               0.8260       0.8058        0.5381       0.8258        \u001b[36m0.5573\u001b[0m  1.5032\n",
      "     79      0.8258               0.8260       0.8011        0.5343       0.8258        \u001b[36m0.5510\u001b[0m  1.5028\n",
      "     80      0.8240               0.8242       0.8218        \u001b[31m0.5068\u001b[0m       0.8240        \u001b[36m0.5496\u001b[0m  1.5031\n",
      "     81      \u001b[36m0.8333\u001b[0m               \u001b[32m0.8336\u001b[0m       0.8349        0.5095       \u001b[94m0.8333\u001b[0m        \u001b[36m0.5481\u001b[0m  1.5030\n",
      "     82      0.8277               0.8279       0.8265        0.5146       0.8277        \u001b[36m0.5431\u001b[0m  1.5032\n",
      "     83      0.8277               0.8279       0.8255        \u001b[31m0.4829\u001b[0m       0.8277        \u001b[36m0.5429\u001b[0m  1.5028\n",
      "     84      0.8296               0.8298       0.8208        \u001b[31m0.4778\u001b[0m       0.8296        \u001b[36m0.5319\u001b[0m  1.5030\n",
      "     85      0.8296               0.8298       0.8340        0.4899       0.8296        \u001b[36m0.5312\u001b[0m  1.5024\n",
      "     86      0.8184               0.8185       0.8265        \u001b[31m0.4693\u001b[0m       0.8184        0.5417  1.5032\n",
      "     87      0.8315               0.8317       0.8302        \u001b[31m0.4664\u001b[0m       0.8315        0.5359  1.5030\n",
      "     88      \u001b[36m0.8371\u001b[0m               \u001b[32m0.8372\u001b[0m       0.8377        \u001b[31m0.4583\u001b[0m       \u001b[94m0.8371\u001b[0m        0.5332  1.5025\n",
      "     89      0.8333               0.8335       0.8433        0.4785       0.8333        \u001b[36m0.5236\u001b[0m  1.5036\n",
      "     90      \u001b[36m0.8390\u001b[0m               \u001b[32m0.8393\u001b[0m       0.8302        \u001b[31m0.4533\u001b[0m       \u001b[94m0.8390\u001b[0m        \u001b[36m0.5167\u001b[0m  1.5028\n",
      "     91      0.8277               0.8280       0.8499        \u001b[31m0.4376\u001b[0m       0.8277        0.5230  1.5030\n",
      "     92      0.8371               0.8372       0.8265        0.4677       0.8371        \u001b[36m0.5088\u001b[0m  1.5026\n",
      "     93      \u001b[36m0.8446\u001b[0m               \u001b[32m0.8447\u001b[0m       0.8462        \u001b[31m0.4371\u001b[0m       \u001b[94m0.8446\u001b[0m        0.5155  1.5036\n",
      "     94      0.8446               0.8447       0.8368        0.4551       0.8446        0.5095  1.5031\n",
      "     95      \u001b[36m0.8483\u001b[0m               \u001b[32m0.8485\u001b[0m       0.8499        0.4539       \u001b[94m0.8483\u001b[0m        0.5107  1.5029\n",
      "     96      0.8408               0.8410       0.8433        0.4420       0.8408        \u001b[36m0.5072\u001b[0m  1.5030\n",
      "     97      0.8408               0.8410       0.8659        \u001b[31m0.4185\u001b[0m       0.8408        \u001b[36m0.5053\u001b[0m  1.5038\n",
      "     98      0.8464               0.8465       0.8386        0.4440       0.8464        \u001b[36m0.4992\u001b[0m  1.5033\n",
      "     99      0.8483               0.8484       0.8415        0.4358       0.8483        0.5028  1.5030\n",
      "    100      0.8427               0.8428       0.8499        0.4315       0.8427        0.5036  1.5035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nTraining and validation\\n\\n\\nmodel = model.fit(X={\\n        \"x_numerical\": X_train[:, :n_numerical].astype(np.float32),\\n        \"x_categorical\": X_train[:, n_numerical:].astype(np.int32)\\n        }, \\n        y=y_train.astype(np.int64)\\n    )\\n    \\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.fit(X={\n",
    "        \"x_numerical\": X_train[:, :n_numerical].astype(np.float32),\n",
    "        \"x_categorical\": X_train[:, n_numerical:].astype(np.int32)\n",
    "        }, \n",
    "        y=y_train.astype(np.int64)\n",
    "    )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Training and validation\n",
    "\n",
    "\n",
    "model = model.fit(X={\n",
    "        \"x_numerical\": X_train[:, :n_numerical].astype(np.float32),\n",
    "        \"x_categorical\": X_train[:, n_numerical:].astype(np.int32)\n",
    "        }, \n",
    "        y=y_train.astype(np.int64)\n",
    "    )\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING\n",
    "predictions = model.predict_proba(X={\n",
    "                \"x_numerical\": X_test[:, :n_numerical].astype(np.float32),\n",
    "                \"x_categorical\": X_test[:, n_numerical:].astype(np.int32)\n",
    "                }\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results:\n",
      "\n",
      "{'balanced_accuracy': 0.85, 'accuracy': 0.85, 'log_loss': 0.43292165051158443}\n"
     ]
    }
   ],
   "source": [
    "print(\"Test results:\\n\")\n",
    "print(evaluating.get_default_scores(y_test.astype(np.int64), predictions, multiclass = True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabtrans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
