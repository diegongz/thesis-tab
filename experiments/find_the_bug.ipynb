{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/diego/Git/thesis-tabtrans')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import training, callback, evaluating, attention, data\n",
    "from sklearn import datasets, model_selection\n",
    "import skorch\n",
    "import pandas as pd\n",
    "import openml\n",
    "from skorch.callbacks import Checkpoint, EarlyStopping, LoadInitState, EpochScoring, Checkpoint, TrainEndCheckpoint\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder #to create one hot encoding for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:openml.datasets.dataset:pickle write arrhythmia\n"
     ]
    }
   ],
   "source": [
    "task_id = 233092\n",
    "task = openml.tasks.get_task(task_id)  \n",
    "dataset_id = task.dataset_id\n",
    "df = data.read_dataset_by_id(dataset_id) #this function returns a dictionary with the dataset's data and metadata\n",
    "\n",
    "X = df[\"features\"] #features\n",
    "y = df[\"outputs\"].codes #outputs\n",
    "\n",
    "categorical_features = df['categorical'].tolist() #name of the categorical features\n",
    "numerical_features = df['numerical'].tolist() #name of the numerical features\n",
    "\n",
    "# Create numerical and categorical datasets\n",
    "X_categorical = X[categorical_features]  # Categorical features\n",
    "X_numerical = X[numerical_features]     # Numerical features\n",
    "\n",
    "# Filter out categorical columns with only one unique value\n",
    "redundant_columns = [col for col in X_categorical.columns if X_categorical[col].nunique() <= 1]\n",
    "X_categorical = X_categorical.drop(columns=redundant_columns)\n",
    "\n",
    "# Recompute categorical features after filtering\n",
    "categorical_features = [col for col in categorical_features if col not in redundant_columns]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a LabelEncoder object\n",
    "le = LabelEncoder()\n",
    "for col in X_categorical.columns:\n",
    "    X_categorical[col] = le.fit_transform(X_categorical[col].astype(str))\n",
    "\n",
    "\n",
    "X_ordered = pd.concat([X_numerical, X_categorical], axis=1)\n",
    "\n",
    "n_instances = X_ordered.shape[0]\n",
    "n_numerical = X_numerical.shape[1]\n",
    "n_categories = [X_categorical[col].nunique() for col in X_categorical.columns] #list that tells the number of categories for each categorical feature\n",
    "#n_categories_2 = df[\"n_categorical\"] #this one is from the metadata\n",
    "n_labels = len(df[\"labels\"].keys()) #number of labels\n",
    "\n",
    "seed = 11\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.20, random_state= seed, stratify=y)\n",
    "\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "\n",
    "\n",
    "train_indices, val_indices = model_selection.train_test_split(np.arange(X_train.shape[0]), test_size=1/3, stratify=y_train) #1/3 of train is equal to 20% of total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 0 (sex) has 2 categories\n",
      "Feature 0 (sex) max index: 1 (should be < 2)\n",
      "Feature 1 (chDI_RRwaveExists) has 2 categories\n",
      "Feature 1 (chDI_RRwaveExists) max index: 1 (should be < 2)\n",
      "Feature 2 (chDI_DD_RRwaveExists) has 2 categories\n",
      "Feature 2 (chDI_DD_RRwaveExists) max index: 1 (should be < 2)\n",
      "Feature 3 (chDI_RPwaveExists) has 2 categories\n",
      "Feature 3 (chDI_RPwaveExists) max index: 1 (should be < 2)\n",
      "Feature 4 (chDI_DD_RPwaveExists) has 2 categories\n",
      "Feature 4 (chDI_DD_RPwaveExists) max index: 1 (should be < 2)\n",
      "Feature 5 (chDI_RTwaveExists) has 2 categories\n",
      "Feature 5 (chDI_RTwaveExists) max index: 1 (should be < 2)\n",
      "Feature 6 (chDI_DD_RTwaveExists) has 2 categories\n",
      "Feature 6 (chDI_DD_RTwaveExists) max index: 1 (should be < 2)\n",
      "Feature 7 (chDII_RRwaveExists) has 2 categories\n",
      "Feature 7 (chDII_RRwaveExists) max index: 1 (should be < 2)\n",
      "Feature 8 (chDII_DD_RRwaveExists) has 2 categories\n",
      "Feature 8 (chDII_DD_RRwaveExists) max index: 1 (should be < 2)\n",
      "Feature 9 (chDII_RPwaveExists) has 2 categories\n",
      "Feature 9 (chDII_RPwaveExists) max index: 1 (should be < 2)\n",
      "Feature 10 (chDII_DD_RPwaveExists) has 2 categories\n",
      "Feature 10 (chDII_DD_RPwaveExists) max index: 1 (should be < 2)\n",
      "Feature 11 (chDII_RTwaveExists) has 2 categories\n",
      "Feature 11 (chDII_RTwaveExists) max index: 1 (should be < 2)\n",
      "Feature 12 (chDII_DD_RTwaveExists) has 2 categories\n",
      "Feature 12 (chDII_DD_RTwaveExists) max index: 1 (should be < 2)\n",
      "Feature 13 (chDIII_RRwaveExists) has 2 categories\n",
      "Feature 13 (chDIII_RRwaveExists) max index: 1 (should be < 2)\n",
      "Feature 14 (chDIII_DD_RRwaveExists) has 2 categories\n",
      "Feature 14 (chDIII_DD_RRwaveExists) max index: 1 (should be < 2)\n",
      "Feature 15 (chDIII_RPwaveExists) has 2 categories\n",
      "Feature 15 (chDIII_RPwaveExists) max index: 1 (should be < 2)\n",
      "Feature 16 (chDIII_DD_RPwaveExists) has 2 categories\n",
      "Feature 16 (chDIII_DD_RPwaveExists) max index: 1 (should be < 2)\n",
      "Feature 17 (chDIII_RTwaveExists) has 2 categories\n",
      "Feature 17 (chDIII_RTwaveExists) max index: 1 (should be < 2)\n",
      "Feature 18 (chDIII_DD_RTwaveExists) has 2 categories\n",
      "Feature 18 (chDIII_DD_RTwaveExists) max index: 1 (should be < 2)\n",
      "Feature 19 (chAVR_RRwaveExists) has 2 categories\n",
      "Feature 19 (chAVR_RRwaveExists) max index: 1 (should be < 2)\n",
      "Feature 20 (chAVR_DD_RRwaveExists) has 2 categories\n",
      "Feature 20 (chAVR_DD_RRwaveExists) max index: 1 (should be < 2)\n",
      "Feature 21 (chAVR_RPwaveExists) has 2 categories\n",
      "Feature 21 (chAVR_RPwaveExists) max index: 1 (should be < 2)\n",
      "Feature 22 (chAVR_DD_RPwaveExists) has 2 categories\n",
      "Feature 22 (chAVR_DD_RPwaveExists) max index: 1 (should be < 2)\n",
      "Feature 23 (chAVR_RTwaveExists) has 2 categories\n",
      "Feature 23 (chAVR_RTwaveExists) max index: 1 (should be < 2)\n",
      "Feature 24 (chAVR_DD_RTwaveExists) has 2 categories\n",
      "Feature 24 (chAVR_DD_RTwaveExists) max index: 1 (should be < 2)\n",
      "Feature 25 (chAVL_DD_RRwaveExists) has 2 categories\n",
      "Feature 25 (chAVL_DD_RRwaveExists) max index: 1 (should be < 2)\n",
      "Feature 26 (chAVL_RPwaveExists) has 2 categories\n",
      "Feature 26 (chAVL_RPwaveExists) max index: 1 (should be < 2)\n",
      "Feature 27 (chAVL_DD_RPwaveExists) has 2 categories\n",
      "Feature 27 (chAVL_DD_RPwaveExists) max index: 1 (should be < 2)\n",
      "Feature 28 (chAVL_RTwaveExists) has 2 categories\n",
      "Feature 28 (chAVL_RTwaveExists) max index: 1 (should be < 2)\n",
      "Feature 29 (chAVL_DD_RTwaveExists) has 2 categories\n",
      "Feature 29 (chAVL_DD_RTwaveExists) max index: 1 (should be < 2)\n",
      "Feature 30 (chAVF_RRwaveExists) has 2 categories\n",
      "Feature 30 (chAVF_RRwaveExists) max index: 1 (should be < 2)\n",
      "Feature 31 (chAVF_DD_RRwaveExists) has 2 categories\n",
      "Feature 31 (chAVF_DD_RRwaveExists) max index: 1 (should be < 2)\n",
      "Feature 32 (chAVF_DD_RPwaveExists) has 2 categories\n",
      "Feature 32 (chAVF_DD_RPwaveExists) max index: 1 (should be < 2)\n",
      "Feature 33 (chAVF_RTwaveExists) has 2 categories\n",
      "Feature 33 (chAVF_RTwaveExists) max index: 1 (should be < 2)\n",
      "Feature 34 (chAVF_DD_RTwaveExists) has 2 categories\n",
      "Feature 34 (chAVF_DD_RTwaveExists) max index: 1 (should be < 2)\n",
      "Feature 35 (chV1_RRwaveExists) has 2 categories\n",
      "Feature 35 (chV1_RRwaveExists) max index: 1 (should be < 2)\n",
      "Feature 36 (chV1_DD_RRwaveExists) has 2 categories\n",
      "Feature 36 (chV1_DD_RRwaveExists) max index: 1 (should be < 2)\n",
      "Feature 37 (chV1_RPwaveExists) has 2 categories\n",
      "Feature 37 (chV1_RPwaveExists) max index: 1 (should be < 2)\n",
      "Feature 38 (chV1_DD_RPwaveExists) has 2 categories\n",
      "Feature 38 (chV1_DD_RPwaveExists) max index: 1 (should be < 2)\n",
      "Feature 39 (chV1_RTwaveExists) has 2 categories\n",
      "Feature 39 (chV1_RTwaveExists) max index: 1 (should be < 2)\n",
      "Feature 40 (chV1_DD_RTwaveExists) has 2 categories\n",
      "Feature 40 (chV1_DD_RTwaveExists) max index: 1 (should be < 2)\n",
      "Feature 41 (chV2_RRwaveExists) has 2 categories\n",
      "Feature 41 (chV2_RRwaveExists) max index: 1 (should be < 2)\n",
      "Feature 42 (chV2_DD_RRwaveExists) has 2 categories\n",
      "Feature 42 (chV2_DD_RRwaveExists) max index: 1 (should be < 2)\n",
      "Feature 43 (chV2_RPwaveExists) has 2 categories\n",
      "Feature 43 (chV2_RPwaveExists) max index: 1 (should be < 2)\n",
      "Feature 44 (chV2_DD_RPwaveExists) has 2 categories\n",
      "Feature 44 (chV2_DD_RPwaveExists) max index: 1 (should be < 2)\n",
      "Feature 45 (chV2_RTwaveExists) has 2 categories\n",
      "Feature 45 (chV2_RTwaveExists) max index: 1 (should be < 2)\n",
      "Feature 46 (chV2_DD_RTwaveExists) has 2 categories\n",
      "Feature 46 (chV2_DD_RTwaveExists) max index: 1 (should be < 2)\n",
      "Feature 47 (chV3_RRwaveExists) has 2 categories\n",
      "Feature 47 (chV3_RRwaveExists) max index: 1 (should be < 2)\n",
      "Feature 48 (chV3_DD_RRwaveExists) has 2 categories\n",
      "Feature 48 (chV3_DD_RRwaveExists) max index: 1 (should be < 2)\n",
      "Feature 49 (chV3_RPwaveExists) has 2 categories\n",
      "Feature 49 (chV3_RPwaveExists) max index: 1 (should be < 2)\n",
      "Feature 50 (chV3_DD_RPwaveExists) has 2 categories\n",
      "Feature 50 (chV3_DD_RPwaveExists) max index: 1 (should be < 2)\n",
      "Feature 51 (chV3_RTwaveExists) has 2 categories\n",
      "Feature 51 (chV3_RTwaveExists) max index: 1 (should be < 2)\n",
      "Feature 52 (chV3_DD_RTwaveExists) has 2 categories\n",
      "Feature 52 (chV3_DD_RTwaveExists) max index: 1 (should be < 2)\n",
      "Feature 53 (chV4_RRwaveExists) has 2 categories\n",
      "Feature 53 (chV4_RRwaveExists) max index: 1 (should be < 2)\n",
      "Feature 54 (chV4_DD_RRwaveExists) has 2 categories\n",
      "Feature 54 (chV4_DD_RRwaveExists) max index: 1 (should be < 2)\n",
      "Feature 55 (chV4_RTwaveExists) has 2 categories\n",
      "Feature 55 (chV4_RTwaveExists) max index: 1 (should be < 2)\n",
      "Feature 56 (chV4_DD_RTwaveExists) has 2 categories\n",
      "Feature 56 (chV4_DD_RTwaveExists) max index: 1 (should be < 2)\n",
      "Feature 57 (chV5_DD_RRwaveExists) has 2 categories\n",
      "Feature 57 (chV5_DD_RRwaveExists) max index: 1 (should be < 2)\n",
      "Feature 58 (chV5_DD_RPwaveExists) has 2 categories\n",
      "Feature 58 (chV5_DD_RPwaveExists) max index: 1 (should be < 2)\n",
      "Feature 59 (chV5_DD_RTwaveExists) has 2 categories\n",
      "Feature 59 (chV5_DD_RTwaveExists) max index: 1 (should be < 2)\n",
      "Feature 60 (chV6_RRwaveExists) has 2 categories\n",
      "Feature 60 (chV6_RRwaveExists) max index: 1 (should be < 2)\n",
      "Feature 61 (chV6_DD_RRwaveExists) has 2 categories\n",
      "Feature 61 (chV6_DD_RRwaveExists) max index: 1 (should be < 2)\n",
      "Feature 62 (chV6_RPwaveExists) has 2 categories\n",
      "Feature 62 (chV6_RPwaveExists) max index: 1 (should be < 2)\n",
      "Feature 63 (chV6_DD_RTwaveExists) has 2 categories\n",
      "Feature 63 (chV6_DD_RTwaveExists) max index: 1 (should be < 2)\n"
     ]
    }
   ],
   "source": [
    "n_heads = 4 # In average 4 works better\n",
    "embed_dim = 128 # In average 256 works better\n",
    "n_layers = 4\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "\n",
    "#parameters for the model\n",
    "ff_pw_size = 30  #this value because of the paper \n",
    "attn_dropout = 0.3 #paper\n",
    "ff_dropout = 0.1 #paper value\n",
    "aggregator = \"cls\"\n",
    "aggregator_parameters = None\n",
    "decoder_hidden_units = [128,64] #paper value\n",
    "decoder_activation_fn = nn.ReLU()\n",
    "need_weights = False\n",
    "numerical_passthrough = False\n",
    "\n",
    "for i, n_cat in enumerate(n_categories):\n",
    "    print(f\"Feature {i} ({X_categorical.columns[i]}) has {n_cat} categories\")\n",
    "    max_index = X_ordered[X_categorical.columns[i]].max()\n",
    "    print(f\"Feature {i} ({X_categorical.columns[i]}) max index: {max_index} (should be < {n_cat})\")\n",
    "    if max_index >= n_cat:\n",
    "        print(f\"Warning: Feature {i} ({X_categorical.columns[i]}) has indices out of range!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego/anaconda3/envs/tabtrans/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer was not TransformerEncoderLayer\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Building PyTorch module.\n",
    "\n",
    "We provide a wrapper function for building the PyTorch module.\n",
    "The function is utils.training.build_module.\n",
    "\"\"\"\n",
    "module = training.build_module(\n",
    "    n_categories, # List of number of categories\n",
    "    n_numerical, # Number of numerical features\n",
    "    n_heads, # Number of heads per layer\n",
    "    ff_pw_size, # Size of the MLP inside each transformer encoder layer\n",
    "    n_layers, # Number of transformer encoder layers    \n",
    "    n_labels, # Number of output neurons\n",
    "    embed_dim,\n",
    "    attn_dropout, \n",
    "    ff_dropout, \n",
    "    aggregator, # The aggregator for output vectors before decoder\n",
    "    rnn_aggregator_parameters=aggregator_parameters,\n",
    "    decoder_hidden_units=decoder_hidden_units,\n",
    "    decoder_activation_fn=decoder_activation_fn,\n",
    "    need_weights=need_weights,\n",
    "    numerical_passthrough=numerical_passthrough\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = skorch.NeuralNetClassifier(\n",
    "    module=module,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    device = \"cpu\", #if torch.cuda.is_available() else \"cpu\",\n",
    "    batch_size = batch_size,\n",
    "    max_epochs = epochs,\n",
    "    train_split=skorch.dataset.ValidSplit(((train_indices, val_indices),)),\n",
    "    callbacks=[\n",
    "        (\"balanced_accuracy\", skorch.callbacks.EpochScoring(\"balanced_accuracy\", lower_is_better=False)),\n",
    "        (\"duration\", skorch.callbacks.EpochTimer()),\n",
    "        EpochScoring(scoring='accuracy', name='train_acc', on_train=True), #        Checkpoint(monitor='valid_acc_best', dirname=path_of_checkpoint, load_best = True), \n",
    "        EarlyStopping(patience=15)\n",
    "\n",
    "    ],\n",
    "    optimizer__lr=1e-4,\n",
    "    optimizer__weight_decay=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (73) must match the size of tensor b (64) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mTraining and validation\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_numerical\u001b[39m\u001b[38;5;124m\"\u001b[39m: X_train[:, :n_numerical]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_categorical\u001b[39m\u001b[38;5;124m\"\u001b[39m: X_train[:, n_numerical:]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m      8\u001b[0m         }, \n\u001b[1;32m      9\u001b[0m         y\u001b[38;5;241m=\u001b[39my_train\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint64)\n\u001b[1;32m     10\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/tabtrans/lib/python3.11/site-packages/skorch/classifier.py:165\u001b[0m, in \u001b[0;36mNeuralNetClassifier.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See ``NeuralNet.fit``.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03mIn contrast to ``NeuralNet.fit``, ``y`` is non-optional to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m \n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# pylint: disable=useless-super-delegation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# this is actually a pylint bug:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# https://github.com/PyCQA/pylint/issues/1085\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(NeuralNetClassifier, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "File \u001b[0;32m~/anaconda3/envs/tabtrans/lib/python3.11/site-packages/skorch/net.py:1319\u001b[0m, in \u001b[0;36mNeuralNet.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarm_start \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialized_:\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize()\n\u001b[0;32m-> 1319\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_fit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/tabtrans/lib/python3.11/site-packages/skorch/net.py:1278\u001b[0m, in \u001b[0;36mNeuralNet.partial_fit\u001b[0;34m(self, X, y, classes, **fit_params)\u001b[0m\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_train_begin\u001b[39m\u001b[38;5;124m'\u001b[39m, X\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my)\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tabtrans/lib/python3.11/site-packages/skorch/net.py:1190\u001b[0m, in \u001b[0;36mNeuralNet.fit_loop\u001b[0;34m(self, X, y, epochs, **fit_params)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_epoch_begin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mon_epoch_kwargs)\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single_epoch(iterator_train, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1191\u001b[0m                           step_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m   1193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single_epoch(iterator_valid, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1194\u001b[0m                           step_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_step, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mon_epoch_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tabtrans/lib/python3.11/site-packages/skorch/net.py:1226\u001b[0m, in \u001b[0;36mNeuralNet.run_single_epoch\u001b[0;34m(self, iterator, training, prefix, step_fn, **fit_params)\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[1;32m   1225\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_batch_begin\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch\u001b[38;5;241m=\u001b[39mbatch, training\u001b[38;5;241m=\u001b[39mtraining)\n\u001b[0;32m-> 1226\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_fn(batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mrecord_batch(prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, step[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m   1228\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m (get_len(batch[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m))\n\u001b[1;32m   1229\u001b[0m                   \u001b[38;5;28;01melse\u001b[39;00m get_len(batch))\n",
      "File \u001b[0;32m~/anaconda3/envs/tabtrans/lib/python3.11/site-packages/skorch/net.py:1105\u001b[0m, in \u001b[0;36mNeuralNet.train_step\u001b[0;34m(self, batch, **fit_params)\u001b[0m\n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify(\n\u001b[1;32m   1098\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_grad_computed\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1099\u001b[0m         named_parameters\u001b[38;5;241m=\u001b[39mTeeGenerator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_all_learnable_params()),\n\u001b[1;32m   1100\u001b[0m         batch\u001b[38;5;241m=\u001b[39mbatch,\n\u001b[1;32m   1101\u001b[0m         training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1102\u001b[0m     )\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m step[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m-> 1105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_optimizer(step_fn)\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_accumulator\u001b[38;5;241m.\u001b[39mget_step()\n",
      "File \u001b[0;32m~/anaconda3/envs/tabtrans/lib/python3.11/site-packages/skorch/net.py:1060\u001b[0m, in \u001b[0;36mNeuralNet._step_optimizer\u001b[0;34m(self, step_fn)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1060\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep(step_fn)\n",
      "File \u001b[0;32m~/anaconda3/envs/tabtrans/lib/python3.11/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tabtrans/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/envs/tabtrans/lib/python3.11/site-packages/torch/optim/adamw.py:164\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 164\u001b[0m         loss \u001b[38;5;241m=\u001b[39m closure()\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    167\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/tabtrans/lib/python3.11/site-packages/skorch/net.py:1094\u001b[0m, in \u001b[0;36mNeuralNet.train_step.<locals>.step_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_fn\u001b[39m():\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_zero_grad_optimizer()\n\u001b[0;32m-> 1094\u001b[0m     step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step_single(batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m   1095\u001b[0m     step_accumulator\u001b[38;5;241m.\u001b[39mstore_step(step)\n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify(\n\u001b[1;32m   1098\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_grad_computed\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1099\u001b[0m         named_parameters\u001b[38;5;241m=\u001b[39mTeeGenerator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_all_learnable_params()),\n\u001b[1;32m   1100\u001b[0m         batch\u001b[38;5;241m=\u001b[39mbatch,\n\u001b[1;32m   1101\u001b[0m         training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1102\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/tabtrans/lib/python3.11/site-packages/skorch/net.py:993\u001b[0m, in \u001b[0;36mNeuralNet.train_step_single\u001b[0;34m(self, batch, **fit_params)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_training(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    992\u001b[0m Xi, yi \u001b[38;5;241m=\u001b[39m unpack_data(batch)\n\u001b[0;32m--> 993\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(Xi, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    994\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_loss(y_pred, yi, X\u001b[38;5;241m=\u001b[39mXi, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    995\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/tabtrans/lib/python3.11/site-packages/skorch/net.py:1520\u001b[0m, in \u001b[0;36mNeuralNet.infer\u001b[0;34m(self, x, **fit_params)\u001b[0m\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, Mapping):\n\u001b[1;32m   1519\u001b[0m     x_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_x_and_fit_params(x, fit_params)\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mx_dict)\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "File \u001b[0;32m~/anaconda3/envs/tabtrans/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tabtrans/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tabtrans/lib/python3.11/site-packages/ndsl/architecture/attention.py:224\u001b[0m, in \u001b[0;36mTabularTransformer.forward\u001b[0;34m(self, x_categorical, x_numerical)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_categorical, x_numerical):\n\u001b[1;32m    222\u001b[0m \n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# src came with two dims: (batch_size, num_features)\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical_encoder(x_categorical \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories_offset)\n\u001b[1;32m    226\u001b[0m     numerical_embedding \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumerical_passthrough:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (73) must match the size of tensor b (64) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training and validation\n",
    "\"\"\"\n",
    "\n",
    "model = model.fit(X={\n",
    "        \"x_numerical\": X_train[:, :n_numerical].astype(np.float32),\n",
    "        \"x_categorical\": X_train[:, n_numerical:].astype(np.int32)\n",
    "        }, \n",
    "        y=y_train.astype(np.int64)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabtrans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
