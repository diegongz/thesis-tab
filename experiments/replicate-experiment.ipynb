{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FT Replicate 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/diego/Git/thesis-tabtrans')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import training, callback, evaluating, attention, data\n",
    "from sklearn import datasets, model_selection\n",
    "import skorch\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# Configuration\n",
    "#####################################################\n",
    "\n",
    "dataset = \"christine\"\n",
    "aggregator = \"cls\"\n",
    "\n",
    "print(f\"Using -- Dataset:{dataset} Aggregator:{aggregator}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 554 #41142 #this is the ID of the Iris dataset in OpenML  \n",
    "\n",
    "# Load the Iris dataset\n",
    "df = data.read_dataset_by_id(id) #this function returns a dictionary with the dataset's data and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"categories\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"In total there are {len(df['numerical'])} numerical columns and {len(df['categorical'])} categorical columns \\n\")\n",
    "print(f\"The categorical columns are: {df['categorical']}\")\n",
    "print(f\"The numerical columns are: {df['numerical']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = df[\"features\"]\n",
    "df_pandas\n",
    "print(type(df_pandas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First numerical and then the categorcial columns\n",
    "\n",
    "categorical_features = df['categorical'].tolist()\n",
    "numerical_features = df['numerical'].tolist()\n",
    "#target = df[\"target\"]\n",
    "\n",
    "numerical_features = df_pandas[numerical_features]  # Assuming numerical_features is a list of column names\n",
    "categorical_features = df_pandas[categorical_features]  # Assuming categorical_features is a list of column names\n",
    "#target = df_pandas[target]  # Assuming target is a column name\n",
    "\n",
    "#print(target)\n",
    "\n",
    "print(type(categorical_features))\n",
    "print(type(numerical_features))\n",
    "print(type(df[\"target\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ordered = pd.concat([numerical_features,categorical_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose 'df' is your DataFrame and 'categorical_columns' is a list of column names with categorical features\n",
    "for col in categorical_features:\n",
    "    df_ordered[col], _ = pd.factorize(df_ordered[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_ordered.values\n",
    "y = df[\"outputs\"].codes\n",
    "\n",
    "print(type(X))\n",
    "print(type(y))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset metadata definition.\n",
    "\n",
    "    n_instances: Number of instances (rows) in your dataset.\n",
    "    n_numerical: Number of numerical features in your dataset.\n",
    "    n_categorical: List of the number of categories for each categorical column.\n",
    "    n_labels: Number of classification labels.\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "n_instances = len(X)\n",
    "n_numerical = df[\"n_numerical\"]\n",
    "n_categorical = df[\"n_categorical\"]\n",
    "n_labels = len(df[\"labels\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "    # Split data\n",
    "#####################################################\n",
    "\n",
    "from sklearn import datasets, model_selection\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.1)\n",
    "train_indices, val_indices = model_selection.train_test_split(np.arange(X_train.shape[0]), test_size=1/9) #1/9 of train is equal to 10% of total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Transformer hyperparameters definition.\n",
    "\n",
    "n_heads: Number of heads oneach Transformer Encoder.\n",
    "embed_dim: The embeddings' dimension.\n",
    "n_layers: Number of stacked Transformer Encoders.\n",
    "ff_pw_size: Position-wise Feed Forward network hidden layer size.\n",
    "attn_dropout: Dropout applied in the Multi-head self-attention mechanism.\n",
    "ff_dropout: Position-wise Feed Forward network dropout.\n",
    "aggregator: Aggregator to use. Must be in {concatenate, cls, max, mean, sum, rnn}\n",
    "aggregator_parameters: If the aggregator is different from rnn, set to None. Otherwise, a dictionary expecting:\n",
    "                        cell: Cellused in the RNN. Must be one of {GRU, LSTM}\n",
    "                        output_size: Recurrent neural network hidden size \n",
    "                        num_layers: Number of stacked layers in the RNN\n",
    "                        dropout: Dropout applied to the RNN\n",
    "    }\n",
    "decoder_hidden_units: List of hidden layer's sizes of the decoder MLP.\n",
    "decoder_activation_fn: Activation function used in the hidden layers\n",
    "need_weights: Set True if you require the attention cubes. During training is recommended to set it to False.\n",
    "numerical_passthrough: False if numerical features will be processed by the Multi-head self-attention mechanism.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "n_heads = 4 # In average 4 works better\n",
    "embed_dim = 4 # In average 256 works better\n",
    "n_layers = 3\n",
    "ff_pw_size = 30  #this value because of the paper \n",
    "attn_dropout = 0.3 #paper\n",
    "ff_dropout = 0.1 #paper value\n",
    "aggregator = \"cls\"\n",
    "aggregator_parameters = None\n",
    "decoder_hidden_units = [128,64] #paper value\n",
    "decoder_activation_fn = nn.ReLU()\n",
    "need_weights = False\n",
    "numerical_passthrough = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Building PyTorch module.\n",
    "\n",
    "We provide a wrapper function for building the PyTorch module.\n",
    "The function is utils.training.build_module.\n",
    "\"\"\"\n",
    "module = training.build_module(\n",
    "    n_categorical, # List of number of categories\n",
    "    n_numerical, # Number of numerical features\n",
    "    n_heads, # Number of heads per layer\n",
    "    ff_pw_size, # Size of the MLP inside each transformer encoder layer\n",
    "    n_layers, # Number of transformer encoder layers    \n",
    "    n_labels, # Number of output neurons\n",
    "    embed_dim,\n",
    "    attn_dropout, \n",
    "    ff_dropout, \n",
    "    aggregator, # The aggregator for output vectors before decoder\n",
    "    rnn_aggregator_parameters=aggregator_parameters,\n",
    "    decoder_hidden_units=decoder_hidden_units,\n",
    "    decoder_activation_fn=decoder_activation_fn,\n",
    "    need_weights=need_weights,\n",
    "    numerical_passthrough=numerical_passthrough\n",
    ")\n",
    "\n",
    "print(module)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Wrapping module in skorch.\n",
    "\n",
    "The PyTorch module can be used for a custom training.\n",
    "\n",
    "However, in this example we use the skorch library,\n",
    "which avoid the implementation of a custom training loop.\n",
    "\"\"\"\n",
    "\n",
    "model = skorch.NeuralNetClassifier(\n",
    "            module = module,\n",
    "            criterion=torch.nn.CrossEntropyLoss,\n",
    "            optimizer=torch.optim.AdamW,\n",
    "            device= \"cuda\", #cuda\" if torch.cuda.is_available() else\n",
    "            batch_size=32,\n",
    "            max_epochs=100,\n",
    "            train_split=skorch.dataset.ValidSplit(((train_indices, val_indices),)),\n",
    "            callbacks=[\n",
    "                (\"balanced_accuracy\", skorch.callbacks.EpochScoring(\"balanced_accuracy\", lower_is_better=False)),\n",
    "                (\"accuracy\", skorch.callbacks.EpochScoring(\"accuracy\", lower_is_better=False)),\n",
    "                (\"duration\", skorch.callbacks.EpochTimer())\n",
    "            ],\n",
    "            optimizer__lr=1e-4,\n",
    "            optimizer__weight_decay=1e-4\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training and validation\n",
    "\"\"\"\n",
    "\n",
    "model = model.fit(X={\n",
    "        \"x_numerical\": X_train[:, :n_numerical].astype(np.float32),\n",
    "        \"x_categorical\": X_train[:, n_numerical:].astype(np.int32)\n",
    "        }, \n",
    "        y=y_train.astype(np.int64)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabtrans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
