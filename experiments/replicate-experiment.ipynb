{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FT Replicate 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/diego/Git/thesis-tabtrans')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import training, callback, evaluating, attention, data\n",
    "from sklearn import datasets, model_selection\n",
    "import skorch\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using -- Dataset:christine Aggregator:cls\n"
     ]
    }
   ],
   "source": [
    "#####################################################\n",
    "# Configuration\n",
    "#####################################################\n",
    "\n",
    "dataset = \"christine\"\n",
    "aggregator = \"cls\"\n",
    "\n",
    "print(f\"Using -- Dataset:{dataset} Aggregator:{aggregator}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:openml.datasets.dataset:pickle write mnist_784\n"
     ]
    }
   ],
   "source": [
    "id = 554 #41142 #this is the ID of the Iris dataset in OpenML  \n",
    "\n",
    "# Load the Iris dataset\n",
    "df = data.read_dataset_by_id(id) #this function returns a dictionary with the dataset's data and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['features', 'outputs', 'target', 'labels', 'columns', 'categorical', 'categories', 'n_categorical', 'numerical', 'n_numerical'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"categories\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total there are 784 numerical columns and 0 categorical columns \n",
      "\n",
      "The categorical columns are: []\n",
      "The numerical columns are: ['pixel1' 'pixel2' 'pixel3' 'pixel4' 'pixel5' 'pixel6' 'pixel7' 'pixel8'\n",
      " 'pixel9' 'pixel10' 'pixel11' 'pixel12' 'pixel13' 'pixel14' 'pixel15'\n",
      " 'pixel16' 'pixel17' 'pixel18' 'pixel19' 'pixel20' 'pixel21' 'pixel22'\n",
      " 'pixel23' 'pixel24' 'pixel25' 'pixel26' 'pixel27' 'pixel28' 'pixel29'\n",
      " 'pixel30' 'pixel31' 'pixel32' 'pixel33' 'pixel34' 'pixel35' 'pixel36'\n",
      " 'pixel37' 'pixel38' 'pixel39' 'pixel40' 'pixel41' 'pixel42' 'pixel43'\n",
      " 'pixel44' 'pixel45' 'pixel46' 'pixel47' 'pixel48' 'pixel49' 'pixel50'\n",
      " 'pixel51' 'pixel52' 'pixel53' 'pixel54' 'pixel55' 'pixel56' 'pixel57'\n",
      " 'pixel58' 'pixel59' 'pixel60' 'pixel61' 'pixel62' 'pixel63' 'pixel64'\n",
      " 'pixel65' 'pixel66' 'pixel67' 'pixel68' 'pixel69' 'pixel70' 'pixel71'\n",
      " 'pixel72' 'pixel73' 'pixel74' 'pixel75' 'pixel76' 'pixel77' 'pixel78'\n",
      " 'pixel79' 'pixel80' 'pixel81' 'pixel82' 'pixel83' 'pixel84' 'pixel85'\n",
      " 'pixel86' 'pixel87' 'pixel88' 'pixel89' 'pixel90' 'pixel91' 'pixel92'\n",
      " 'pixel93' 'pixel94' 'pixel95' 'pixel96' 'pixel97' 'pixel98' 'pixel99'\n",
      " 'pixel100' 'pixel101' 'pixel102' 'pixel103' 'pixel104' 'pixel105'\n",
      " 'pixel106' 'pixel107' 'pixel108' 'pixel109' 'pixel110' 'pixel111'\n",
      " 'pixel112' 'pixel113' 'pixel114' 'pixel115' 'pixel116' 'pixel117'\n",
      " 'pixel118' 'pixel119' 'pixel120' 'pixel121' 'pixel122' 'pixel123'\n",
      " 'pixel124' 'pixel125' 'pixel126' 'pixel127' 'pixel128' 'pixel129'\n",
      " 'pixel130' 'pixel131' 'pixel132' 'pixel133' 'pixel134' 'pixel135'\n",
      " 'pixel136' 'pixel137' 'pixel138' 'pixel139' 'pixel140' 'pixel141'\n",
      " 'pixel142' 'pixel143' 'pixel144' 'pixel145' 'pixel146' 'pixel147'\n",
      " 'pixel148' 'pixel149' 'pixel150' 'pixel151' 'pixel152' 'pixel153'\n",
      " 'pixel154' 'pixel155' 'pixel156' 'pixel157' 'pixel158' 'pixel159'\n",
      " 'pixel160' 'pixel161' 'pixel162' 'pixel163' 'pixel164' 'pixel165'\n",
      " 'pixel166' 'pixel167' 'pixel168' 'pixel169' 'pixel170' 'pixel171'\n",
      " 'pixel172' 'pixel173' 'pixel174' 'pixel175' 'pixel176' 'pixel177'\n",
      " 'pixel178' 'pixel179' 'pixel180' 'pixel181' 'pixel182' 'pixel183'\n",
      " 'pixel184' 'pixel185' 'pixel186' 'pixel187' 'pixel188' 'pixel189'\n",
      " 'pixel190' 'pixel191' 'pixel192' 'pixel193' 'pixel194' 'pixel195'\n",
      " 'pixel196' 'pixel197' 'pixel198' 'pixel199' 'pixel200' 'pixel201'\n",
      " 'pixel202' 'pixel203' 'pixel204' 'pixel205' 'pixel206' 'pixel207'\n",
      " 'pixel208' 'pixel209' 'pixel210' 'pixel211' 'pixel212' 'pixel213'\n",
      " 'pixel214' 'pixel215' 'pixel216' 'pixel217' 'pixel218' 'pixel219'\n",
      " 'pixel220' 'pixel221' 'pixel222' 'pixel223' 'pixel224' 'pixel225'\n",
      " 'pixel226' 'pixel227' 'pixel228' 'pixel229' 'pixel230' 'pixel231'\n",
      " 'pixel232' 'pixel233' 'pixel234' 'pixel235' 'pixel236' 'pixel237'\n",
      " 'pixel238' 'pixel239' 'pixel240' 'pixel241' 'pixel242' 'pixel243'\n",
      " 'pixel244' 'pixel245' 'pixel246' 'pixel247' 'pixel248' 'pixel249'\n",
      " 'pixel250' 'pixel251' 'pixel252' 'pixel253' 'pixel254' 'pixel255'\n",
      " 'pixel256' 'pixel257' 'pixel258' 'pixel259' 'pixel260' 'pixel261'\n",
      " 'pixel262' 'pixel263' 'pixel264' 'pixel265' 'pixel266' 'pixel267'\n",
      " 'pixel268' 'pixel269' 'pixel270' 'pixel271' 'pixel272' 'pixel273'\n",
      " 'pixel274' 'pixel275' 'pixel276' 'pixel277' 'pixel278' 'pixel279'\n",
      " 'pixel280' 'pixel281' 'pixel282' 'pixel283' 'pixel284' 'pixel285'\n",
      " 'pixel286' 'pixel287' 'pixel288' 'pixel289' 'pixel290' 'pixel291'\n",
      " 'pixel292' 'pixel293' 'pixel294' 'pixel295' 'pixel296' 'pixel297'\n",
      " 'pixel298' 'pixel299' 'pixel300' 'pixel301' 'pixel302' 'pixel303'\n",
      " 'pixel304' 'pixel305' 'pixel306' 'pixel307' 'pixel308' 'pixel309'\n",
      " 'pixel310' 'pixel311' 'pixel312' 'pixel313' 'pixel314' 'pixel315'\n",
      " 'pixel316' 'pixel317' 'pixel318' 'pixel319' 'pixel320' 'pixel321'\n",
      " 'pixel322' 'pixel323' 'pixel324' 'pixel325' 'pixel326' 'pixel327'\n",
      " 'pixel328' 'pixel329' 'pixel330' 'pixel331' 'pixel332' 'pixel333'\n",
      " 'pixel334' 'pixel335' 'pixel336' 'pixel337' 'pixel338' 'pixel339'\n",
      " 'pixel340' 'pixel341' 'pixel342' 'pixel343' 'pixel344' 'pixel345'\n",
      " 'pixel346' 'pixel347' 'pixel348' 'pixel349' 'pixel350' 'pixel351'\n",
      " 'pixel352' 'pixel353' 'pixel354' 'pixel355' 'pixel356' 'pixel357'\n",
      " 'pixel358' 'pixel359' 'pixel360' 'pixel361' 'pixel362' 'pixel363'\n",
      " 'pixel364' 'pixel365' 'pixel366' 'pixel367' 'pixel368' 'pixel369'\n",
      " 'pixel370' 'pixel371' 'pixel372' 'pixel373' 'pixel374' 'pixel375'\n",
      " 'pixel376' 'pixel377' 'pixel378' 'pixel379' 'pixel380' 'pixel381'\n",
      " 'pixel382' 'pixel383' 'pixel384' 'pixel385' 'pixel386' 'pixel387'\n",
      " 'pixel388' 'pixel389' 'pixel390' 'pixel391' 'pixel392' 'pixel393'\n",
      " 'pixel394' 'pixel395' 'pixel396' 'pixel397' 'pixel398' 'pixel399'\n",
      " 'pixel400' 'pixel401' 'pixel402' 'pixel403' 'pixel404' 'pixel405'\n",
      " 'pixel406' 'pixel407' 'pixel408' 'pixel409' 'pixel410' 'pixel411'\n",
      " 'pixel412' 'pixel413' 'pixel414' 'pixel415' 'pixel416' 'pixel417'\n",
      " 'pixel418' 'pixel419' 'pixel420' 'pixel421' 'pixel422' 'pixel423'\n",
      " 'pixel424' 'pixel425' 'pixel426' 'pixel427' 'pixel428' 'pixel429'\n",
      " 'pixel430' 'pixel431' 'pixel432' 'pixel433' 'pixel434' 'pixel435'\n",
      " 'pixel436' 'pixel437' 'pixel438' 'pixel439' 'pixel440' 'pixel441'\n",
      " 'pixel442' 'pixel443' 'pixel444' 'pixel445' 'pixel446' 'pixel447'\n",
      " 'pixel448' 'pixel449' 'pixel450' 'pixel451' 'pixel452' 'pixel453'\n",
      " 'pixel454' 'pixel455' 'pixel456' 'pixel457' 'pixel458' 'pixel459'\n",
      " 'pixel460' 'pixel461' 'pixel462' 'pixel463' 'pixel464' 'pixel465'\n",
      " 'pixel466' 'pixel467' 'pixel468' 'pixel469' 'pixel470' 'pixel471'\n",
      " 'pixel472' 'pixel473' 'pixel474' 'pixel475' 'pixel476' 'pixel477'\n",
      " 'pixel478' 'pixel479' 'pixel480' 'pixel481' 'pixel482' 'pixel483'\n",
      " 'pixel484' 'pixel485' 'pixel486' 'pixel487' 'pixel488' 'pixel489'\n",
      " 'pixel490' 'pixel491' 'pixel492' 'pixel493' 'pixel494' 'pixel495'\n",
      " 'pixel496' 'pixel497' 'pixel498' 'pixel499' 'pixel500' 'pixel501'\n",
      " 'pixel502' 'pixel503' 'pixel504' 'pixel505' 'pixel506' 'pixel507'\n",
      " 'pixel508' 'pixel509' 'pixel510' 'pixel511' 'pixel512' 'pixel513'\n",
      " 'pixel514' 'pixel515' 'pixel516' 'pixel517' 'pixel518' 'pixel519'\n",
      " 'pixel520' 'pixel521' 'pixel522' 'pixel523' 'pixel524' 'pixel525'\n",
      " 'pixel526' 'pixel527' 'pixel528' 'pixel529' 'pixel530' 'pixel531'\n",
      " 'pixel532' 'pixel533' 'pixel534' 'pixel535' 'pixel536' 'pixel537'\n",
      " 'pixel538' 'pixel539' 'pixel540' 'pixel541' 'pixel542' 'pixel543'\n",
      " 'pixel544' 'pixel545' 'pixel546' 'pixel547' 'pixel548' 'pixel549'\n",
      " 'pixel550' 'pixel551' 'pixel552' 'pixel553' 'pixel554' 'pixel555'\n",
      " 'pixel556' 'pixel557' 'pixel558' 'pixel559' 'pixel560' 'pixel561'\n",
      " 'pixel562' 'pixel563' 'pixel564' 'pixel565' 'pixel566' 'pixel567'\n",
      " 'pixel568' 'pixel569' 'pixel570' 'pixel571' 'pixel572' 'pixel573'\n",
      " 'pixel574' 'pixel575' 'pixel576' 'pixel577' 'pixel578' 'pixel579'\n",
      " 'pixel580' 'pixel581' 'pixel582' 'pixel583' 'pixel584' 'pixel585'\n",
      " 'pixel586' 'pixel587' 'pixel588' 'pixel589' 'pixel590' 'pixel591'\n",
      " 'pixel592' 'pixel593' 'pixel594' 'pixel595' 'pixel596' 'pixel597'\n",
      " 'pixel598' 'pixel599' 'pixel600' 'pixel601' 'pixel602' 'pixel603'\n",
      " 'pixel604' 'pixel605' 'pixel606' 'pixel607' 'pixel608' 'pixel609'\n",
      " 'pixel610' 'pixel611' 'pixel612' 'pixel613' 'pixel614' 'pixel615'\n",
      " 'pixel616' 'pixel617' 'pixel618' 'pixel619' 'pixel620' 'pixel621'\n",
      " 'pixel622' 'pixel623' 'pixel624' 'pixel625' 'pixel626' 'pixel627'\n",
      " 'pixel628' 'pixel629' 'pixel630' 'pixel631' 'pixel632' 'pixel633'\n",
      " 'pixel634' 'pixel635' 'pixel636' 'pixel637' 'pixel638' 'pixel639'\n",
      " 'pixel640' 'pixel641' 'pixel642' 'pixel643' 'pixel644' 'pixel645'\n",
      " 'pixel646' 'pixel647' 'pixel648' 'pixel649' 'pixel650' 'pixel651'\n",
      " 'pixel652' 'pixel653' 'pixel654' 'pixel655' 'pixel656' 'pixel657'\n",
      " 'pixel658' 'pixel659' 'pixel660' 'pixel661' 'pixel662' 'pixel663'\n",
      " 'pixel664' 'pixel665' 'pixel666' 'pixel667' 'pixel668' 'pixel669'\n",
      " 'pixel670' 'pixel671' 'pixel672' 'pixel673' 'pixel674' 'pixel675'\n",
      " 'pixel676' 'pixel677' 'pixel678' 'pixel679' 'pixel680' 'pixel681'\n",
      " 'pixel682' 'pixel683' 'pixel684' 'pixel685' 'pixel686' 'pixel687'\n",
      " 'pixel688' 'pixel689' 'pixel690' 'pixel691' 'pixel692' 'pixel693'\n",
      " 'pixel694' 'pixel695' 'pixel696' 'pixel697' 'pixel698' 'pixel699'\n",
      " 'pixel700' 'pixel701' 'pixel702' 'pixel703' 'pixel704' 'pixel705'\n",
      " 'pixel706' 'pixel707' 'pixel708' 'pixel709' 'pixel710' 'pixel711'\n",
      " 'pixel712' 'pixel713' 'pixel714' 'pixel715' 'pixel716' 'pixel717'\n",
      " 'pixel718' 'pixel719' 'pixel720' 'pixel721' 'pixel722' 'pixel723'\n",
      " 'pixel724' 'pixel725' 'pixel726' 'pixel727' 'pixel728' 'pixel729'\n",
      " 'pixel730' 'pixel731' 'pixel732' 'pixel733' 'pixel734' 'pixel735'\n",
      " 'pixel736' 'pixel737' 'pixel738' 'pixel739' 'pixel740' 'pixel741'\n",
      " 'pixel742' 'pixel743' 'pixel744' 'pixel745' 'pixel746' 'pixel747'\n",
      " 'pixel748' 'pixel749' 'pixel750' 'pixel751' 'pixel752' 'pixel753'\n",
      " 'pixel754' 'pixel755' 'pixel756' 'pixel757' 'pixel758' 'pixel759'\n",
      " 'pixel760' 'pixel761' 'pixel762' 'pixel763' 'pixel764' 'pixel765'\n",
      " 'pixel766' 'pixel767' 'pixel768' 'pixel769' 'pixel770' 'pixel771'\n",
      " 'pixel772' 'pixel773' 'pixel774' 'pixel775' 'pixel776' 'pixel777'\n",
      " 'pixel778' 'pixel779' 'pixel780' 'pixel781' 'pixel782' 'pixel783'\n",
      " 'pixel784']\n"
     ]
    }
   ],
   "source": [
    "print(f\"In total there are {len(df['numerical'])} numerical columns and {len(df['categorical'])} categorical columns \\n\")\n",
    "print(f\"The categorical columns are: {df['categorical']}\")\n",
    "print(f\"The numerical columns are: {df['numerical']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "df_pandas = df[\"features\"]\n",
    "df_pandas\n",
    "print(type(df_pandas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "#First numerical and then the categorcial columns\n",
    "\n",
    "categorical_features = df['categorical'].tolist()\n",
    "numerical_features = df['numerical'].tolist()\n",
    "#target = df[\"target\"]\n",
    "\n",
    "numerical_features = df_pandas[numerical_features]  # Assuming numerical_features is a list of column names\n",
    "categorical_features = df_pandas[categorical_features]  # Assuming categorical_features is a list of column names\n",
    "#target = df_pandas[target]  # Assuming target is a column name\n",
    "\n",
    "#print(target)\n",
    "\n",
    "print(type(categorical_features))\n",
    "print(type(numerical_features))\n",
    "print(type(df[\"target\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ordered = pd.concat([numerical_features,categorical_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose 'df' is your DataFrame and 'categorical_columns' is a list of column names with categorical features\n",
    "for col in categorical_features:\n",
    "    df_ordered[col], _ = pd.factorize(df_ordered[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "X = df_ordered.values\n",
    "y = df[\"outputs\"].codes\n",
    "\n",
    "print(type(X))\n",
    "print(type(y))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset metadata definition.\n",
    "\n",
    "    n_instances: Number of instances (rows) in your dataset.\n",
    "    n_numerical: Number of numerical features in your dataset.\n",
    "    n_categorical: List of the number of categories for each categorical column.\n",
    "    n_labels: Number of classification labels.\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "n_instances = len(X)\n",
    "n_numerical = df[\"n_numerical\"]\n",
    "n_categorical = df[\"n_categorical\"]\n",
    "n_labels = len(df[\"labels\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "    # Split data\n",
    "#####################################################\n",
    "\n",
    "from sklearn import datasets, model_selection\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.1)\n",
    "train_indices, val_indices = model_selection.train_test_split(np.arange(X_train.shape[0]), test_size=1/9) #1/9 of train is equal to 10% of total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Transformer hyperparameters definition.\n",
    "\n",
    "n_heads: Number of heads oneach Transformer Encoder.\n",
    "embed_dim: The embeddings' dimension.\n",
    "n_layers: Number of stacked Transformer Encoders.\n",
    "ff_pw_size: Position-wise Feed Forward network hidden layer size.\n",
    "attn_dropout: Dropout applied in the Multi-head self-attention mechanism.\n",
    "ff_dropout: Position-wise Feed Forward network dropout.\n",
    "aggregator: Aggregator to use. Must be in {concatenate, cls, max, mean, sum, rnn}\n",
    "aggregator_parameters: If the aggregator is different from rnn, set to None. Otherwise, a dictionary expecting:\n",
    "                        cell: Cellused in the RNN. Must be one of {GRU, LSTM}\n",
    "                        output_size: Recurrent neural network hidden size \n",
    "                        num_layers: Number of stacked layers in the RNN\n",
    "                        dropout: Dropout applied to the RNN\n",
    "    }\n",
    "decoder_hidden_units: List of hidden layer's sizes of the decoder MLP.\n",
    "decoder_activation_fn: Activation function used in the hidden layers\n",
    "need_weights: Set True if you require the attention cubes. During training is recommended to set it to False.\n",
    "numerical_passthrough: False if numerical features will be processed by the Multi-head self-attention mechanism.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "n_heads = 4 # In average 4 works better\n",
    "embed_dim = 4 # In average 256 works better\n",
    "n_layers = 3\n",
    "ff_pw_size = 30  #this value because of the paper \n",
    "attn_dropout = 0.3 #paper\n",
    "ff_dropout = 0.1 #paper value\n",
    "aggregator = \"cls\"\n",
    "aggregator_parameters = None\n",
    "decoder_hidden_units = [128,64] #paper value\n",
    "decoder_activation_fn = nn.ReLU()\n",
    "need_weights = False\n",
    "numerical_passthrough = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Building PyTorch module.\n",
    "\n",
    "We provide a wrapper function for building the PyTorch module.\n",
    "The function is utils.training.build_module.\n",
    "\"\"\"\n",
    "module = training.build_module(\n",
    "    n_categorical, # List of number of categories\n",
    "    n_numerical, # Number of numerical features\n",
    "    n_heads, # Number of heads per layer\n",
    "    ff_pw_size, # Size of the MLP inside each transformer encoder layer\n",
    "    n_layers, # Number of transformer encoder layers    \n",
    "    n_labels, # Number of output neurons\n",
    "    embed_dim,\n",
    "    attn_dropout, \n",
    "    ff_dropout, \n",
    "    aggregator, # The aggregator for output vectors before decoder\n",
    "    rnn_aggregator_parameters=aggregator_parameters,\n",
    "    decoder_hidden_units=decoder_hidden_units,\n",
    "    decoder_activation_fn=decoder_activation_fn,\n",
    "    need_weights=need_weights,\n",
    "    numerical_passthrough=numerical_passthrough\n",
    ")\n",
    "\n",
    "print(module)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Wrapping module in skorch.\n",
    "\n",
    "The PyTorch module can be used for a custom training.\n",
    "\n",
    "However, in this example we use the skorch library,\n",
    "which avoid the implementation of a custom training loop.\n",
    "\"\"\"\n",
    "\n",
    "model = skorch.NeuralNetClassifier(\n",
    "            module = module,\n",
    "            criterion=torch.nn.CrossEntropyLoss,\n",
    "            optimizer=torch.optim.AdamW,\n",
    "            device= \"cuda\", #cuda\" if torch.cuda.is_available() else\n",
    "            batch_size=32,\n",
    "            max_epochs=100,\n",
    "            train_split=skorch.dataset.ValidSplit(((train_indices, val_indices),)),\n",
    "            callbacks=[\n",
    "                (\"balanced_accuracy\", skorch.callbacks.EpochScoring(\"balanced_accuracy\", lower_is_better=False)),\n",
    "                (\"accuracy\", skorch.callbacks.EpochScoring(\"accuracy\", lower_is_better=False)),\n",
    "                (\"duration\", skorch.callbacks.EpochTimer())\n",
    "            ],\n",
    "            optimizer__lr=1e-4,\n",
    "            optimizer__weight_decay=1e-4\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    accuracy    balanced_accuracy    train_loss    valid_acc    valid_loss       dur\n",
      "-------  ----------  -------------------  ------------  -----------  ------------  --------\n",
      "      1      \u001b[36m0.2541\u001b[0m               \u001b[32m0.2481\u001b[0m        \u001b[35m2.2053\u001b[0m       \u001b[31m0.2541\u001b[0m        \u001b[94m1.9903\u001b[0m  246.0755\n",
      "      2      \u001b[36m0.2799\u001b[0m               \u001b[32m0.2751\u001b[0m        \u001b[35m1.9399\u001b[0m       \u001b[31m0.2799\u001b[0m        \u001b[94m1.8806\u001b[0m  245.9242\n",
      "      3      \u001b[36m0.3119\u001b[0m               \u001b[32m0.3080\u001b[0m        \u001b[35m1.8424\u001b[0m       \u001b[31m0.3119\u001b[0m        \u001b[94m1.7946\u001b[0m  245.9214\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training and validation\n",
    "\"\"\"\n",
    "\n",
    "model = model.fit(X={\n",
    "        \"x_numerical\": X_train[:, :n_numerical].astype(np.float32),\n",
    "        \"x_categorical\": X_train[:, n_numerical:].astype(np.int32)\n",
    "        }, \n",
    "        y=y_train.astype(np.int64)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabtrans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
