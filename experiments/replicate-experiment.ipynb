{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FT Replicate 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/diego/Git/thesis-tabtrans')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import training, callback, evaluating, attention, data\n",
    "from sklearn import datasets, model_selection\n",
    "import skorch\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using -- Dataset:anneal Aggregator:cls\n"
     ]
    }
   ],
   "source": [
    "#####################################################\n",
    "# Configuration\n",
    "#####################################################\n",
    "\n",
    "dataset = \"anneal\"\n",
    "aggregator = \"cls\"\n",
    "\n",
    "print(f\"Using -- Dataset:{dataset} Aggregator:{aggregator}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:openml.datasets.dataset:pickle write anneal\n"
     ]
    }
   ],
   "source": [
    "task_id = 233090 #anneal dataset\n",
    "\n",
    "# Load the Iris dataset\n",
    "X_train, X_test, y_train, y_test, train_indices, val_indices, n_instances, n_labels, n_numerical, n_categories = data.import_data(task_id, 100) #this function returns a dictionary with the dataset's data and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Transformer hyperparameters definition.\n",
    "\n",
    "n_heads: Number of heads oneach Transformer Encoder.\n",
    "embed_dim: The embeddings' dimension.\n",
    "n_layers: Number of stacked Transformer Encoders.\n",
    "ff_pw_size: Position-wise Feed Forward network hidden layer size.\n",
    "attn_dropout: Dropout applied in the Multi-head self-attention mechanism.\n",
    "ff_dropout: Position-wise Feed Forward network dropout.\n",
    "aggregator: Aggregator to use. Must be in {concatenate, cls, max, mean, sum, rnn}\n",
    "aggregator_parameters: If the aggregator is different from rnn, set to None. Otherwise, a dictionary expecting:\n",
    "                        cell: Cellused in the RNN. Must be one of {GRU, LSTM}\n",
    "                        output_size: Recurrent neural network hidden size \n",
    "                        num_layers: Number of stacked layers in the RNN\n",
    "                        dropout: Dropout applied to the RNN\n",
    "    }\n",
    "decoder_hidden_units: List of hidden layer's sizes of the decoder MLP.\n",
    "decoder_activation_fn: Activation function used in the hidden layers\n",
    "need_weights: Set True if you require the attention cubes. During training is recommended to set it to False.\n",
    "numerical_passthrough: False if numerical features will be processed by the Multi-head self-attention mechanism.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "n_heads = 4 # In average 4 works better\n",
    "embed_dim = 128 # In average 256 works better\n",
    "n_layers = 2\n",
    "ff_pw_size = 30  #this value because of the paper \n",
    "attn_dropout = 0.3 #paper\n",
    "ff_dropout = 0.1 #paper value\n",
    "aggregator = \"cls\"\n",
    "aggregator_parameters = None\n",
    "decoder_hidden_units = [128,64] #paper value\n",
    "decoder_activation_fn = nn.ReLU()\n",
    "need_weights = False\n",
    "numerical_passthrough = False\n",
    "\n",
    "epochs = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego/anaconda3/envs/tabtrans/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer was not TransformerEncoderLayer\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Building PyTorch module.\n",
    "\n",
    "We provide a wrapper function for building the PyTorch module.\n",
    "The function is utils.training.build_module.\n",
    "\"\"\"\n",
    "#module\n",
    "module = training.build_module(\n",
    "    n_categories, # List of number of categories\n",
    "    n_numerical, # Number of numerical features\n",
    "    n_heads, # Number of heads per layer\n",
    "    ff_pw_size, # Size of the MLP inside each transformer encoder layer\n",
    "    n_layers, # Number of transformer encoder layers    \n",
    "    n_labels, # Number of output neurons\n",
    "    embed_dim,\n",
    "    attn_dropout, \n",
    "    ff_dropout, \n",
    "    aggregator, # The aggregator for output vectors before decoder\n",
    "    rnn_aggregator_parameters=aggregator_parameters,\n",
    "    decoder_hidden_units=decoder_hidden_units,\n",
    "    decoder_activation_fn=decoder_activation_fn,\n",
    "    need_weights=need_weights,\n",
    "    numerical_passthrough=numerical_passthrough\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#train_split=skorch.dataset.ValidSplit(((train_indices, val_indices),)),\\ncallbacks=[\\n                (\"balanced_accuracy\", skorch.callbacks.EpochScoring(\"balanced_accuracy\", lower_is_better=False)),\\n                (\"accuracy\", skorch.callbacks.EpochScoring(\"accuracy\", lower_is_better=False)),\\n                (\"duration\", skorch.callbacks.EpochTimer())\\n            ]\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Wrapping module in skorch.\n",
    "\n",
    "The PyTorch module can be used for a custom training.\n",
    "\n",
    "However, in this example we use the skorch library,\n",
    "which avoid the implementation of a custom training loop.\n",
    "\"\"\"\n",
    "\n",
    "model = skorch.NeuralNetClassifier(\n",
    "            module = module,\n",
    "            criterion=torch.nn.CrossEntropyLoss,\n",
    "            optimizer=torch.optim.AdamW,\n",
    "            device= \"cuda\", #cuda\" if torch.cuda.is_available() else\n",
    "            batch_size=32,\n",
    "            train_split=None,\n",
    "            max_epochs= epochs,\n",
    "            optimizer__lr=1e-4,\n",
    "            optimizer__weight_decay=1e-4\n",
    "        )\n",
    "\n",
    "'''\n",
    "#train_split=skorch.dataset.ValidSplit(((train_indices, val_indices),)),\n",
    "callbacks=[\n",
    "                (\"balanced_accuracy\", skorch.callbacks.EpochScoring(\"balanced_accuracy\", lower_is_better=False)),\n",
    "                (\"accuracy\", skorch.callbacks.EpochScoring(\"accuracy\", lower_is_better=False)),\n",
    "                (\"duration\", skorch.callbacks.EpochTimer())\n",
    "            ]\n",
    "'''   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.0829\u001b[0m  0.1258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2        \u001b[36m0.0825\u001b[0m  0.1214\n",
      "      3        \u001b[36m0.0709\u001b[0m  0.1164\n",
      "      4        0.0855  0.1188\n",
      "      5        0.0777  0.1164\n",
      "      6        0.0905  0.1166\n",
      "      7        0.0853  0.1186\n",
      "      8        0.0753  0.1166\n",
      "      9        0.0854  0.1175\n",
      "     10        0.0792  0.1187\n",
      "     11        0.0840  0.1173\n",
      "     12        0.0849  0.1175\n",
      "     13        0.0713  0.1167\n",
      "     14        0.0832  0.1173\n",
      "     15        \u001b[36m0.0627\u001b[0m  0.1167\n",
      "     16        0.0856  0.1166\n",
      "     17        0.0789  0.1171\n",
      "     18        \u001b[36m0.0585\u001b[0m  0.1175\n",
      "     19        \u001b[36m0.0553\u001b[0m  0.1182\n",
      "     20        0.0581  0.1160\n",
      "     21        0.0553  0.1174\n",
      "     22        0.0601  0.1170\n",
      "     23        0.0589  0.1170\n",
      "     24        0.0564  0.1162\n",
      "     25        0.0582  0.1187\n",
      "     26        0.0585  0.1160\n",
      "     27        \u001b[36m0.0463\u001b[0m  0.1173\n",
      "     28        0.0622  0.1167\n",
      "     29        0.0517  0.1160\n",
      "     30        \u001b[36m0.0400\u001b[0m  0.1169\n",
      "     31        0.0514  0.1166\n",
      "     32        0.0433  0.1159\n",
      "     33        0.0513  0.1166\n",
      "     34        0.0404  0.1174\n",
      "     35        \u001b[36m0.0367\u001b[0m  0.1197\n",
      "     36        0.0423  0.1173\n",
      "     37        0.0446  0.1169\n",
      "     38        \u001b[36m0.0363\u001b[0m  0.1170\n",
      "     39        0.0368  0.1170\n",
      "     40        \u001b[36m0.0346\u001b[0m  0.1166\n",
      "     41        \u001b[36m0.0338\u001b[0m  0.1159\n",
      "     42        0.0441  0.1180\n",
      "     43        0.0350  0.1167\n",
      "     44        \u001b[36m0.0334\u001b[0m  0.1182\n",
      "     45        0.0364  0.1171\n",
      "     46        \u001b[36m0.0314\u001b[0m  0.1158\n",
      "     47        0.0373  0.1156\n",
      "     48        \u001b[36m0.0260\u001b[0m  0.1178\n",
      "     49        0.0287  0.1152\n",
      "     50        \u001b[36m0.0182\u001b[0m  0.1175\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training and validation\n",
    "\"\"\"\n",
    "\n",
    "model = model.fit(X={\n",
    "        \"x_numerical\": X_train[:, :n_numerical].astype(np.float32),\n",
    "        \"x_categorical\": X_train[:, n_numerical:].astype(np.int32)\n",
    "        }, \n",
    "        y=y_train.astype(np.int64)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_proba(X={\n",
    "    \"x_numerical\": X_train[:, :n_numerical].astype(np.float32),\n",
    "    \"x_categorical\": X_train[:, n_numerical:].astype(np.int32)\n",
    "    }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results in validation:\n",
      "\n",
      "{'balanced_accuracy': 0.9990859232175503, 'accuracy': 0.9971910112359551, 'log_loss': 0.008496357153436617}\n"
     ]
    }
   ],
   "source": [
    "print(\"Test results in validation:\\n\")\n",
    "print(evaluating.get_default_scores(y_train.astype(np.int64), predictions, multiclass = True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabtrans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
