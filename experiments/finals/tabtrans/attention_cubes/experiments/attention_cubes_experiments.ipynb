{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/diego-ngz/Git/thesis-tabtrans\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "#path of the project\n",
    "project_path = \"/home/diego-ngz/Git/thesis-tabtrans\"\n",
    "\n",
    "sys.path.append(project_path) #This helps to be able to import the data from the parent directory to other files\n",
    "\n",
    "from utils import data, tabtrans_file, plots,attention, training, attention_file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "from sklearn.cluster import KMeans\n",
    "from skorch.callbacks import Checkpoint, TrainEndCheckpoint, EarlyStopping, LoadInitState, EpochScoring, Checkpoint\n",
    "import skorch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import (\n",
    "                    linear_model, \n",
    "                    pipeline, \n",
    "                    neighbors, \n",
    "                    base, \n",
    "                    model_selection, \n",
    "                    tree, \n",
    "                    feature_selection, \n",
    "                    neural_network,\n",
    "                    cluster\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:openml.datasets.dataset:pickle write credit-g\n"
     ]
    }
   ],
   "source": [
    "df_id = 31\n",
    "X_train, X_test, y_train, y_test, train_indices, val_indices, n_instances, n_labels, n_numerical, n_categories = data.import_data(df_id)\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "name_df = data.get_dataset_name(df_id)\n",
    "\n",
    "path_of_datset = f'{project_path}/Final_models_4/{name_df}' #The path can be \n",
    "\n",
    "path_to_hyperparameters = f'{path_of_datset}/tabtrans/hyperparameter_selection'\n",
    "\n",
    "#define the path to final_tabtrans\n",
    "path_to_final_tabtrans = f'{path_of_datset}/tabtrans/final_tabtrans_cv'\n",
    "\n",
    "sample = 100\n",
    "path_of_size = f'{path_to_hyperparameters}/{sample}'\n",
    "path_of_results = f'{path_of_size}/results.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = data.import_hyperparameters(path_of_results, cv = True)\n",
    "n_layers = int(hyperparameters[\"n_layers\"])\n",
    "n_heads = int(hyperparameters[\"n_heads\"])\n",
    "embedding_size = int(hyperparameters[\"embedding_size\"])\n",
    "batch_size = int(hyperparameters[\"batch_size\"])\n",
    "epochs = int(hyperparameters[\"max_epochs_mean\"])\n",
    "n_features = X_train.shape[1]+1\n",
    "aggregator = \"cls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego-ngz/anaconda3/envs/tabtrans/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer was not TransformerEncoderLayer\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "#Let's load a model that it's already trained\n",
    "#parameters of the NN\n",
    "ff_pw_size = 30  #this value because of the paper\n",
    "attn_dropout = 0.3 #paper\n",
    "ff_dropout = 0.1 #paper value\n",
    "aggregator = \"cls\"\n",
    "aggregator_parameters = None\n",
    "decoder_hidden_units = [128,64] #paper value\n",
    "decoder_activation_fn = nn.ReLU()\n",
    "need_weights = False\n",
    "numerical_passthrough = False\n",
    "\n",
    "#module\n",
    "module = training.build_module(\n",
    "    n_categories, # List of number of categories\n",
    "    n_numerical, # Number of numerical features\n",
    "    n_heads, # Number of heads per layer\n",
    "    ff_pw_size, # Size of the MLP inside each transformer encoder layer\n",
    "    n_layers, # Number of transformer encoder layers    \n",
    "    n_labels, # Number of output neurons\n",
    "    embedding_size,\n",
    "    attn_dropout, \n",
    "    ff_dropout, \n",
    "    aggregator, # The aggregator for output vectors before decoder\n",
    "    rnn_aggregator_parameters=aggregator_parameters,\n",
    "    decoder_hidden_units=decoder_hidden_units,\n",
    "    decoder_activation_fn=decoder_activation_fn,\n",
    "    need_weights=need_weights,\n",
    "    numerical_passthrough=numerical_passthrough\n",
    ")\n",
    "\n",
    "#First lets define the model... should be the same as the one that was trained\n",
    "model = skorch.NeuralNetClassifier(\n",
    "        module = module,\n",
    "        criterion=torch.nn.CrossEntropyLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        device= \"cuda\", #cuda\" if torch.cuda.is_available() else\n",
    "        batch_size = batch_size,\n",
    "        train_split = None,\n",
    "        max_epochs = 10,\n",
    "        optimizer__lr=1e-4,\n",
    "        optimizer__weight_decay=1e-4,\n",
    "    )\n",
    "\n",
    "# Initialize the model\n",
    "model.initialize()\n",
    "\n",
    "#path to the file train_end_params.pt\n",
    "checkpoint_path = \"/home/diego-ngz/Git/thesis-tabtrans/Final_models_4/credit-g/tabtrans/final_tabtrans_cv/100/checkpoints/epoch_12/train_end_params.pt\"\n",
    "# Load the saved parameters\n",
    "\n",
    "model.load_params(f_params = checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = attention_file.attention_matrix(model, X_train, y_train, n_numerical, n_layers, n_heads, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ATTENTION CUBES\n",
    "\n",
    "\"\"\"\n",
    "Eneabling and extracting the attention cubes.\n",
    "\n",
    "To eneable the attention cubes recovering, the only requirement is to \n",
    "set the PyTorch module need_weights=True. When the cubes are required the\n",
    "new output will be:\n",
    "\n",
    "    - predictions: The predictionsfor the given instances\n",
    "    - layer outputs: The output of each encoder layer\n",
    "    - weights: The attention cube of each encoder\n",
    "\n",
    "In skorch, the trained PyTorch module is saved in the variable .module_.\n",
    "\n",
    "When using skorch, the only way to recover multiple outputs is by\n",
    "using the forward/forward_iter method.\n",
    "\"\"\"\n",
    "\n",
    "model.module_.need_weights = True\n",
    "cumulative_attns = []\n",
    "\n",
    "for X_inst, y_inst in zip(X_train, y_train):\n",
    "    pred, layer_outputs, attn = model.forward(X={\n",
    "        \"x_numerical\": X_inst[None, :n_numerical].astype(np.float32),\n",
    "        \"x_categorical\": X_inst[None, n_numerical:].astype(np.int32)\n",
    "        })\n",
    "        \n",
    "    \"\"\"\n",
    "    The attention cubes dimensions are:\n",
    "    \n",
    "    (num. layers, batch size, num. heads, num. features, num. features)\n",
    "    #Why does the batch size is 1?\n",
    "    \"\"\"\n",
    "    assert attn.shape == (n_layers, 1, n_heads, n_features, n_features) \n",
    "    \n",
    "    \"\"\"\n",
    "    To compute the cumulative attention we provide a function in:\n",
    "    \n",
    "        utils.attention.compute_std_attentions(attention, aggregator)\n",
    "        \n",
    "    The function returns:\n",
    "        The inidivual attention (non cumulative) of each layer. Shape:  (num layers, batch size, num. features)\n",
    "        The cumulative attention at each layer. Shape: (num layers, batch size, num. features)\n",
    "        \n",
    "    The last layerof the cumulative attention represents the cumulative attention over all\n",
    "    Transformer Encoders.\n",
    "    \"\"\"\n",
    "    \n",
    "    ind_attn, cum_attn = attention.compute_std_attentions(attn, aggregator)\n",
    "    \n",
    "    assert ind_attn.shape == (n_layers, 1, n_features)\n",
    "    assert cum_attn.shape == (n_layers, 1, n_features)\n",
    "    \n",
    "    cumulative_attns.append( cum_attn[-1, 0])\n",
    "    \n",
    "cumulative_attns = np.array(cumulative_attns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_attns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize KMeans with 4 clusters\n",
    "kmeans = KMeans(n_clusters=4, random_state=11)\n",
    "\n",
    "# Fit the model to the data\n",
    "kmeans.fit(cumulative_attns)\n",
    "\n",
    "# Get the cluster label for every instance in cumulative_attns\n",
    "cluster_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get the sorted indices based on cluster labels\n",
    "sorted_indices = np.argsort(cluster_labels) #returns the indices that would sort the array cluster_labels\n",
    "sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Use sorted indices of the clusters to reorder cumulative_attns and to order the y_train labels\n",
    "# Note that the same indices that order the clusters will make the arrays to be sorted in ascending order for clusters\n",
    "sorted_cumulative_attns = cumulative_attns[sorted_indices]\n",
    "sorted_y_train = y_train[sorted_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now order the cluster labels based on the sorted indices to have the instance ordered, and the clusters ordered and also the labels\n",
    "sorted_cluster_labels = cluster_labels[sorted_indices]\n",
    "sorted_cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here I will save the index of the last element of each cluster\n",
    "last_cluster_indices = []\n",
    "\n",
    "for i in range(len(sorted_cluster_labels)-1):\n",
    "    actual_value = sorted_cluster_labels[i]\n",
    "    next_value = sorted_cluster_labels[i+1]\n",
    "    \n",
    "    if actual_value != next_value:\n",
    "        last_cluster_indices.append(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted_cumulative_attns.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_cumm_vector = sorted_cumulative_attns.shape[1]\n",
    "\n",
    "# last_cluster_indices is a list of indices where you want to add NaN rows\n",
    "nan_row = np.full(len_cumm_vector, np.nan)\n",
    "\n",
    "# Insert NaN rows\n",
    "#It does it in reverse in order to not change the indices of the elements\n",
    "for i in sorted(last_cluster_indices, reverse=True):\n",
    "    sorted_cumulative_attns = np.insert(sorted_cumulative_attns, i + 1, nan_row, axis=0) #insert the NaN array row in the sorted_cumulative_attns\n",
    "    # OR Solution 2: Do it all at once\n",
    "    sorted_y_train = np.insert(sorted_y_train.astype(float), i + 1, np.nan, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_bound_index = []\n",
    "\n",
    "s = 1\n",
    "for i in last_cluster_indices:\n",
    "    upper_bound_index.append(i+s)\n",
    "    s+=1\n",
    "\n",
    "upper_bound_index.append(len(sorted_cumulative_attns)-1)\n",
    "\n",
    "print(upper_bound_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to collect clusters with NaNs in between groups\n",
    "final_sorted_cumulative_attns = []\n",
    "final_sorted_y_train = []  # New list for final_sorted_y_train\n",
    "\n",
    "# Copy the list to avoid changing the original list\n",
    "new_upper_bound_index = upper_bound_index.copy()\n",
    "\n",
    "# Process each cluster individually\n",
    "start_idx = 0\n",
    "for i in range(len(upper_bound_index)):\n",
    "    \n",
    "    end_idx = upper_bound_index[i]\n",
    "    \n",
    "    # Extract the cluster range for both data and labels\n",
    "    cluster_data = sorted_cumulative_attns[start_idx:end_idx]  # Extract the cluster (end_idx is exclusive)\n",
    "    cluster_labels = sorted_y_train[start_idx:end_idx]\n",
    "\n",
    "    # Separate data based on labels (assuming only 0's and 1's)\n",
    "    data_zeros = cluster_data[cluster_labels == 0]\n",
    "    data_ones = cluster_data[cluster_labels == 1]\n",
    "    labels_zeros = cluster_labels[cluster_labels == 0]\n",
    "    labels_ones = cluster_labels[cluster_labels == 1]\n",
    "\n",
    "    # Combine with a NaN row between 0's and 1's if both are present\n",
    "    if len(data_zeros) > 0 and len(data_ones) > 0:\n",
    "        combined_cluster = np.vstack([data_zeros, np.full((1, cluster_data.shape[1]), np.nan), data_ones])\n",
    "        combined_labels = np.concatenate([labels_zeros, [np.nan], labels_ones])\n",
    "\n",
    "        # Update the upper bound indexes:\n",
    "        for j in range(i, len(new_upper_bound_index)):\n",
    "            new_upper_bound_index[j] += 1\n",
    "\n",
    "    else:\n",
    "        combined_cluster = np.vstack([data_zeros, data_ones])  # No NaN row if only 0's or only 1's\n",
    "        combined_labels = np.concatenate([labels_zeros, labels_ones])\n",
    "\n",
    "    # Append the processed cluster data and labels to the final lists\n",
    "    final_sorted_cumulative_attns.append(combined_cluster)\n",
    "    final_sorted_y_train.append(combined_labels)  # Append labels with NaNs as well\n",
    "\n",
    "    # Insert an extra NaN row to separate clusters\n",
    "    final_sorted_cumulative_attns.append(np.full((1, cluster_data.shape[1]), np.nan))\n",
    "    final_sorted_y_train.append([np.nan])  # Add NaN row to labels\n",
    "\n",
    "    # Update the starting index for the next cluster\n",
    "    start_idx = upper_bound_index[i] + 1\n",
    "\n",
    "# Concatenate all parts to form the final sorted arrays with NaNs between clusters\n",
    "final_sorted_cumulative_attns = np.vstack(final_sorted_cumulative_attns[:-1])  # Remove the last NaN row\n",
    "final_sorted_y_train = np.concatenate(final_sorted_y_train[:-1])  # Remove the last NaN value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the intervals for every cluster\n",
    "number_of_clusters = 4\n",
    "\n",
    "clusters = {}\n",
    "\n",
    "initial_index = 0\n",
    "\n",
    "for i in range(number_of_clusters):\n",
    "    name = f\"cluster_{i}\"\n",
    "    \n",
    "    interval =[]\n",
    "    \n",
    "    if i == number_of_clusters:\n",
    "        end_index = new_upper_bound_index[i]\n",
    "    else:\n",
    "        end_index = new_upper_bound_index[i]-1\n",
    "    \n",
    "    interval.append(initial_index)\n",
    "    interval.append(end_index)\n",
    "    \n",
    "    clusters[name] = interval\n",
    "    \n",
    "    initial_index = end_index+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clusters)\n",
    "print(\"--------------------\")\n",
    "print(new_upper_bound_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store intervals for each cluster with respect to labels, ignoring NaNs\n",
    "label_intervals = {}\n",
    "\n",
    "# Extract intervals within each cluster based on labels\n",
    "for cluster, (start, end) in clusters.items():\n",
    "    cluster_data = final_sorted_y_train[start:end + 1]  # Extract the cluster slice\n",
    "    intervals = []  # List to store intervals within the cluster\n",
    "\n",
    "    # Initialize tracking variables\n",
    "    label_start = None\n",
    "    current_label = None\n",
    "\n",
    "    for idx in range(start, end + 1):\n",
    "        if np.isnan(final_sorted_y_train[idx]):  # Skip NaN values\n",
    "            continue\n",
    "\n",
    "        # If starting a new interval\n",
    "        if current_label is None:\n",
    "            current_label = final_sorted_y_train[idx]\n",
    "            label_start = idx\n",
    "        elif final_sorted_y_train[idx] != current_label:\n",
    "            # Close the current interval when label changes\n",
    "            intervals.append({\n",
    "                'label': current_label,\n",
    "                'start': label_start,\n",
    "                'end': idx - 1\n",
    "            })\n",
    "            # Start a new interval\n",
    "            current_label = final_sorted_y_train[idx]\n",
    "            label_start = idx\n",
    "\n",
    "    # Append the final interval if there’s an ongoing label sequence\n",
    "    if current_label is not None:\n",
    "        intervals.append({\n",
    "            'label': current_label,\n",
    "            'start': label_start,\n",
    "            'end': end\n",
    "        })\n",
    "\n",
    "    # Store intervals in the dictionary for the current cluster\n",
    "    label_intervals[cluster] = intervals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_intervals[\"cluster_0\"])\n",
    "for x in label_intervals[\"cluster_0\"]:\n",
    "    print(x)\n",
    "    print(type(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the plot\n",
    "# Create the Plotly Express figure\n",
    "fig = px.imshow(final_sorted_cumulative_attns, color_continuous_scale='Inferno')\n",
    "\n",
    "# Remove y-axis tick labels\n",
    "fig.update_layout(title = f\"{name_df}\",title_x=0.5, yaxis_showticklabels=False, yaxis=dict(showgrid=False),plot_bgcolor='white')\n",
    "\n",
    "image_width = final_sorted_cumulative_attns.shape[1]\n",
    "\n",
    "# Assume image_width is the width of the image (number of columns in sorted_cumulative_attns)\n",
    "right_x_position = image_width + 5  # Set position for right-side shapes\n",
    "\n",
    "#clusters variable is {'cluster_0': [0, 18], 'cluster_1': [20, 50], 'cluster_2': [52, 78], 'cluster_3': [80, 105]}\n",
    "# Add cluster labels and brackets\n",
    "\n",
    "for i in range(number_of_clusters):\n",
    "    interval = clusters[f\"cluster_{i}\"]\n",
    "    start_index = interval[0]\n",
    "    end_index = interval[1]\n",
    "    mid_index = (start_index + end_index) / 2\n",
    "\n",
    "    fig.add_annotation(x=-13, y=mid_index, text=f\"C{i}\", showarrow=False, font=dict(size=16, color='black'))\n",
    "    \n",
    "    #LEFT SIDE ANNOTATIONS\n",
    "    # Add bracket-like shapes resembling \" [ \"\n",
    "    fig.add_shape(type=\"line\", x0=-5, x1=-5, y0=start_index, y1=end_index, line=dict(color=\"black\", width=1))  # vertical part of the bracket\n",
    "    fig.add_shape(type=\"line\", x0=-5, x1=-4, y0=start_index, y1=start_index, line=dict(color=\"black\", width=1))     # upper horizontal part\n",
    "    fig.add_shape(type=\"line\", x0=-5, x1=-4, y0=end_index, y1=end_index, line=dict(color=\"black\", width=1))     # lower horizontal part\n",
    "    fig.add_shape(type=\"line\", x0=-6, x1=-5, y0=mid_index, y1=mid_index, line=dict(color=\"black\", width=1))     # middel line of the bracket\n",
    "    \n",
    "    \n",
    "    #RIGHT SIDE ANNOTATIONS\n",
    "    # Right-side shapes for the bracket\n",
    "    cluster_dict = label_intervals[f\"cluster_{i}\"]\n",
    "    #{'label': 0.0, 'start': 0, 'end': 1}\n",
    "    for labels_dict in cluster_dict:\n",
    "        label = f\"L{int(labels_dict['label'])}\"\n",
    "        start_index = labels_dict['start']\n",
    "        end_index = labels_dict['end']\n",
    "        mid_index = (start_index + end_index) / 2\n",
    "        \n",
    "        fig.add_annotation(x=right_x_position + 10, y=mid_index, text=label, showarrow=False, font=dict(size=16, color='black'))\n",
    "        fig.add_shape(type=\"line\", x0=right_x_position, x1=right_x_position, y0=start_index, y1=end_index, line=dict(color=\"black\", width=1))  # vertical part of the bracket    \n",
    "        fig.add_shape(type=\"line\", x0=right_x_position, x1=right_x_position - 1, y0=start_index, y1=start_index, line=dict(color=\"black\", width=1))  # upper horizontal part\n",
    "        fig.add_shape(type=\"line\", x0=right_x_position, x1=right_x_position - 1, y0=end_index, y1=end_index, line=dict(color=\"black\", width=1))  # lower horizontal part\n",
    "        fig.add_shape(type=\"line\", x0=right_x_position + 1, x1=right_x_position, y0=mid_index, y1=mid_index, line=dict(color=\"black\", width=1))  # middle line of the bracket\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabtrans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
