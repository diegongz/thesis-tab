{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/diego-ngz/Git/thesis-tabtrans\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "#path of the project\n",
    "project_path = \"/home/diego-ngz/Git/thesis-tabtrans\"\n",
    "\n",
    "sys.path.append(project_path) #This helps to be able to import the data from the parent directory to other files\n",
    "\n",
    "from utils import data, tabtrans_file, plots,attention, training, attention_file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "from skorch.callbacks import TrainEndCheckpoint, EarlyStopping, LoadInitState, Checkpoint\n",
    "import skorch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  \\nSteps to follow:\\n1. Load the data\\n2. separate between training and testing.\\n3. Separate the validation from the training set\\n4. Import the best hyperparameters\\n5. Train the model and in every epoch check the validation error, the training error, and the attention matrix\\n8. Stop the model when 10 epochs the entropy doesnt decrease\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''  \n",
    "Steps to follow:\n",
    "1. Load the data\n",
    "2. separate between training and testing.\n",
    "3. Separate the validation from the training set\n",
    "4. Import the best hyperparameters\n",
    "5. Train the model and in every epoch check the validation error, the training error, and the attention matrix\n",
    "8. Stop the model when 10 epochs the entropy doesnt decrease\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:openml.datasets.dataset:pickle write anneal\n"
     ]
    }
   ],
   "source": [
    "df_id = 2\n",
    "X_train, X_test, y_train, y_test, train_indices, val_indices, n_instances, n_labels, n_numerical, n_categories = data.import_data(df_id)\n",
    "'''\n",
    "The train_indices are the ones used for training the model\n",
    "The val_indices are the ones used for validation (is the 20% of the training set)\n",
    "'''\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "name_df = data.get_dataset_name(df_id)\n",
    "\n",
    "path_of_datset = f'{project_path}/Final_models_4/{name_df}'\n",
    "\n",
    "path_to_hyperparameters = f'{path_of_datset}/tabtrans/hyperparameter_selection'\n",
    "\n",
    "#define the path to final_tabtrans\n",
    "path_to_final_tabtrans = f'{path_of_datset}/tabtrans/final_tabtrans_cv'\n",
    "\n",
    "sample = 100\n",
    "path_of_hyper_size = f'{path_to_hyperparameters}/{sample}'\n",
    "path_of_hyper_results = f'{path_of_hyper_size}/results.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#parameters for the model\n",
    "ff_pw_size = 30  #this value because of the paper \n",
    "attn_dropout = 0.3 #paper\n",
    "ff_dropout = 0.1 #paper value\n",
    "aggregator = \"cls\"\n",
    "aggregator_parameters = None\n",
    "decoder_hidden_units = [128,64] #paper value [128,64]\n",
    "decoder_activation_fn = nn.ReLU()\n",
    "need_weights = False\n",
    "numerical_passthrough = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the hyperparameters\n",
    "hyperparameters = data.import_hyperparameters(path_of_hyper_results, cv = True)\n",
    "\n",
    "\n",
    "n_layers = int(hyperparameters[\"n_layers\"])\n",
    "n_heads = int(hyperparameters[\"n_heads\"])\n",
    "embedding_size = int(hyperparameters[\"embedding_size\"])\n",
    "batch_size = int(hyperparameters[\"batch_size\"])\n",
    "epochs = int(hyperparameters[\"max_epochs_mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego-ngz/anaconda3/envs/tabtrans/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer was not TransformerEncoderLayer\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "#module\n",
    "module = training.build_module(\n",
    "    n_categories, # List of number of categories\n",
    "    n_numerical, # Number of numerical features\n",
    "    n_heads, # Number of heads per layer\n",
    "    ff_pw_size, # Size of the MLP inside each transformer encoder layer\n",
    "    n_layers, # Number of transformer encoder layers    \n",
    "    n_labels, # Number of output neurons\n",
    "    embedding_size,\n",
    "    attn_dropout, \n",
    "    ff_dropout, \n",
    "    aggregator, # The aggregator for output vectors before decoder\n",
    "    rnn_aggregator_parameters=aggregator_parameters,\n",
    "    decoder_hidden_units=decoder_hidden_units,\n",
    "    decoder_activation_fn=decoder_activation_fn,\n",
    "    need_weights=need_weights,\n",
    "    numerical_passthrough=numerical_passthrough\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.9362\u001b[0m  0.4392\n",
      "      2        \u001b[36m0.7370\u001b[0m  0.2628\n",
      "      3        \u001b[36m0.5795\u001b[0m  0.2636\n",
      "      4        \u001b[36m0.4020\u001b[0m  0.2642\n",
      "      5        \u001b[36m0.2729\u001b[0m  0.2641\n",
      "      6        \u001b[36m0.1874\u001b[0m  0.2642\n",
      "      7        \u001b[36m0.1438\u001b[0m  0.2622\n",
      "      8        \u001b[36m0.1151\u001b[0m  0.2645\n",
      "      9        \u001b[36m0.0985\u001b[0m  0.2635\n",
      "     10        \u001b[36m0.0865\u001b[0m  0.2645\n",
      "--------------------------------------------------------------------\n",
      "Model saved for 10 epochs\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "     11        \u001b[36m0.0693\u001b[0m  0.3684\n",
      "     12        \u001b[36m0.0567\u001b[0m  0.3843\n",
      "     13        \u001b[36m0.0423\u001b[0m  0.3850\n",
      "     14        \u001b[36m0.0329\u001b[0m  0.3840\n",
      "     15        0.0336  0.3853\n",
      "     16        \u001b[36m0.0234\u001b[0m  0.3821\n",
      "     17        0.0282  0.3852\n",
      "     18        \u001b[36m0.0176\u001b[0m  0.3849\n",
      "     19        0.0277  0.3853\n",
      "     20        0.0177  0.3862\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "     21        \u001b[36m0.0186\u001b[0m  0.3861\n",
      "     22        \u001b[36m0.0155\u001b[0m  0.3858\n",
      "     23        \u001b[36m0.0092\u001b[0m  0.3856\n",
      "     24        0.0096  0.3858\n",
      "     25        0.0150  0.3864\n",
      "     26        0.0114  0.3859\n",
      "     27        0.0093  0.4231\n",
      "     28        \u001b[36m0.0068\u001b[0m  0.5037\n",
      "     29        \u001b[36m0.0053\u001b[0m  0.5044\n",
      "     30        0.0079  0.5011\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "     31        \u001b[36m0.0130\u001b[0m  0.3803\n",
      "     32        \u001b[36m0.0088\u001b[0m  0.3858\n",
      "     33        0.0093  0.3863\n",
      "     34        \u001b[36m0.0048\u001b[0m  0.3862\n",
      "     35        0.0076  0.3830\n",
      "     36        0.0074  0.3871\n",
      "     37        0.0053  0.3818\n",
      "     38        0.0056  0.3799\n",
      "     39        \u001b[36m0.0022\u001b[0m  0.3868\n",
      "     40        0.0243  0.3861\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "     41        \u001b[36m0.0331\u001b[0m  0.3862\n",
      "     42        \u001b[36m0.0284\u001b[0m  0.3851\n",
      "     43        \u001b[36m0.0143\u001b[0m  0.3852\n",
      "     44        \u001b[36m0.0128\u001b[0m  0.3877\n",
      "     45        \u001b[36m0.0123\u001b[0m  0.3863\n",
      "     46        \u001b[36m0.0086\u001b[0m  0.3841\n",
      "     47        \u001b[36m0.0038\u001b[0m  0.3873\n",
      "     48        0.0125  0.3859\n",
      "     49        0.0077  0.3873\n",
      "     50        0.0083  0.3861\n",
      "     51        0.0045  0.3841\n",
      "     52        \u001b[36m0.0013\u001b[0m  0.3871\n",
      "     53        0.0018  0.3873\n"
     ]
    }
   ],
   "source": [
    "path_to_checkpoint = f\"{path_of_datset}/entropy\" #create the path to save the checkpoint\n",
    "os.makedirs(path_to_checkpoint, exist_ok = True)\n",
    "\n",
    "intervals_size = epochs // 5\n",
    "epochs_to_save = [intervals_size, 2*intervals_size, 3*intervals_size, 4*intervals_size, epochs]\n",
    "\n",
    "train_end_cp = TrainEndCheckpoint(dirname = f\"{path_to_checkpoint}/epoch_{intervals_size}\")\n",
    "\n",
    "\n",
    "model = skorch.NeuralNetClassifier(\n",
    "    module = module,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    device= \"cuda\", #cuda\" if torch.cuda.is_available() else\n",
    "    batch_size = batch_size,\n",
    "    train_split = None,\n",
    "    max_epochs = intervals_size,\n",
    "    optimizer__lr=1e-4,\n",
    "    optimizer__weight_decay=1e-4,\n",
    "    callbacks=[train_end_cp]\n",
    "    )\n",
    "\n",
    "model = model.fit(X={\n",
    "    \"x_numerical\": X_train[:, :n_numerical].astype(np.float32),\n",
    "    \"x_categorical\": X_train[:, n_numerical:].astype(np.int32)\n",
    "    }, \n",
    "    y=y_train.astype(np.int64)     \n",
    "    )\n",
    "\n",
    "\n",
    "print(\"--------------------------------------------------------------------\")\n",
    "print(f\"Model saved for {intervals_size} epochs\")\n",
    "         \n",
    "for i in range(1,len(epochs_to_save)):\n",
    "    \n",
    "    epoch = epochs_to_save[i]\n",
    "    \n",
    "    load_state = LoadInitState(train_end_cp) #load the state of the past model\n",
    "    train_end_cp = TrainEndCheckpoint(dirname = f\"{path_to_checkpoint}/epoch_{epoch}\")\n",
    "    \n",
    "    #create the model\n",
    "    model = skorch.NeuralNetClassifier(\n",
    "            module = module,\n",
    "            criterion=torch.nn.CrossEntropyLoss,\n",
    "            optimizer=torch.optim.AdamW,\n",
    "            device= \"cuda\", #cuda\" if torch.cuda.is_available() else\n",
    "            batch_size = batch_size,\n",
    "            train_split = None,\n",
    "            max_epochs = epochs_to_save[i]-epochs_to_save[i-1], #It will train for the difference between the epochs given that it will start where the last end\n",
    "            optimizer__lr=1e-4,\n",
    "            optimizer__weight_decay=1e-4,\n",
    "            callbacks=[load_state, train_end_cp]\n",
    "            )\n",
    "\n",
    "    model = model.fit(X={\n",
    "        \"x_numerical\": X_train[:, :n_numerical].astype(np.float32),\n",
    "        \"x_categorical\": X_train[:, n_numerical:].astype(np.int32)\n",
    "        }, \n",
    "        y=y_train.astype(np.int64)\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_avg_entropy = []\n",
    "\n",
    "matrix = attention_file.attention_matrix(model, X_train[train_indices], y_train[train_indices], n_numerical, n_layers, n_heads, n_features+1)\n",
    "entropy_per_row = np.apply_along_axis(entropy, 1, matrix, base=2) / np.log2(n_features)\n",
    "average_entropy = np.mean(entropy_per_row)\n",
    "\n",
    "\n",
    "epoch_avg_entropy.append(average_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_state = LoadInitState(train_end_cp) #load the state of the past model\n",
    "train_end_cp = TrainEndCheckpoint(dirname = f\"{path_to_checkpoint}/epoch_{4}\")\n",
    "\n",
    "#create the model\n",
    "model = skorch.NeuralNetClassifier(\n",
    "        module = module,\n",
    "        criterion=torch.nn.CrossEntropyLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        device= \"cuda\", #cuda\" if torch.cuda.is_available() else\n",
    "        batch_size = batch_size,\n",
    "        train_split = None,\n",
    "        max_epochs = 3, #It will train for the difference between the epochs given that it will start where the last end\n",
    "        optimizer__lr=1e-4,\n",
    "        optimizer__weight_decay=1e-4,\n",
    "        callbacks=[load_state, train_end_cp]\n",
    "        )\n",
    "\n",
    "model = model.fit(X={\n",
    "    \"x_numerical\": X_train[:, :n_numerical].astype(np.float32),\n",
    "    \"x_categorical\": X_train[:, n_numerical:].astype(np.int32)\n",
    "    }, \n",
    "    y=y_train.astype(np.int64)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = skorch.NeuralNetClassifier(\n",
    "    module=module,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    device = \"cuda\", #if torch.cuda.is_available() else \"cpu\",\n",
    "    batch_size = batch_size,\n",
    "    max_epochs = 1,\n",
    "    train_split=skorch.dataset.ValidSplit(((train_indices, val_indices),)),\n",
    "    callbacks=[load_state,\n",
    "    ],\n",
    "    optimizer__lr=1e-4,\n",
    "    optimizer__weight_decay=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.fit(X={\n",
    "    \"x_numerical\": X_train[:, :n_numerical].astype(np.float32),\n",
    "    \"x_categorical\": X_train[:, n_numerical:].astype(np.int32)\n",
    "    }, \n",
    "    y=y_train.astype(np.int64)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "while epoch_counter <= max_epochs:\n",
    "    load_state = LoadInitState(train_end_cp) #load the state of the past model\n",
    "    train_end_cp = TrainEndCheckpoint(dirname = path_to_checkpoint)\n",
    "    \n",
    "    #Train the first model    \n",
    "    model = skorch.NeuralNetClassifier(\n",
    "        module=module,\n",
    "        criterion=torch.nn.CrossEntropyLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        device = \"cuda\", #if torch.cuda.is_available() else \"cpu\",\n",
    "        batch_size = batch_size,\n",
    "        max_epochs = 1,\n",
    "        train_split=skorch.dataset.ValidSplit(((train_indices, val_indices),)),\n",
    "        callbacks=[load_state, train_end_cp,\n",
    "            (\"balanced_accuracy\", skorch.callbacks.EpochScoring(\"balanced_accuracy\", lower_is_better=False)),\n",
    "            (\"duration\", skorch.callbacks.EpochTimer()),\n",
    "            EpochScoring(scoring='accuracy', name='train_acc', on_train=True),\n",
    "            #Checkpoint(dirname = path_of_checkpoint, load_best = True), \n",
    "            EarlyStopping(patience=10)\n",
    "\n",
    "        ],\n",
    "        optimizer__lr=1e-4,\n",
    "        optimizer__weight_decay=1e-4\n",
    "    )\n",
    "\n",
    "    #Trainning \n",
    "    model = model.fit(X={\n",
    "        \"x_numerical\": X_train[:, :n_numerical].astype(np.float32),\n",
    "        \"x_categorical\": X_train[:, n_numerical:].astype(np.int32)\n",
    "        }, \n",
    "        y=y_train.astype(np.int64)\n",
    "        )\n",
    "    \n",
    "    matrix = attention_file.attention_matrix(model, X_train[train_indices], y_train[train_indices], n_numerical, n_layers, n_heads, n_features+1)\n",
    "    entropy_per_row = np.apply_along_axis(entropy, 1, matrix, base=2) / np.log2(n_features)\n",
    "    average_entropy = np.mean(entropy_per_row)\n",
    "    epoch_avg_entropy.append(average_entropy)\n",
    "    \n",
    "    epoch_counter += 1\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 5\n",
    "\n",
    "path_to_checkpoint = f\"{path_of_datset}/entropy\" #create the path to save the checkpoint\n",
    "os.makedirs(path_to_checkpoint, exist_ok = True)\n",
    "\n",
    "#first lest define the train end checkpoint\n",
    "train_end_cp = TrainEndCheckpoint(dirname = path_to_checkpoint)\n",
    "\n",
    "average_entropy = []\n",
    "\n",
    "\n",
    "model = skorch.NeuralNetClassifier(\n",
    "    module=module,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    device = \"cuda\", #if torch.cuda.is_available() else \"cpu\",\n",
    "    batch_size = batch_size,\n",
    "    max_epochs = 1,\n",
    "    train_split=skorch.dataset.ValidSplit(((train_indices, val_indices),)),\n",
    "    callbacks=[train_end_cp,\n",
    "        (\"balanced_accuracy\", skorch.callbacks.EpochScoring(\"balanced_accuracy\", lower_is_better=False)),\n",
    "        (\"duration\", skorch.callbacks.EpochTimer()),\n",
    "        EpochScoring(scoring='accuracy', name='train_acc', on_train=True),\n",
    "        #Checkpoint(dirname = path_of_checkpoint, load_best = True), \n",
    "        EarlyStopping(patience=10)\n",
    "\n",
    "    ],\n",
    "    optimizer__lr=1e-4,\n",
    "    optimizer__weight_decay=1e-4\n",
    ")\n",
    "\n",
    "#Trainning \n",
    "model = model.fit(X={\n",
    "    \"x_numerical\": X_train[:, :n_numerical].astype(np.float32),\n",
    "    \"x_categorical\": X_train[:, n_numerical:].astype(np.int32)\n",
    "    }, \n",
    "    y=y_train.astype(np.int64)\n",
    "    )\n",
    "\n",
    "\n",
    "average_entropy.append(entropy)\n",
    "\n",
    "print(average_entropy)\n",
    "''' \n",
    "epoch_counter = 1\n",
    "\n",
    "while epoch_counter <= max_epochs:\n",
    "    load_state = LoadInitState(train_end_cp) #load the state of the past model\n",
    "    \n",
    "    #Train the first model    \n",
    "    model = skorch.NeuralNetClassifier(\n",
    "        module=module,\n",
    "        criterion=torch.nn.CrossEntropyLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        device = \"cuda\", #if torch.cuda.is_available() else \"cpu\",\n",
    "        batch_size = batch_size,\n",
    "        max_epochs = 1,\n",
    "        train_split=skorch.dataset.ValidSplit(((train_indices, val_indices),)),\n",
    "        callbacks=[\n",
    "            load_state,\n",
    "            train_end_cp,\n",
    "            (\"balanced_accuracy\", skorch.callbacks.EpochScoring(\"balanced_accuracy\", lower_is_better=False)),\n",
    "            (\"duration\", skorch.callbacks.EpochTimer()),\n",
    "            EpochScoring(scoring='accuracy', name='train_acc', on_train=True),\n",
    "        ],\n",
    "        optimizer__lr=1e-4,\n",
    "        optimizer__weight_decay=1e-4\n",
    "    )\n",
    "\n",
    "    model = model.fit(X={\n",
    "        \"x_numerical\": X_train[:, :n_numerical].astype(np.float32),\n",
    "        \"x_categorical\": X_train[:, n_numerical:].astype(np.int32)\n",
    "        }, \n",
    "        y=y_train.astype(np.int64)\n",
    "        )\n",
    "    \n",
    "    entropy = attention_file.entropy_attention_matrix(model, X_train, y_train, n_numerical, n_layers, n_heads, n_features)\n",
    "    average_entropy.append(entropy)\n",
    "    \n",
    "    epoch_counter += 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = []\n",
    "\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "for x in model.history:\n",
    "    epoch_num = x[\"epoch\"]\n",
    "    epochs.append(epoch_num)\n",
    "\n",
    "    train_acc.append(x['train_acc'])\n",
    "    val_acc.append(x['valid_acc'])\n",
    "\n",
    "    train_loss.append(x[\"train_loss\"])\n",
    "    val_loss.append(x[\"valid_loss\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabtrans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
