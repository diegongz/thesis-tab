{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/diego-ngz/Git/thesis-tabtrans\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "#path of the project\n",
    "project_path = \"/home/diego-ngz/Git/thesis-tabtrans\"\n",
    "\n",
    "sys.path.append(project_path) #This helps to be able to import the data from the parent directory to other files\n",
    "\n",
    "from utils import data, tabtrans_file, plots,attention, training, attention_file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "from skorch.callbacks import Checkpoint, TrainEndCheckpoint, EarlyStopping, LoadInitState, EpochScoring, Checkpoint\n",
    "import skorch\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  \\nSteps to follow:\\n1. Load the data\\n2. separate between training and testing.\\n3. Separate the validation from the training set\\n4. Import the best hyperparameters\\n5. Train the model and in every epoch check the validation error, the training error, and the attention matrix\\n8. Stop the model when 10 epochs the entropy doesnt decrease\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''  \n",
    "Steps to follow:\n",
    "1. Load the data\n",
    "2. separate between training and testing.\n",
    "3. Separate the validation from the training set\n",
    "4. Import the best hyperparameters\n",
    "5. Train the model and in every epoch check the validation error, the training error, and the attention matrix\n",
    "8. Stop the model when 10 epochs the entropy doesnt decrease\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:openml.datasets.dataset:pickle write anneal\n"
     ]
    }
   ],
   "source": [
    "df_id = 2\n",
    "X_train, X_test, y_train, y_test, train_indices, val_indices, n_instances, n_labels, n_numerical, n_categories = data.import_data(df_id)\n",
    "'''\n",
    "The train_indices are the ones used for training the model\n",
    "The val_indices are the ones used for validation (is the 20% of the training set)\n",
    "'''\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "name_df = data.get_dataset_name(df_id)\n",
    "\n",
    "path_of_datset = f'{project_path}/Final_models_4/{name_df}'\n",
    "\n",
    "path_to_hyperparameters = f'{path_of_datset}/tabtrans/hyperparameter_selection'\n",
    "\n",
    "#define the path to final_tabtrans\n",
    "path_to_final_tabtrans = f'{path_of_datset}/tabtrans/final_tabtrans_cv'\n",
    "\n",
    "sample = 100\n",
    "path_of_hyper_size = f'{path_to_hyperparameters}/{sample}'\n",
    "path_of_hyper_results = f'{path_of_hyper_size}/results.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the hyperparameters\n",
    "hyperparameters = data.import_hyperparameters(path_of_hyper_results, cv = True)\n",
    "\n",
    "\n",
    "n_layers = int(hyperparameters[\"n_layers\"])\n",
    "n_heads = int(hyperparameters[\"n_heads\"])\n",
    "embedding_size = int(hyperparameters[\"embedding_size\"])\n",
    "batch_size = int(hyperparameters[\"batch_size\"])\n",
    "epochs = int(hyperparameters[\"max_epochs_mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego-ngz/anaconda3/envs/tabtrans/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer was not TransformerEncoderLayer\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "#Define the model\n",
    "\n",
    "#parameters for the model\n",
    "ff_pw_size = 30  #this value because of the paper \n",
    "attn_dropout = 0.3 #paper\n",
    "ff_dropout = 0.1 #paper value\n",
    "aggregator = \"cls\"\n",
    "aggregator_parameters = None\n",
    "decoder_hidden_units = [128,64] #paper value [128,64]\n",
    "decoder_activation_fn = nn.ReLU()\n",
    "need_weights = False\n",
    "numerical_passthrough = False\n",
    "\n",
    "#module\n",
    "module = training.build_module(\n",
    "    n_categories, # List of number of categories\n",
    "    n_numerical, # Number of numerical features\n",
    "    n_heads, # Number of heads per layer\n",
    "    ff_pw_size, # Size of the MLP inside each transformer encoder layer\n",
    "    n_layers, # Number of transformer encoder layers    \n",
    "    n_labels, # Number of output neurons\n",
    "    embedding_size,\n",
    "    attn_dropout, \n",
    "    ff_dropout, \n",
    "    aggregator, # The aggregator for output vectors before decoder\n",
    "    rnn_aggregator_parameters=aggregator_parameters,\n",
    "    decoder_hidden_units=decoder_hidden_units,\n",
    "    decoder_activation_fn=decoder_activation_fn,\n",
    "    need_weights=need_weights,\n",
    "    numerical_passthrough=numerical_passthrough\n",
    ") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import EpochScoring\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Custom EpochScoring class\n",
    "class CustomEpochScoring(EpochScoring):\n",
    "    def __init__(self,X, y, scoring_func, n_numerical, n_layers, n_heads, n_features, **kwargs):\n",
    "        super().__init__(scoring_func, **kwargs)  # Initialize the base class with your custom scoring function\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n_numerical = n_numerical\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.n_features = n_features\n",
    "\n",
    "    def on_epoch_end(self, net, dataset_train=None, dataset_valid=None, **kwargs):\n",
    "        \n",
    "        # Call your custom entropy function with the extra parameters\n",
    "        entropy_value = attention_file.entropy_attention_matrix(\n",
    "            net, self.X, self.y, self.n_numerical, self.n_layers, self.n_heads, self.n_features\n",
    "        )\n",
    "        \n",
    "        # You can now log or use the `entropy_value` as needed\n",
    "        print(f\"Entropy for this epoch: {entropy_value}\")\n",
    "\n",
    "        # Optionally, record the score in the history or process further\n",
    "        # Use this for logging the entropy score into the history\n",
    "        self._record_score(net.history, entropy_value)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = skorch.NeuralNetClassifier(\n",
    "    module=module,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    device = \"cuda\", #if torch.cuda.is_available() else \"cpu\",\n",
    "    batch_size = batch_size,\n",
    "    max_epochs = epochs,\n",
    "    train_split=skorch.dataset.ValidSplit(((train_indices, val_indices),)),\n",
    "    callbacks=[\n",
    "        CustomEpochScoring(\n",
    "            scoring_func=attention_file.entropy_attention_matrix,\n",
    "            name=\"entropy_attention\",  # Give it a name\n",
    "            X = X_train,\n",
    "            y = y_train,\n",
    "            on_train=True,  # Whether you want to compute this during training or validation\n",
    "            n_numerical=10,  # Just as an example, pass the additional parameters\n",
    "            n_layers=12,\n",
    "            n_heads=8,\n",
    "            n_features=256\n",
    "        )\n",
    "\n",
    "    ],\n",
    "    optimizer__lr=1e-4,\n",
    "    optimizer__weight_decay=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (22) must match the size of tensor b (26) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m      2\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_numerical\u001b[39m\u001b[38;5;124m\"\u001b[39m: X_train[:, :n_numerical]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m      3\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_categorical\u001b[39m\u001b[38;5;124m\"\u001b[39m: X_train[:, n_numerical:]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m      4\u001b[0m             }, \n\u001b[1;32m      5\u001b[0m             y\u001b[38;5;241m=\u001b[39my_train\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint64)\n\u001b[1;32m      6\u001b[0m             )\n",
      "File \u001b[0;32m~/anaconda3/envs/tabtrans/lib/python3.11/site-packages/skorch/classifier.py:165\u001b[0m, in \u001b[0;36mNeuralNetClassifier.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See ``NeuralNet.fit``.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03mIn contrast to ``NeuralNet.fit``, ``y`` is non-optional to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m \n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# pylint: disable=useless-super-delegation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# this is actually a pylint bug:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# https://github.com/PyCQA/pylint/issues/1085\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(NeuralNetClassifier, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "File \u001b[0;32m~/anaconda3/envs/tabtrans/lib/python3.11/site-packages/skorch/net.py:1319\u001b[0m, in \u001b[0;36mNeuralNet.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarm_start \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialized_:\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize()\n\u001b[0;32m-> 1319\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_fit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/tabtrans/lib/python3.11/site-packages/skorch/net.py:1278\u001b[0m, in \u001b[0;36mNeuralNet.partial_fit\u001b[0;34m(self, X, y, classes, **fit_params)\u001b[0m\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_train_begin\u001b[39m\u001b[38;5;124m'\u001b[39m, X\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my)\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tabtrans/lib/python3.11/site-packages/skorch/net.py:1196\u001b[0m, in \u001b[0;36mNeuralNet.fit_loop\u001b[0;34m(self, X, y, epochs, **fit_params)\u001b[0m\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single_epoch(iterator_train, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1191\u001b[0m                           step_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m   1193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single_epoch(iterator_valid, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1194\u001b[0m                           step_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_step, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m-> 1196\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mon_epoch_kwargs)\n\u001b[1;32m   1197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/tabtrans/lib/python3.11/site-packages/skorch/net.py:386\u001b[0m, in \u001b[0;36mNeuralNet.notify\u001b[0;34m(self, method_name, **cb_kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, method_name)(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcb_kwargs)\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks_:\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(cb, method_name)(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcb_kwargs)\n",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m, in \u001b[0;36mCustomEpochScoring.on_epoch_end\u001b[0;34m(self, net, dataset_train, dataset_valid, **kwargs)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_epoch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, net, dataset_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dataset_valid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     18\u001b[0m     \n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Call your custom entropy function with the extra parameters\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     entropy_value \u001b[38;5;241m=\u001b[39m attention_file\u001b[38;5;241m.\u001b[39mentropy_attention_matrix(\n\u001b[1;32m     21\u001b[0m         net, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_numerical, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features\n\u001b[1;32m     22\u001b[0m     )\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# You can now log or use the `entropy_value` as needed\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntropy for this epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentropy_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Git/thesis-tabtrans/utils/attention_file.py:90\u001b[0m, in \u001b[0;36mentropy_attention_matrix\u001b[0;34m(net, X_train, y_train, n_numerical, n_layers, n_heads, n_features)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mentropy_attention_matrix\u001b[39m(net, X_train, y_train, n_numerical, n_layers, n_heads, n_features):\n\u001b[0;32m---> 90\u001b[0m     matrix \u001b[38;5;241m=\u001b[39m attention_matrix(net, X_train, y_train, n_numerical, n_layers, n_heads, n_features)\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Output Shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, net\u001b[38;5;241m.\u001b[39mpredict(X_train)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget Labels Shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_train\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/Git/thesis-tabtrans/utils/attention_file.py:33\u001b[0m, in \u001b[0;36mattention_matrix\u001b[0;34m(model, X_train, y_train, n_numerical, n_layers, n_heads, n_features)\u001b[0m\n\u001b[1;32m     30\u001b[0m cumulative_attns \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_inst, y_inst \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X_train, y_train):\n\u001b[0;32m---> 33\u001b[0m     pred, layer_outputs, attn \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(X\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_numerical\u001b[39m\u001b[38;5;124m\"\u001b[39m: X_inst[\u001b[38;5;28;01mNone\u001b[39;00m, :n_numerical]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_categorical\u001b[39m\u001b[38;5;124m\"\u001b[39m: X_inst[\u001b[38;5;28;01mNone\u001b[39;00m, n_numerical:]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m     36\u001b[0m         })\n\u001b[1;32m     38\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    The attention cubes dimensions are:\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    (num. layers, batch size, num. heads, num. features, num. features)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m    #Why does the batch size is 1?\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(attn\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/tabtrans/lib/python3.11/site-packages/skorch/net.py:1486\u001b[0m, in \u001b[0;36mNeuralNet.forward\u001b[0;34m(self, X, training, device)\u001b[0m\n\u001b[1;32m   1444\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   1445\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Gather and concatenate the output from forward call with\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;124;03m    input data.\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1484\u001b[0m \n\u001b[1;32m   1485\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1486\u001b[0m     y_infer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_iter(X, training\u001b[38;5;241m=\u001b[39mtraining, device\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m   1488\u001b[0m     is_multioutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_infer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y_infer[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mtuple\u001b[39m)\n\u001b[1;32m   1489\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_multioutput:\n",
      "File \u001b[0;32m~/anaconda3/envs/tabtrans/lib/python3.11/site-packages/skorch/net.py:1441\u001b[0m, in \u001b[0;36mNeuralNet.forward_iter\u001b[0;34m(self, X, training, device)\u001b[0m\n\u001b[1;32m   1439\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(dataset, training\u001b[38;5;241m=\u001b[39mtraining)\n\u001b[1;32m   1440\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[0;32m-> 1441\u001b[0m     yp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_step(batch, training\u001b[38;5;241m=\u001b[39mtraining)\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m to_device(yp, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/anaconda3/envs/tabtrans/lib/python3.11/site-packages/skorch/net.py:1134\u001b[0m, in \u001b[0;36mNeuralNet.evaluation_step\u001b[0;34m(self, batch, training)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(training):\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_training(training)\n\u001b[0;32m-> 1134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(Xi)\n",
      "File \u001b[0;32m~/anaconda3/envs/tabtrans/lib/python3.11/site-packages/skorch/net.py:1520\u001b[0m, in \u001b[0;36mNeuralNet.infer\u001b[0;34m(self, x, **fit_params)\u001b[0m\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, Mapping):\n\u001b[1;32m   1519\u001b[0m     x_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_x_and_fit_params(x, fit_params)\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mx_dict)\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "File \u001b[0;32m~/anaconda3/envs/tabtrans/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tabtrans/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tabtrans/lib/python3.11/site-packages/ndsl/architecture/attention.py:224\u001b[0m, in \u001b[0;36mTabularTransformer.forward\u001b[0;34m(self, x_categorical, x_numerical)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_categorical, x_numerical):\n\u001b[1;32m    222\u001b[0m \n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# src came with two dims: (batch_size, num_features)\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical_encoder(x_categorical \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories_offset)\n\u001b[1;32m    226\u001b[0m     numerical_embedding \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumerical_passthrough:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (22) must match the size of tensor b (26) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "model = model.fit(X={\n",
    "            \"x_numerical\": X_train[:, :n_numerical].astype(np.float32),\n",
    "            \"x_categorical\": X_train[:, n_numerical:].astype(np.int32)\n",
    "            }, \n",
    "            y=y_train.astype(np.int64)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "        CustomEpochScoring(\n",
    "            scoring_func=attention_file.entropy_attention_matrix,\n",
    "            name=\"entropy_attention\",  # Give it a name\n",
    "            X = X_train,\n",
    "            y = y_train,\n",
    "            on_train=True,  # Whether you want to compute this during training or validation\n",
    "            n_numerical=10,  # Just as an example, pass the additional parameters\n",
    "            n_layers=12,\n",
    "            n_heads=8,\n",
    "            n_features=256\n",
    "        )\n",
    "''' "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabtrans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
