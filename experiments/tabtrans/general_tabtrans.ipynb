{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/diego/Git/thesis-tabtrans\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the project directory\n",
    "project_path = \"/home/diego/Git/thesis-tabtrans\"\n",
    "\n",
    "sys.path.append(project_path) #This helps to be able to import the data from the parent directory to other files\n",
    "\n",
    "from utils import tabtrans_file, data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:openml.datasets.dataset:pickle write lsvt\n"
     ]
    }
   ],
   "source": [
    "id = 1484\n",
    "\n",
    "X_train, X_test, y_train, y_test, train_indices, val_indices, n_instances, n_labels, n_numerical, n_categories = data.import_data(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Size: 128\n",
      "Number of Layers: 2\n",
      "Number of Heads: 4\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7124\u001b[0m  0.1195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego/anaconda3/envs/tabtrans/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer was not TransformerEncoderLayer\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2        \u001b[36m0.6834\u001b[0m  0.1158\n",
      "      3        \u001b[36m0.6671\u001b[0m  0.1128\n",
      "      4        \u001b[36m0.6507\u001b[0m  0.1099\n",
      "      5        \u001b[36m0.6435\u001b[0m  0.1094\n",
      "      6        \u001b[36m0.6374\u001b[0m  0.1090\n",
      "      7        0.6416  0.1089\n",
      "      8        0.6400  0.1089\n",
      "      9        0.6381  0.1091\n",
      "     10        \u001b[36m0.6345\u001b[0m  0.1090\n",
      "     11        \u001b[36m0.6342\u001b[0m  0.1091\n",
      "     12        \u001b[36m0.6332\u001b[0m  0.1090\n",
      "     13        0.6347  0.1089\n",
      "     14        0.6345  0.1089\n",
      "     15        \u001b[36m0.6326\u001b[0m  0.1091\n",
      "     16        \u001b[36m0.6323\u001b[0m  0.1088\n",
      "     17        \u001b[36m0.6308\u001b[0m  0.1089\n",
      "     18        0.6317  0.1089\n",
      "     19        \u001b[36m0.6277\u001b[0m  0.1090\n",
      "     20        \u001b[36m0.6267\u001b[0m  0.1089\n",
      "     21        0.6274  0.1091\n",
      "     22        \u001b[36m0.6241\u001b[0m  0.1089\n",
      "     23        0.6251  0.1088\n",
      "     24        \u001b[36m0.6215\u001b[0m  0.1089\n",
      "     25        \u001b[36m0.6170\u001b[0m  0.1088\n",
      "     26        \u001b[36m0.6125\u001b[0m  0.1095\n",
      "     27        \u001b[36m0.6050\u001b[0m  0.1089\n",
      "     28        \u001b[36m0.5952\u001b[0m  0.1096\n",
      "     29        \u001b[36m0.5757\u001b[0m  0.1088\n",
      "     30        \u001b[36m0.5557\u001b[0m  0.1090\n",
      "     31        \u001b[36m0.5424\u001b[0m  0.1091\n",
      "     32        \u001b[36m0.5091\u001b[0m  0.1091\n",
      "     33        \u001b[36m0.4976\u001b[0m  0.1088\n",
      "     34        \u001b[36m0.4365\u001b[0m  0.1089\n",
      "     35        0.4540  0.1089\n",
      "     36        \u001b[36m0.4063\u001b[0m  0.1090\n",
      "     37        0.4105  0.1088\n",
      "     38        \u001b[36m0.3927\u001b[0m  0.1089\n",
      "     39        \u001b[36m0.3825\u001b[0m  0.1088\n",
      "     40        0.3835  0.1090\n",
      "     41        \u001b[36m0.3355\u001b[0m  0.1090\n",
      "     42        0.3457  0.1089\n",
      "     43        \u001b[36m0.3084\u001b[0m  0.1090\n",
      "     44        \u001b[36m0.2566\u001b[0m  0.1092\n",
      "     45        0.3290  0.1093\n",
      "     46        0.3578  0.1092\n",
      "     47        0.2988  0.1088\n",
      "     48        0.3063  0.1091\n",
      "     49        0.2687  0.1088\n",
      "     50        0.3440  0.1091\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "n_layers_lst = [2, 3, 4, 5] #2, 3, 4, 5\n",
    "n_heads_lst = [4, 8, 16, 32] #4, 8, 16, 32\n",
    "embed_dim = [128,256] #The embedding size is set one by one to avoid the out of memory error {128, 256}\n",
    "batch_size = 32 # 32, 64, 128, 256, 512, 1024\n",
    "epochs = 100\n",
    "sample_size = [100,80,60,40,20]\n",
    "'''\n",
    "n_layers = 2\n",
    "n_heads = 4\n",
    "embedding_size = 128\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "model, metrics, training_time = tabtrans_file.general_tabtrans(X_train, X_test, y_train, y_test, train_indices, val_indices, n_labels, n_numerical, n_categories, n_layers, n_heads, embedding_size, batch_size, epochs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabtrans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
