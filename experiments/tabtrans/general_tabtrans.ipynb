{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/diego-ngz/Git/thesis-tabtrans\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the project directory\n",
    "project_path = \"/home/diego-ngz/Git/thesis-tabtrans\"\n",
    "\n",
    "sys.path.append(project_path) #This helps to be able to import the data from the parent directory to other files\n",
    "\n",
    "from utils import tabtrans_file, data, training\n",
    "import numpy as np\n",
    "from skorch.callbacks import Checkpoint, TrainEndCheckpoint, EarlyStopping, LoadInitState, EpochScoring, Checkpoint\n",
    "import skorch\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:openml.datasets.dataset:pickle write credit-g\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "FINAL DATASETS\n",
    "1484 lsvt\n",
    "31 credit \n",
    "12 mfeat-factors 217 pass\n",
    "20 mfeat-pixel 241 #have probelms this dataset check this one\n",
    "9964 semeion 257 pass\n",
    "233092 arrhythmia 280\n",
    "3485 scene 300\n",
    "9976 madelon 501\n",
    "3481 isolet 618 (TO MUCH INSTANCES) \n",
    "'''\n",
    "\n",
    "df_id = 31\n",
    "\n",
    "X_train, X_test, y_train, y_test, train_indices, val_indices, n_instances, n_labels, n_numerical, n_categories = data.import_data(df_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego-ngz/anaconda3/envs/tabtrans/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer was not TransformerEncoderLayer\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "n_layers = 3\n",
    "n_heads = 2\n",
    "embedding_size = 128\n",
    "batch_size = 32\n",
    "epochs = 25\n",
    "\n",
    "#parameters of the NN\n",
    "ff_pw_size = 30  #this value because of the paper\n",
    "attn_dropout = 0.3 #paper\n",
    "ff_dropout = 0.1 #paper value\n",
    "aggregator = \"cls\"\n",
    "aggregator_parameters = None\n",
    "decoder_hidden_units = [128,64] #paper value\n",
    "decoder_activation_fn = nn.ReLU()\n",
    "need_weights = False\n",
    "numerical_passthrough = False\n",
    "\n",
    "#module\n",
    "module = training.build_module(\n",
    "    n_categories, # List of number of categories\n",
    "    n_numerical, # Number of numerical features\n",
    "    n_heads, # Number of heads per layer\n",
    "    ff_pw_size, # Size of the MLP inside each transformer encoder layer\n",
    "    n_layers, # Number of transformer encoder layers    \n",
    "    n_labels, # Number of output neurons\n",
    "    embedding_size,\n",
    "    attn_dropout, \n",
    "    ff_dropout, \n",
    "    aggregator, # The aggregator for output vectors before decoder\n",
    "    rnn_aggregator_parameters=aggregator_parameters,\n",
    "    decoder_hidden_units=decoder_hidden_units,\n",
    "    decoder_activation_fn=decoder_activation_fn,\n",
    "    need_weights=need_weights,\n",
    "    numerical_passthrough=numerical_passthrough\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_to_save = [10,25]\n",
    "\n",
    "for i in range(len(epochs_to_save)):\n",
    "    print(i)\n",
    "    print(\"Epoch: \", epochs_to_save[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6504\u001b[0m  0.4109\n",
      "      2        \u001b[36m0.6098\u001b[0m  0.2281\n",
      "      3        \u001b[36m0.6050\u001b[0m  0.2271\n",
      "      4        \u001b[36m0.5901\u001b[0m  0.2265\n",
      "      5        \u001b[36m0.5605\u001b[0m  0.2269\n",
      "      6        \u001b[36m0.5346\u001b[0m  0.2268\n",
      "      7        \u001b[36m0.5202\u001b[0m  0.2273\n",
      "      8        \u001b[36m0.5101\u001b[0m  0.2268\n",
      "      9        \u001b[36m0.5039\u001b[0m  0.2265\n",
      "     10        \u001b[36m0.4991\u001b[0m  0.2251\n",
      "--------------------------------------------------------------------\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "     11        \u001b[36m0.4933\u001b[0m  0.3164\n",
      "     12        \u001b[36m0.4902\u001b[0m  0.3274\n",
      "     13        \u001b[36m0.4817\u001b[0m  0.3261\n",
      "     14        0.4831  0.3272\n",
      "     15        \u001b[36m0.4726\u001b[0m  0.3253\n",
      "     16        \u001b[36m0.4655\u001b[0m  0.3261\n",
      "     17        0.4702  0.3261\n",
      "     18        \u001b[36m0.4603\u001b[0m  0.3274\n",
      "     19        0.4639  0.3273\n",
      "     20        \u001b[36m0.4550\u001b[0m  0.3272\n",
      "     21        \u001b[36m0.4473\u001b[0m  0.3267\n",
      "     22        0.4577  0.3273\n",
      "     23        \u001b[36m0.4425\u001b[0m  0.3264\n",
      "     24        0.4463  0.3272\n",
      "     25        0.4453  0.3258\n",
      "--------------------------------------------------------------------\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "     26        \u001b[36m0.4391\u001b[0m  0.3290\n",
      "     27        \u001b[36m0.4272\u001b[0m  0.3271\n",
      "     28        0.4322  0.3223\n",
      "     29        0.4296  0.3297\n",
      "     30        \u001b[36m0.4228\u001b[0m  0.3257\n"
     ]
    }
   ],
   "source": [
    "epochs_to_save = [10,25,30]\n",
    "\n",
    "train_end_cp = TrainEndCheckpoint(dirname = f\"epoch_{epochs_to_save[0]}\")\n",
    "\n",
    "#create the model\n",
    "model = skorch.NeuralNetClassifier(\n",
    "        module = module,\n",
    "        criterion=torch.nn.CrossEntropyLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        device= \"cuda\", #cuda\" if torch.cuda.is_available() else\n",
    "        batch_size = batch_size,\n",
    "        train_split = None,\n",
    "        max_epochs = 10,\n",
    "        optimizer__lr=1e-4,\n",
    "        optimizer__weight_decay=1e-4,\n",
    "        callbacks=[train_end_cp]\n",
    "    )\n",
    "\n",
    "model = model.fit(X={\n",
    "    \"x_numerical\": X_train[:, :n_numerical].astype(np.float32),\n",
    "    \"x_categorical\": X_train[:, n_numerical:].astype(np.int32)\n",
    "    }, \n",
    "    y=y_train.astype(np.int64)\n",
    ")\n",
    "\n",
    "print(\"--------------------------------------------------------------------\")\n",
    "\n",
    "load_state = LoadInitState(train_end_cp)\n",
    "\n",
    "train_end_cp = TrainEndCheckpoint(dirname = f\"epoch_{epochs_to_save[1]}\")\n",
    "\n",
    "#create the model\n",
    "model = skorch.NeuralNetClassifier(\n",
    "        module = module,\n",
    "        criterion=torch.nn.CrossEntropyLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        device= \"cuda\", #cuda\" if torch.cuda.is_available() else\n",
    "        batch_size = batch_size,\n",
    "        train_split = None,\n",
    "        max_epochs = epochs_to_save[1]-epochs_to_save[0],\n",
    "        optimizer__lr=1e-4,\n",
    "        optimizer__weight_decay=1e-4,\n",
    "        callbacks=[load_state, train_end_cp]\n",
    "    )\n",
    "\n",
    "model = model.fit(X={\n",
    "    \"x_numerical\": X_train[:, :n_numerical].astype(np.float32),\n",
    "    \"x_categorical\": X_train[:, n_numerical:].astype(np.int32)\n",
    "    }, \n",
    "    y=y_train.astype(np.int64)\n",
    ")\n",
    "\n",
    "\n",
    "print(\"--------------------------------------------------------------------\")\n",
    "load_state = LoadInitState(train_end_cp)\n",
    "train_end_cp = TrainEndCheckpoint(dirname = f\"epoch_{epochs_to_save[2]}\")\n",
    "\n",
    "#create the model\n",
    "model = skorch.NeuralNetClassifier(\n",
    "        module = module,\n",
    "        criterion=torch.nn.CrossEntropyLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        device= \"cuda\", #cuda\" if torch.cuda.is_available() else\n",
    "        batch_size = batch_size,\n",
    "        train_split = None,\n",
    "        max_epochs = epochs_to_save[2]-epochs_to_save[1],\n",
    "        optimizer__lr=1e-4,\n",
    "        optimizer__weight_decay=1e-4,\n",
    "        callbacks=[load_state, train_end_cp]\n",
    "    )\n",
    "\n",
    "model = model.fit(X={\n",
    "    \"x_numerical\": X_train[:, :n_numerical].astype(np.float32),\n",
    "    \"x_categorical\": X_train[:, n_numerical:].astype(np.int32)\n",
    "    }, \n",
    "    y=y_train.astype(np.int64)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model structure\n",
    "\n",
    "model = skorch.NeuralNetClassifier(\n",
    "        module = module,\n",
    "        criterion=torch.nn.CrossEntropyLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        device= \"cuda\", #cuda\" if torch.cuda.is_available() else\n",
    "        batch_size = batch_size,\n",
    "        train_split = None,\n",
    "        max_epochs = 10,\n",
    "        optimizer__lr=1e-4,\n",
    "        optimizer__weight_decay=1e-4,\n",
    "        callbacks=[train_end_cp]\n",
    "    )\n",
    "\n",
    "# Initialize the model\n",
    "model.initialize()\n",
    "\n",
    "# Load the saved parameters\n",
    "checkpoint_path = f\"epoch_{epochs_to_save[0]}/train_end_params.pt\"  # Adjust path as needed\n",
    "model.load_params(f_params = checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "skorch.classifier.NeuralNetClassifier"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_proba(X={\n",
    "        \"x_numerical\": X_test[:, :n_numerical].astype(np.float32),\n",
    "        \"x_categorical\": X_test[:, n_numerical:].astype(np.int32)\n",
    "        }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Instances: {X_train.shape[0]+X_test.shape[0]}\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "print(f\"F2i: {X_train.shape[1]/(X_train.shape[0]+X_test.shape[0])}\")\n",
    "print(f\"Classes: {n_labels}\")\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Initialize StratifiedKFold with 5 splits\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state = 11)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf.split(X_train, y_train)\n",
    "\n",
    "dict_split = {}\n",
    "i = 1\n",
    "\n",
    "\n",
    "for x,y in skf.split(X_train, y_train):\n",
    "\n",
    "    dict_split[f\"split_{i}\"] = set(y)    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "n_layers_lst = [2, 3, 4, 5] #2, 3, 4, 5\n",
    "n_heads_lst = [4, 8, 16, 32] #4, 8, 16, 32\n",
    "embed_dim = [128,256] #The embedding size is set one by one to avoid the out of memory error {128, 256}\n",
    "batch_size = 32 # 32, 64, 128, 256, 512, 1024\n",
    "epochs = 100\n",
    "sample_size = [100,80,60,40,20]\n",
    "'''\n",
    "n_layers = 2\n",
    "n_heads = 4\n",
    "embedding_size = 128\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "model, metrics = tabtrans_file.general_tabtrans(X_train, X_test, y_train, y_test, train_indices, val_indices, n_labels, n_numerical, n_categories, n_layers, n_heads, embedding_size, batch_size, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabtrans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
