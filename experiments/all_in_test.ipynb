{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All in 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parameters(task_id, Layers, Heads, Emedding_dim, batch_size):\n",
    "    \n",
    "    parameters = {\n",
    "    \"task_id\" : task_id,\n",
    "    \"Layers\": Layers,\n",
    "    \"Heads\": Heads,\n",
    "    \"Emedding_dim\": Emedding_dim,\n",
    "    \"batch_size\": batch_size,\n",
    "    }\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "project_path = \"/home/diego/Git/thesis-tabtrans\"\n",
    "sys.path.append(project_path) #import folders from the project_path\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import training, callback, evaluating, attention, data, plots \n",
    "from sklearn import datasets, model_selection\n",
    "import skorch\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import openml\n",
    "from sklearn import datasets, model_selection\n",
    "from skorch.callbacks import Checkpoint, EarlyStopping, LoadInitState, EpochScoring, Checkpoint, TrainEndCheckpoint\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_folder(project_path, new_folder_name):\n",
    "    # Create the full path for the new folder\n",
    "    new_folder_path = os.path.join(project_path, new_folder_name)\n",
    "\n",
    "    # Check if the new folder exists\n",
    "    if not os.path.exists(new_folder_path):\n",
    "        # If it doesn't exist, create the new folder\n",
    "        os.makedirs(new_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_csv(results_table, columns_names, folder_path):\n",
    "    # Creating the file path\n",
    "    file_path = folder_path + \"/results.csv\"\n",
    "\n",
    "    # Writing to the CSV file\n",
    "    with open(file_path, 'w', newline='') as csvfile:\n",
    "        # Creating a CSV writer object\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "\n",
    "        # Writing the column names\n",
    "        csv_writer.writerow(columns_names)\n",
    "\n",
    "        # Writing the data rows\n",
    "        csv_writer.writerows(results_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_creation(parameters, project_path):\n",
    "    \n",
    "    #extract parameters\n",
    "    task_id = parameters[\"task_id\"]\n",
    "    layers = parameters[\"Layers\"]\n",
    "    heads = parameters[\"Heads\"]\n",
    "    embed_dim = parameters[\"Emedding_dim\"]\n",
    "    batch_size = parameters[\"batch_size\"]\n",
    "    \n",
    "    total_combs = len(Layers)*len(Heads)\n",
    "    columns_names = [\"dataset_name\", \"experiment_num\", \"n_layers\", \"n_heads\", \"embed_dim\", \"batch_size\", \"balanced_accuracy\", \"accuracy\", \"log_loss\"]\n",
    "    results_table = []\n",
    "\n",
    "    #parameters for the model\n",
    "    ff_pw_size = 30  #this value because of the paper \n",
    "    attn_dropout = 0.3 #paper\n",
    "    ff_dropout = 0.1 #paper value\n",
    "    aggregator = \"cls\"\n",
    "    aggregator_parameters = None\n",
    "    decoder_hidden_units = [128,64] #paper value\n",
    "    decoder_activation_fn = nn.ReLU()\n",
    "    need_weights = False\n",
    "    numerical_passthrough = False\n",
    "\n",
    "\n",
    "    #get the dataset_name\n",
    "    dataset_name = data.get_dataset_name(task_id)\n",
    "\n",
    "    X_train, X_test, y_train, y_test, n_instances, n_labels, n_numerical, n_categories = data.import_data(task_id, \"task\")\n",
    "    #getting validation indices\n",
    "    train_indices, val_indices = model_selection.train_test_split(np.arange(X_train.shape[0]), test_size=0.333) #1/9 of train is equal to 10% of total\n",
    "\n",
    "\n",
    "    #create the folder to save the dataset experiments if it doesn't exist\n",
    "    new_folder(project_path, \"data_models\")\n",
    "    path_of_data_models = os.path.join(project_path, \"data_models\") #path of the folder data_models\n",
    "\n",
    "    #create the folder for specific dataset name\n",
    "    new_folder(path_of_data_models, dataset_name)\n",
    "    path_of_dataset = os.path.join(path_of_data_models, dataset_name) #path of the folder dataset \n",
    "\n",
    "    #save a .txt in the folder to sava the validation indices\n",
    "    np.savetxt(os.path.join(path_of_dataset, \"validation_indices\"), val_indices)\n",
    "    \n",
    "    experiment_num = 1\n",
    "    \n",
    "    for n_layers in layers:\n",
    "        for n_heads in heads:\n",
    "            #experiment i folder\n",
    "            new_folder(path_of_dataset, f\"experiment_{experiment_num}\")\n",
    "            path_of_experiment = os.path.join(path_of_dataset, f\"experiment_{experiment_num}\") #In this folder it will be saved the model and images\n",
    "\n",
    "\n",
    "            #create the folder for the checkpoints\n",
    "            new_folder(path_of_experiment, \"checkpoints\")\n",
    "            path_of_checkpoint = os.path.join(path_of_experiment, \"checkpoints\") #path to save the checkpoints\n",
    "\n",
    "            #create the folder for the plots\n",
    "            new_folder(path_of_experiment, \"plots\")\n",
    "            path_of_plots = os.path.join(path_of_experiment, \"plots\") #path of the folder dataset\n",
    "\n",
    "            #module\n",
    "            module = training.build_module(\n",
    "                n_categories, # List of number of categories\n",
    "                n_numerical, # Number of numerical features\n",
    "                n_heads, # Number of heads per layer\n",
    "                ff_pw_size, # Size of the MLP inside each transformer encoder layer\n",
    "                n_layers, # Number of transformer encoder layers    \n",
    "                n_labels, # Number of output neurons\n",
    "                embed_dim,\n",
    "                attn_dropout, \n",
    "                ff_dropout, \n",
    "                aggregator, # The aggregator for output vectors before decoder\n",
    "                rnn_aggregator_parameters=aggregator_parameters,\n",
    "                decoder_hidden_units=decoder_hidden_units,\n",
    "                decoder_activation_fn=decoder_activation_fn,\n",
    "                need_weights=need_weights,\n",
    "                numerical_passthrough=numerical_passthrough\n",
    "            )\n",
    "\n",
    "            #MODEL\n",
    "            model = skorch.NeuralNetClassifier(\n",
    "                module=module,\n",
    "                criterion=torch.nn.CrossEntropyLoss,\n",
    "                optimizer=torch.optim.AdamW,\n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                batch_size = batch_size,\n",
    "                max_epochs = 100,\n",
    "                train_split=skorch.dataset.ValidSplit(((train_indices, val_indices),)),\n",
    "                callbacks=[\n",
    "                    (\"balanced_accuracy\", skorch.callbacks.EpochScoring(\"balanced_accuracy\", lower_is_better=False)),\n",
    "                    (\"duration\", skorch.callbacks.EpochTimer()),\n",
    "                    EpochScoring(scoring='accuracy', name='train_acc', on_train=True),\n",
    "                    Checkpoint(monitor='valid_acc_best',dirname = path_of_checkpoint, load_best = True),\n",
    "                    EarlyStopping(patience = 15)\n",
    "\n",
    "                ],\n",
    "                optimizer__lr=1e-4,\n",
    "                optimizer__weight_decay=1e-4\n",
    "            )\n",
    "\n",
    "            # Define Checkpoint and TrainEndCheckpoint callbacks with custom directory\n",
    "            cp = Checkpoint()\n",
    "            train_end_cp = TrainEndCheckpoint()\n",
    "\n",
    "\n",
    "            #TRAINING\n",
    "            model = model.fit(X={\n",
    "                    \"x_numerical\": X_train[:, :n_numerical].astype(np.float32),\n",
    "                    \"x_categorical\": X_train[:, n_numerical:].astype(np.int32)\n",
    "                    }, \n",
    "                    y=y_train.astype(np.int64)\n",
    "                    )\n",
    "            \n",
    "            #TESTING\n",
    "            predictions = model.predict_proba(X={\n",
    "                            \"x_numerical\": X_test[:, :n_numerical].astype(np.float32),\n",
    "                            \"x_categorical\": X_test[:, n_numerical:].astype(np.int32)\n",
    "                            }\n",
    "                            )\n",
    "            \n",
    "            print(\"Test results:\\n\")\n",
    "            print(evaluating.get_default_scores(y_test.astype(np.int64), predictions, multiclass=True))\n",
    "            \n",
    "            balanced_accuracy = evaluating.get_default_scores(y_test, predictions, multiclass=True)[\"balanced_accuracy\"]\n",
    "            accuracy = evaluating.get_default_scores(y_test, predictions, multiclass=True)[\"accuracy\"]\n",
    "            log_loss = evaluating.get_default_scores(y_test, predictions, multiclass=True)[\"log_loss\"]\n",
    "\n",
    "            #save the results in a list\n",
    "            result_row = [dataset_name, experiment_num, n_layers, n_heads, embed_dim, batch_size, balanced_accuracy, accuracy, log_loss]\n",
    "            results_table.append(result_row)\n",
    "\n",
    "            #create and save the plots\n",
    "            fig_1, fig_2 = plots.model_plots(model, f\"Experiment {experiment_num}\")\n",
    "\n",
    "            # Save the first figure\n",
    "            fig_1.savefig(os.path.join(path_of_plots, 'figure1.png'))\n",
    "\n",
    "            # Save the second figure\n",
    "            fig_2.savefig(os.path.join(path_of_plots, 'figure2.png'))\n",
    "\n",
    "            #increase experiment number\n",
    "            experiment_num += 1\n",
    "\n",
    "\n",
    "    export_to_csv(results_table, columns_names, path_of_dataset)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = 233092\n",
    "Layers = [4]\n",
    "Heads = [4]\n",
    "Emedding_dim = 128 #The embedding size is set one by one to avoid the out of memory error\n",
    "total_combs = len(Layers)*len(Heads)\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = create_parameters(task_id, Layers, Heads, Emedding_dim, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:openml.datasets.dataset:pickle write arrhythmia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego/anaconda3/envs/tabtrans/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer was not TransformerEncoderLayer\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    balanced_accuracy    train_acc    train_loss    valid_acc    valid_loss    cp     dur\n",
      "-------  -------------------  -----------  ------------  -----------  ------------  ----  ------\n",
      "      1               \u001b[36m0.1000\u001b[0m       \u001b[32m0.5104\u001b[0m           nan       \u001b[31m0.5702\u001b[0m           nan     +  0.6535\n",
      "      2               0.1000       0.5228           nan       0.5702           nan        0.4788\n",
      "      3               0.1000       0.5228           nan       0.5702           nan        0.4762\n",
      "      4               0.1000       0.5228           nan       0.5702           nan        0.4763\n",
      "      5               0.1000       0.5228           nan       0.5702           nan        0.4760\n",
      "      6               0.1000       0.5228           nan       0.5702           nan        0.4767\n",
      "      7               0.1000       0.5228           nan       0.5702           nan        0.4768\n",
      "      8               0.1000       0.5228           nan       0.5702           nan        0.4755\n",
      "      9               0.1000       0.5228           nan       0.5702           nan        0.4768\n",
      "     10               0.1000       0.5228           nan       0.5702           nan        0.4767\n",
      "     11               0.1000       0.5228           nan       0.5702           nan        0.4755\n",
      "     12               0.1000       0.5228           nan       0.5702           nan        0.4770\n",
      "     13               0.1000       0.5228           nan       0.5702           nan        0.4790\n",
      "     14               0.1000       0.5228           nan       0.5702           nan        0.4780\n",
      "Stopping since valid_loss has not improved in the last 15 epochs.\n",
      "Test results:\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m project_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/diego/Git/thesis-tabtrans\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m model_creation(parameters, project_path)\n",
      "Cell \u001b[0;32mIn[5], line 123\u001b[0m, in \u001b[0;36mmodel_creation\u001b[0;34m(parameters, project_path)\u001b[0m\n\u001b[1;32m    116\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    117\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_numerical\u001b[39m\u001b[38;5;124m\"\u001b[39m: X_test[:, :n_numerical]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m    118\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_categorical\u001b[39m\u001b[38;5;124m\"\u001b[39m: X_test[:, n_numerical:]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m    119\u001b[0m                 }\n\u001b[1;32m    120\u001b[0m                 )\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest results:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28mprint\u001b[39m(evaluating\u001b[38;5;241m.\u001b[39mget_default_scores(y_test\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint64), predictions, multiclass\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m    125\u001b[0m balanced_accuracy \u001b[38;5;241m=\u001b[39m evaluating\u001b[38;5;241m.\u001b[39mget_default_scores(y_test, predictions, multiclass\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    126\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m evaluating\u001b[38;5;241m.\u001b[39mget_default_scores(y_test, predictions, multiclass\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Git/thesis-tabtrans/utils/evaluating.py:14\u001b[0m, in \u001b[0;36mget_default_scores\u001b[0;34m(target, prediction_proba, prefix, multiclass)\u001b[0m\n\u001b[1;32m      8\u001b[0m prediction \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(prediction_proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      9\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(prediction_proba\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     11\u001b[0m scores \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mbalanced_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: metrics\u001b[38;5;241m.\u001b[39mbalanced_accuracy_score(target, prediction),\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: metrics\u001b[38;5;241m.\u001b[39maccuracy_score(target, prediction),\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mlog_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: metrics\u001b[38;5;241m.\u001b[39mlog_loss(target, prediction_proba, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[1;32m     15\u001b[0m }\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m multiclass:\n\u001b[1;32m     18\u001b[0m     scores \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mscores,\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m\"\u001b[39m: metrics\u001b[38;5;241m.\u001b[39mroc_auc_score(target, prediction),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m\"\u001b[39m: metrics\u001b[38;5;241m.\u001b[39mrecall_score(target, prediction)\n\u001b[1;32m     24\u001b[0m     }\n",
      "File \u001b[0;32m~/anaconda3/envs/tabtrans/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/tabtrans/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2891\u001b[0m, in \u001b[0;36mlog_loss\u001b[0;34m(y_true, y_pred, eps, normalize, sample_weight, labels)\u001b[0m\n\u001b[1;32m   2799\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m   2800\u001b[0m     {\n\u001b[1;32m   2801\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2811\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2812\u001b[0m ):\n\u001b[1;32m   2813\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Log loss, aka logistic loss or cross-entropy loss.\u001b[39;00m\n\u001b[1;32m   2814\u001b[0m \n\u001b[1;32m   2815\u001b[0m \u001b[38;5;124;03m    This is the loss function used in (multinomial) logistic regression\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2889\u001b[0m \u001b[38;5;124;03m    0.21616...\u001b[39;00m\n\u001b[1;32m   2890\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2891\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   2892\u001b[0m         y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32, np\u001b[38;5;241m.\u001b[39mfloat16]\n\u001b[1;32m   2893\u001b[0m     )\n\u001b[1;32m   2894\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eps \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2895\u001b[0m         eps \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfinfo(y_pred\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39meps\n",
      "File \u001b[0;32m~/anaconda3/envs/tabtrans/lib/python3.11/site-packages/sklearn/utils/validation.py:1003\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    998\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    999\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1000\u001b[0m     )\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m-> 1003\u001b[0m     _assert_all_finite(\n\u001b[1;32m   1004\u001b[0m         array,\n\u001b[1;32m   1005\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m   1006\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m   1007\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1008\u001b[0m     )\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1012\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tabtrans/lib/python3.11/site-packages/sklearn/utils/validation.py:126\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m _assert_all_finite_element_wise(\n\u001b[1;32m    127\u001b[0m     X,\n\u001b[1;32m    128\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[1;32m    129\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[1;32m    130\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[1;32m    131\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    132\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    133\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/tabtrans/lib/python3.11/site-packages/sklearn/utils/validation.py:175\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m     )\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "project_path = \"/home/diego/Git/thesis-tabtrans\"\n",
    "model_creation(parameters, project_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabtrans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
