{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All in 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parameters(task_id, Layers, Heads, Emedding_dim, batch_size, epochs):\n",
    "    \n",
    "    parameters = {\n",
    "    \"task_id\" : task_id,\n",
    "    \"Layers\": Layers,\n",
    "    \"Heads\": Heads,\n",
    "    \"Emedding_dim\": Emedding_dim,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"epochs\": epochs,\n",
    "    }\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "project_path = \"/home/diego/Git/thesis-tabtrans\"\n",
    "sys.path.append(project_path) #import folders from the project_path\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import training, callback, evaluating, attention, data, plots \n",
    "from sklearn import datasets, model_selection\n",
    "import skorch\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import openml\n",
    "from sklearn import datasets, model_selection\n",
    "from skorch.callbacks import Checkpoint, EarlyStopping, LoadInitState, EpochScoring, Checkpoint, TrainEndCheckpoint\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_folder(project_path, new_folder_name):\n",
    "    # Create the full path for the new folder\n",
    "    new_folder_path = os.path.join(project_path, new_folder_name)\n",
    "\n",
    "    # Check if the new folder exists\n",
    "    if not os.path.exists(new_folder_path):\n",
    "        # If it doesn't exist, create the new folder\n",
    "        os.makedirs(new_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_csv(results_table, columns_names, folder_path):\n",
    "    # Creating the file path\n",
    "    file_path = folder_path + \"/results.csv\"\n",
    "\n",
    "    # Writing to the CSV file\n",
    "    with open(file_path, 'w', newline='') as csvfile:\n",
    "        # Creating a CSV writer object\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "\n",
    "        # Writing the column names\n",
    "        csv_writer.writerow(columns_names)\n",
    "\n",
    "        # Writing the data rows\n",
    "        csv_writer.writerows(results_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_creation(parameters, project_path):\n",
    "    \n",
    "    #extract parameters\n",
    "    task_id = parameters[\"task_id\"]\n",
    "    layers = parameters[\"Layers\"]\n",
    "    heads = parameters[\"Heads\"]\n",
    "    embed_dim = parameters[\"Emedding_dim\"]\n",
    "    batch_size = parameters[\"batch_size\"]\n",
    "    epochs = parameters[\"epochs\"]\n",
    "    \n",
    "    total_combs = len(Layers)*len(Heads)\n",
    "    columns_names = [\"dataset_name\", \"experiment_num\", \"n_layers\", \"n_heads\", \"embed_dim\", \"batch_size\", \"balanced_accuracy\", \"accuracy\", \"log_loss\"]\n",
    "    results_table = []\n",
    "\n",
    "    #parameters for the model\n",
    "    ff_pw_size = 30  #this value because of the paper \n",
    "    attn_dropout = 0.3 #paper\n",
    "    ff_dropout = 0.1 #paper value\n",
    "    aggregator = \"cls\"\n",
    "    aggregator_parameters = None\n",
    "    decoder_hidden_units = [128,64] #paper value\n",
    "    decoder_activation_fn = nn.ReLU()\n",
    "    need_weights = False\n",
    "    numerical_passthrough = False\n",
    "\n",
    "\n",
    "    #get the dataset_name\n",
    "    dataset_name = data.get_dataset_name(task_id)\n",
    "\n",
    "    X_train, X_test, y_train, y_test, n_instances, n_labels, n_numerical, n_categories = data.import_data(task_id)\n",
    "    #getting validation indices\n",
    "    train_indices, val_indices = model_selection.train_test_split(np.arange(X_train.shape[0]), test_size=0.333) #1/9 of train is equal to 10% of total\n",
    "\n",
    "\n",
    "    #create the folder to save the dataset experiments if it doesn't exist\n",
    "    new_folder(project_path, \"data_models\")\n",
    "    path_of_data_models = os.path.join(project_path, \"data_models\") #path of the folder data_models\n",
    "\n",
    "    #create the folder for specific dataset name\n",
    "    new_folder(path_of_data_models, f\"{dataset_name}_{embed_dim}\")\n",
    "    path_of_dataset = os.path.join(path_of_data_models, f\"{dataset_name}_{embed_dim}\") #path of the folder dataset \n",
    "\n",
    "    #save a .txt in the folder to sava the validation indices\n",
    "    np.savetxt(os.path.join(path_of_dataset, \"validation_indices\"), val_indices)\n",
    "    \n",
    "    experiment_num = 1\n",
    "    \n",
    "    for n_layers in layers:\n",
    "        for n_heads in heads:\n",
    "            #experiment i folder\n",
    "            new_folder(path_of_dataset, f\"experiment_{experiment_num}\")\n",
    "            path_of_experiment = os.path.join(path_of_dataset, f\"experiment_{experiment_num}\") #In this folder it will be saved the model and images\n",
    "\n",
    "\n",
    "            #create the folder for the checkpoints\n",
    "            new_folder(path_of_experiment, \"checkpoints\")\n",
    "            path_of_checkpoint = os.path.join(path_of_experiment, \"checkpoints\") #path to save the checkpoints\n",
    "\n",
    "            #create the folder for the plots\n",
    "            new_folder(path_of_experiment, \"plots\")\n",
    "            path_of_plots = os.path.join(path_of_experiment, \"plots\") #path of the folder dataset\n",
    "\n",
    "            #module\n",
    "            module = training.build_module(\n",
    "                n_categories, # List of number of categories\n",
    "                n_numerical, # Number of numerical features\n",
    "                n_heads, # Number of heads per layer\n",
    "                ff_pw_size, # Size of the MLP inside each transformer encoder layer\n",
    "                n_layers, # Number of transformer encoder layers    \n",
    "                n_labels, # Number of output neurons\n",
    "                embed_dim,\n",
    "                attn_dropout, \n",
    "                ff_dropout, \n",
    "                aggregator, # The aggregator for output vectors before decoder\n",
    "                rnn_aggregator_parameters=aggregator_parameters,\n",
    "                decoder_hidden_units=decoder_hidden_units,\n",
    "                decoder_activation_fn=decoder_activation_fn,\n",
    "                need_weights=need_weights,\n",
    "                numerical_passthrough=numerical_passthrough\n",
    "            )\n",
    "\n",
    "            #MODEL\n",
    "            model = skorch.NeuralNetClassifier(\n",
    "                module=module,\n",
    "                criterion=torch.nn.CrossEntropyLoss,\n",
    "                optimizer=torch.optim.AdamW,\n",
    "                device=\"cpu\", #\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                batch_size = batch_size,\n",
    "                max_epochs = epochs,\n",
    "                train_split=skorch.dataset.ValidSplit(((train_indices, val_indices),)),\n",
    "                callbacks=[\n",
    "                    (\"balanced_accuracy\", skorch.callbacks.EpochScoring(\"balanced_accuracy\", lower_is_better=False)),\n",
    "                    (\"duration\", skorch.callbacks.EpochTimer()),\n",
    "                    EpochScoring(scoring='accuracy', name='train_acc', on_train=True),\n",
    "                    Checkpoint(monitor='valid_acc_best',dirname=path_of_checkpoint,load_best = True),\n",
    "                    EarlyStopping(patience=3)\n",
    "\n",
    "                ],\n",
    "                optimizer__lr=1e-4,\n",
    "                optimizer__weight_decay=1e-4\n",
    "            )\n",
    "\n",
    "            # Define Checkpoint and TrainEndCheckpoint callbacks with custom directory\n",
    "            cp = Checkpoint()\n",
    "            train_end_cp = TrainEndCheckpoint()\n",
    "\n",
    "\n",
    "            #TRAINING\n",
    "            model = model.fit(X={\n",
    "                    \"x_numerical\": X_train[:, :n_numerical].astype(np.float32),\n",
    "                    \"x_categorical\": X_train[:, n_numerical:].astype(np.int32)\n",
    "                    }, \n",
    "                    y=y_train.astype(np.int64)\n",
    "                    )\n",
    "            \n",
    "            #TESTING\n",
    "            predictions = model.predict_proba(X={\n",
    "                            \"x_numerical\": X_test[:, :n_numerical].astype(np.float32),\n",
    "                            \"x_categorical\": X_test[:, n_numerical:].astype(np.int32)\n",
    "                            }\n",
    "                            )\n",
    "            \n",
    "            print(\"Test results:\\n\")\n",
    "            print(evaluating.get_default_scores(y_test.astype(np.int64), predictions, multiclass=True))\n",
    "            \n",
    "            balanced_accuracy = evaluating.get_default_scores(y_test, predictions, multiclass=True)[\"balanced_accuracy\"]\n",
    "            accuracy = evaluating.get_default_scores(y_test, predictions, multiclass=True)[\"accuracy\"]\n",
    "            log_loss = evaluating.get_default_scores(y_test, predictions, multiclass=True)[\"log_loss\"]\n",
    "\n",
    "            #save the results in a list\n",
    "            result_row = [dataset_name, experiment_num, n_layers, n_heads, embed_dim, batch_size, balanced_accuracy, accuracy, log_loss]\n",
    "            results_table.append(result_row)\n",
    "\n",
    "            #create and save the plots\n",
    "            fig_1, fig_2 = plots.model_plots(model, f\"Experiment {experiment_num}\")\n",
    "\n",
    "            # Save the first figure\n",
    "            fig_1.savefig(os.path.join(path_of_plots, 'figure1.png'))\n",
    "\n",
    "            # Save the second figure\n",
    "            fig_2.savefig(os.path.join(path_of_plots, 'figure2.png'))\n",
    "\n",
    "            #increase experiment number\n",
    "            experiment_num += 1\n",
    "\n",
    "\n",
    "    export_to_csv(results_table, columns_names, path_of_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = 233092\n",
    "Layers = [4]\n",
    "Heads = [4]\n",
    "Emedding_dim = 128 #The embedding size is set one by one to avoid the out of memory error\n",
    "total_combs = len(Layers)*len(Heads)\n",
    "batch_size = 64\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = create_parameters(task_id, Layers, Heads, Emedding_dim, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = \"/home/diego/Git/thesis-tabtrans\"\n",
    "model_creation(parameters, project_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabtrans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
