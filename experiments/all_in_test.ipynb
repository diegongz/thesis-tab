{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All in 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = 233093\n",
    "Layers = [2]\n",
    "Heads = [4]\n",
    "Emedding_dim = 128 #The embedding size is set one by one to avoid the out of memory error\n",
    "total_combs = len(Layers)*len(Heads)\n",
    "batch_size = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parameters(task_id, Layers, Heads, Emedding_dim, batch_size):\n",
    "    \n",
    "    parameters = {\n",
    "    \"task_id\" : task_id,\n",
    "    \"Layers\": Layers,\n",
    "    \"Heads\": Heads,\n",
    "    \"Emedding_dim\": Emedding_dim,\n",
    "    \"bath_size\": batch_size,\n",
    "    }\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = create_parameters(task_id, Layers, Heads, Emedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "import os\n",
    "\n",
    "path_project = \"/home/diego/Git/thesis-tabtrans/new_folder\"\n",
    "\n",
    "# Use the os.makedirs() function to create the new folder\n",
    "os.makedirs(path_project) #create new_folder\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = \"/home/diego/Git/thesis-tabtrans\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(project_path) #import folders from the project_path\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import training, callback, evaluating, attention, data, plots \n",
    "from sklearn import datasets, model_selection\n",
    "import skorch\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import openml\n",
    "from sklearn import datasets, model_selection\n",
    "from skorch.callbacks import Checkpoint, EarlyStopping, LoadInitState, EpochScoring, Checkpoint, TrainEndCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:openml.datasets.dataset:pickle write mfeat-factors\n"
     ]
    }
   ],
   "source": [
    "task_id = 233093\n",
    "dataset_name = data.get_dataset_name(task_id)\n",
    "\n",
    "X_train, X_test, y_train, y_test, n_instances, n_labels, n_numerical, n_categories = data.import_data(task_id, \"task\")\n",
    "\n",
    "#####################################################\n",
    "    # Split data\n",
    "#####################################################\n",
    "\n",
    "#getting validation indices\n",
    "train_indices, val_indices = model_selection.train_test_split(np.arange(X_train.shape[0]), test_size=0.333) #1/9 of train is equal to 10% of total\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_heads = 4 # In average 4 works better\n",
    "embed_dim = 256 # In average 256 works better\n",
    "n_layers = 3\n",
    "ff_pw_size = 30  #this value because of the paper \n",
    "attn_dropout = 0.3 #paper\n",
    "ff_dropout = 0.1 #paper value\n",
    "aggregator = \"cls\"\n",
    "aggregator_parameters = None\n",
    "decoder_hidden_units = [128,64] #paper value\n",
    "decoder_activation_fn = nn.ReLU()\n",
    "need_weights = False\n",
    "numerical_passthrough = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego/anaconda3/envs/tabtrans/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer was not TransformerEncoderLayer\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "#module\n",
    "module = training.build_module(\n",
    "    n_categories, # List of number of categories\n",
    "    n_numerical, # Number of numerical features\n",
    "    n_heads, # Number of heads per layer\n",
    "    ff_pw_size, # Size of the MLP inside each transformer encoder layer\n",
    "    n_layers, # Number of transformer encoder layers    \n",
    "    n_labels, # Number of output neurons\n",
    "    embed_dim,\n",
    "    attn_dropout, \n",
    "    ff_dropout, \n",
    "    aggregator, # The aggregator for output vectors before decoder\n",
    "    rnn_aggregator_parameters=aggregator_parameters,\n",
    "    decoder_hidden_units=decoder_hidden_units,\n",
    "    decoder_activation_fn=decoder_activation_fn,\n",
    "    need_weights=need_weights,\n",
    "    numerical_passthrough=numerical_passthrough\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_folder(project_path, new_folder_name):\n",
    "    # Create the full path for the new folder\n",
    "    new_folder_path = os.path.join(project_path, new_folder_name)\n",
    "\n",
    "    # Check if the new folder exists\n",
    "    if not os.path.exists(new_folder_path):\n",
    "        # If it doesn't exist, create the new folder\n",
    "        os.makedirs(new_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the project path\n",
    "project_path = \"/home/diego/Git/thesis-tabtrans\"\n",
    "\n",
    "# Specify the name of the new folder\n",
    "new_folder_name = \"data\"\n",
    "\n",
    "new_folder(project_path, new_folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_id': 233093, 'Layers': [2], 'Heads': [4], 'Emedding_dim': 128}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "for i in range(2):\n",
    "    for z in range(3):\n",
    "        num += 1\n",
    "        print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_creation(parameters, project_path):\n",
    "    \n",
    "    #extract parameters\n",
    "    task_id = parameters[\"task_id\"]\n",
    "    Layers = parameters[\"Layers\"]\n",
    "    Heads = parameters[\"Heads\"]\n",
    "    Emedding_dim = parameters[\"Emedding_dim\"]\n",
    "    batch_size = parameters[\"batch_size\"]\n",
    "    total_combs = len(Layers)*len(Heads)\n",
    "\n",
    "    dataset_name = data.get_dataset_name(task_id)\n",
    "\n",
    "    X_train, X_test, y_train, y_test, n_instances, n_labels, n_numerical, n_categories = data.import_data(task_id, \"task\")\n",
    "    #getting validation indices\n",
    "    train_indices, val_indices = model_selection.train_test_split(np.arange(X_train.shape[0]), test_size=0.333) #1/9 of train is equal to 10% of total\n",
    "\n",
    "\n",
    "    #create the folder to save the dataset experiments if it doesn't exist\n",
    "    new_folder(project_path, \"data_models\")\n",
    "    path_of_data_models = os.path.join(project_path, \"data_models\") #path of the folder data_models\n",
    "\n",
    "    #create the folder for specific dataset name\n",
    "    new_folder(path_of_data_models, dataset_name)\n",
    "    path_of_dataset = os.path.join(path_of_data_models, dataset_name) #path of the folder dataset \n",
    "\n",
    "    experiment_num = 1\n",
    "    \n",
    "    for layer in layers:\n",
    "        for head in heads:\n",
    "            #experiment i folder\n",
    "            new_folder(path_of_dataset, f\"experiment_{experiment_num}\")\n",
    "            path_of_experiment = os.path.join(path_of_dataset, f\"experiment_{experiment_i}\") #In this folder it will be saved the model and images\n",
    "\n",
    "\n",
    "            model = skorch.NeuralNetClassifier(\n",
    "                module=module,\n",
    "                criterion=torch.nn.CrossEntropyLoss,\n",
    "                optimizer=torch.optim.AdamW,\n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                batch_size = batch_size,\n",
    "                max_epochs = 100,\n",
    "                train_split=skorch.dataset.ValidSplit(((train_indices, val_indices),)),\n",
    "                callbacks=[\n",
    "                    (\"balanced_accuracy\", skorch.callbacks.EpochScoring(\"balanced_accuracy\", lower_is_better=False)),\n",
    "                    (\"duration\", skorch.callbacks.EpochTimer()),\n",
    "                    EpochScoring(scoring='accuracy', name='train_acc', on_train=True),\n",
    "                    Checkpoint(monitor='valid_acc_best',dirname = path_of_experiment, load_best = True),\n",
    "                    EarlyStopping(patience=3)\n",
    "\n",
    "                ],\n",
    "                optimizer__lr=1e-4,\n",
    "                optimizer__weight_decay=1e-4\n",
    "            )\n",
    "\n",
    "            # Define Checkpoint and TrainEndCheckpoint callbacks with custom directory\n",
    "            cp = Checkpoint()\n",
    "            train_end_cp = TrainEndCheckpoint()\n",
    "\n",
    "            model = model.fit(X={\n",
    "                    \"x_numerical\": X_train[:, :n_numerical].astype(np.float32),\n",
    "                    \"x_categorical\": X_train[:, n_numerical:].astype(np.int32)\n",
    "                    }, \n",
    "                    y=y_train.astype(np.int64)\n",
    "                    )\n",
    "            \n",
    "            '''\n",
    "            TO DO LIST:\n",
    "            - TEST THE MODEL\n",
    "            - FROM THOSE RESULTS CREATE A PANDAS\n",
    "            - SAVE THE PLOTS IN AN SPECIFIC FOLDER\n",
    "            '''\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabtrans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
